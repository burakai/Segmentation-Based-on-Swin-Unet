{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1l4Rk7Nw34-1WWZrGjexeh5bAVm3hqdOP","authorship_tag":"ABX9TyOH87dZj6OpqMeWHlK5LgWp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Create Python 3.7 Environment"],"metadata":{"id":"SfzCRQHuWsZm"}},{"cell_type":"code","source":["!update-alternatives --list python3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fHD4hY7zTfoW","executionInfo":{"status":"ok","timestamp":1686458549577,"user_tz":-180,"elapsed":519,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"aa4c42e9-1ab8-40bf-daed-75e8efd5a527"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/bin/python3.10\n","/usr/bin/python3.8\n"]}]},{"cell_type":"code","source":["!sudo apt install python3.7\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2bF3hI6WP9GR","executionInfo":{"status":"ok","timestamp":1686458558924,"user_tz":-180,"elapsed":9348,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"093814cb-262f-4c03-ee44-7cad7ff21196"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  libpython3.7-minimal libpython3.7-stdlib python3.7-minimal\n","Suggested packages:\n","  python3.7-venv binfmt-support\n","The following NEW packages will be installed:\n","  libpython3.7-minimal libpython3.7-stdlib python3.7 python3.7-minimal\n","0 upgraded, 4 newly installed, 0 to remove and 38 not upgraded.\n","Need to get 4,531 kB of archives.\n","After this operation, 23.3 MB of additional disk space will be used.\n","Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.7-minimal amd64 3.7.17-1+focal1 [589 kB]\n","Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.7-minimal amd64 3.7.17-1+focal1 [1,808 kB]\n","Get:3 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.7-stdlib amd64 3.7.17-1+focal1 [1,774 kB]\n","Get:4 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.7 amd64 3.7.17-1+focal1 [361 kB]\n","Fetched 4,531 kB in 3s (1,324 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package libpython3.7-minimal:amd64.\n","(Reading database ... 122541 files and directories currently installed.)\n","Preparing to unpack .../libpython3.7-minimal_3.7.17-1+focal1_amd64.deb ...\n","Unpacking libpython3.7-minimal:amd64 (3.7.17-1+focal1) ...\n","Selecting previously unselected package python3.7-minimal.\n","Preparing to unpack .../python3.7-minimal_3.7.17-1+focal1_amd64.deb ...\n","Unpacking python3.7-minimal (3.7.17-1+focal1) ...\n","Selecting previously unselected package libpython3.7-stdlib:amd64.\n","Preparing to unpack .../libpython3.7-stdlib_3.7.17-1+focal1_amd64.deb ...\n","Unpacking libpython3.7-stdlib:amd64 (3.7.17-1+focal1) ...\n","Selecting previously unselected package python3.7.\n","Preparing to unpack .../python3.7_3.7.17-1+focal1_amd64.deb ...\n","Unpacking python3.7 (3.7.17-1+focal1) ...\n","Setting up libpython3.7-minimal:amd64 (3.7.17-1+focal1) ...\n","Setting up python3.7-minimal (3.7.17-1+focal1) ...\n","Setting up libpython3.7-stdlib:amd64 (3.7.17-1+focal1) ...\n","Setting up python3.7 (3.7.17-1+focal1) ...\n","Processing triggers for man-db (2.9.1-1) ...\n","Processing triggers for mime-support (3.64ubuntu1) ...\n"]}]},{"cell_type":"code","source":["!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 2\n"],"metadata":{"id":"8oBpp8u57qLU","executionInfo":{"status":"ok","timestamp":1686458559472,"user_tz":-180,"elapsed":550,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!update-alternatives --list python3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zWEW0C7DW0QV","executionInfo":{"status":"ok","timestamp":1686458559472,"user_tz":-180,"elapsed":4,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"7351ecef-63d2-4454-a388-97721c05bae1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/bin/python3.10\n","/usr/bin/python3.7\n","/usr/bin/python3.8\n"]}]},{"cell_type":"code","source":["!update-alternatives --set python3 /usr/bin/python3.7\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5-rAr2U5TwMs","executionInfo":{"status":"ok","timestamp":1686458559472,"user_tz":-180,"elapsed":3,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"3b679eb7-2975-4d60-ff77-67824541cdde"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["update-alternatives: using /usr/bin/python3.7 to provide /usr/bin/python3 (python3) in manual mode\n"]}]},{"cell_type":"code","source":["!python3 --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"--rkWcDNUDty","executionInfo":{"status":"ok","timestamp":1686458559472,"user_tz":-180,"elapsed":2,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"4b5ecfb1-4dab-400e-f177-6e33b7890857"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.7.17\n"]}]},{"cell_type":"code","source":["!apt update\n","!apt install python3.7-distutils\n","!curl -sS https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n","!python3.7 get-pip.py\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bKnaNAGHz8HC","executionInfo":{"status":"ok","timestamp":1686458573641,"user_tz":-180,"elapsed":14171,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"49c0f946-2cba-45b6-8afa-aa9bc96da874"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Get:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n","Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n","Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease\n","Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n","Get:5 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n","Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n","Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,255 kB]\n","Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n","Get:11 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,354 kB]\n","Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n","Get:13 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,776 kB]\n","Hit:14 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n","Fetched 7,725 kB in 2s (4,463 kB/s)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","38 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  python3.7-lib2to3\n","The following NEW packages will be installed:\n","  python3.7-distutils python3.7-lib2to3\n","0 upgraded, 2 newly installed, 0 to remove and 38 not upgraded.\n","Need to get 309 kB of archives.\n","After this operation, 1,229 kB of additional disk space will be used.\n","Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.7-lib2to3 all 3.7.17-1+focal1 [122 kB]\n","Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.7-distutils all 3.7.17-1+focal1 [187 kB]\n","Fetched 309 kB in 1s (279 kB/s)\n","Selecting previously unselected package python3.7-lib2to3.\n","(Reading database ... 123161 files and directories currently installed.)\n","Preparing to unpack .../python3.7-lib2to3_3.7.17-1+focal1_all.deb ...\n","Unpacking python3.7-lib2to3 (3.7.17-1+focal1) ...\n","Selecting previously unselected package python3.7-distutils.\n","Preparing to unpack .../python3.7-distutils_3.7.17-1+focal1_all.deb ...\n","Unpacking python3.7-distutils (3.7.17-1+focal1) ...\n","Setting up python3.7-lib2to3 (3.7.17-1+focal1) ...\n","Setting up python3.7-distutils (3.7.17-1+focal1) ...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pip\n","  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting setuptools\n","  Downloading setuptools-67.8.0-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wheel\n","  Downloading wheel-0.40.0-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: wheel, setuptools, pip\n","Successfully installed pip-23.1.2 setuptools-67.8.0 wheel-0.40.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!sudo apt install python3-pip\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RMTAFvaLUNw4","executionInfo":{"status":"ok","timestamp":1686458578647,"user_tz":-180,"elapsed":5008,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"eef0f3e2-bc7d-4482-8779-c94265719a17"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  python-pip-whl python3-setuptools python3-wheel\n","Suggested packages:\n","  python-setuptools-doc\n","The following NEW packages will be installed:\n","  python-pip-whl python3-pip python3-setuptools python3-wheel\n","0 upgraded, 4 newly installed, 0 to remove and 38 not upgraded.\n","Need to get 2,389 kB of archives.\n","After this operation, 4,933 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python-pip-whl all 20.0.2-5ubuntu1.8 [1,805 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 python3-setuptools all 45.2.0-1ubuntu0.1 [330 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-wheel all 0.34.2-1ubuntu0.1 [23.9 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-pip all 20.0.2-5ubuntu1.8 [231 kB]\n","Fetched 2,389 kB in 0s (5,653 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package python-pip-whl.\n","(Reading database ... 123300 files and directories currently installed.)\n","Preparing to unpack .../python-pip-whl_20.0.2-5ubuntu1.8_all.deb ...\n","Unpacking python-pip-whl (20.0.2-5ubuntu1.8) ...\n","Selecting previously unselected package python3-setuptools.\n","Preparing to unpack .../python3-setuptools_45.2.0-1ubuntu0.1_all.deb ...\n","Unpacking python3-setuptools (45.2.0-1ubuntu0.1) ...\n","Selecting previously unselected package python3-wheel.\n","Preparing to unpack .../python3-wheel_0.34.2-1ubuntu0.1_all.deb ...\n","Unpacking python3-wheel (0.34.2-1ubuntu0.1) ...\n","Selecting previously unselected package python3-pip.\n","Preparing to unpack .../python3-pip_20.0.2-5ubuntu1.8_all.deb ...\n","Unpacking python3-pip (20.0.2-5ubuntu1.8) ...\n","Setting up python3-setuptools (45.2.0-1ubuntu0.1) ...\n","Setting up python3-wheel (0.34.2-1ubuntu0.1) ...\n","Setting up python-pip-whl (20.0.2-5ubuntu1.8) ...\n","Setting up python3-pip (20.0.2-5ubuntu1.8) ...\n","Processing triggers for man-db (2.9.1-1) ...\n"]}]},{"cell_type":"code","source":["!pip3 --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JHaFJyobUv_B","executionInfo":{"status":"ok","timestamp":1686458579162,"user_tz":-180,"elapsed":517,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"c56d4375-6e64-4c08-d2af-3795ddf9163e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["pip 23.1.2 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n"]}]},{"cell_type":"markdown","source":["# Environment Setup"],"metadata":{"id":"eZw7BQfaXBsl"}},{"cell_type":"markdown","source":["Create PATHs"],"metadata":{"id":"YzN7ENYwXOr_"}},{"cell_type":"code","source":["ROOT_DIR = \"/content/drive/My Drive/\"\n","HOME_DIR = ROOT_DIR + \"itu/PhD/BLG 641E - Medical Image Computing/final project/\"\n","CWD_DIR = HOME_DIR + \"code/Swin-Unet/\"\n","\n","DATA_DIR = ROOT_DIR + \"data/project_TransUNet/data/Synapse/\"\n","TRAIN_DIR = DATA_DIR + \"train/\"\n","VAL_DIR = DATA_DIR + \"val/\"\n","\n","CKPT_DIR = HOME_DIR + \"ckpt\"\n","TEMP_DIR = HOME_DIR + \"code/.tmp/\"\n","OUT_DIR = HOME_DIR + \"saved/\"\n"],"metadata":{"id":"qcxnD-MKD9BM","executionInfo":{"status":"ok","timestamp":1686458601398,"user_tz":-180,"elapsed":432,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["List and Install Packages"],"metadata":{"id":"xuSSuHa0NQGA"}},{"cell_type":"code","source":["import os\n","\n","#cwd = os.chdir(HOME_DIR + \"code/\")\n","#!git clone https://github.com/HuCaoFighting/Swin-Unet.git"],"metadata":{"id":"K0c-F9sBVCIN","executionInfo":{"status":"ok","timestamp":1686458601847,"user_tz":-180,"elapsed":2,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["cwd = os.chdir(CWD_DIR)\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8oWfwl6mVHh6","executionInfo":{"status":"ok","timestamp":1686458603230,"user_tz":-180,"elapsed":1384,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"f200919b-50a3-4368-8ff2-03395a3befd7"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/itu/PhD/BLG 641E - Medical Image Computing/final project/code/Swin-Unet\n"]}]},{"cell_type":"code","source":["!cat requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wUop2RIqDa0s","executionInfo":{"status":"ok","timestamp":1686458604356,"user_tz":-180,"elapsed":1128,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"ba3894b1-fec5-4ba7-acbd-ab62df52c123"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["torch==1.4.0\n","torchvision==0.5.0\n","numpy\n","tqdm\n","tensorboard\n","tensorboardX\n","ml-collections\n","medpy\n","SimpleITK\n","scipy\n","h5py\n","timm\n","einops\n","yacs\n"]}]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"KhZKRf6fCfKp","executionInfo":{"status":"ok","timestamp":1686458673674,"user_tz":-180,"elapsed":69320,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"95173a74-9716-40d9-b317-a25304a85f3a"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.4.0 (from -r requirements.txt (line 1))\n","  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.4/753.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision==0.5.0 (from -r requirements.txt (line 2))\n","  Downloading torchvision-0.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy (from -r requirements.txt (line 3))\n","  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tqdm (from -r requirements.txt (line 4))\n","  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard (from -r requirements.txt (line 5))\n","  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboardX (from -r requirements.txt (line 6))\n","  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ml-collections (from -r requirements.txt (line 7))\n","  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting medpy (from -r requirements.txt (line 8))\n","  Downloading MedPy-0.4.0.tar.gz (151 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting SimpleITK (from -r requirements.txt (line 9))\n","  Downloading SimpleITK-2.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy (from -r requirements.txt (line 10))\n","  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h5py (from -r requirements.txt (line 11))\n","  Downloading h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting timm (from -r requirements.txt (line 12))\n","  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops (from -r requirements.txt (line 13))\n","  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting yacs (from -r requirements.txt (line 14))\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Collecting six (from torchvision==0.5.0->-r requirements.txt (line 2))\n","  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Collecting pillow>=4.1.1 (from torchvision==0.5.0->-r requirements.txt (line 2))\n","  Downloading Pillow-9.5.0-cp37-cp37m-manylinux_2_28_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 5))\n","  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting grpcio>=1.24.3 (from tensorboard->-r requirements.txt (line 5))\n","  Downloading grpcio-1.54.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-auth<3,>=1.6.3 (from tensorboard->-r requirements.txt (line 5))\n","  Downloading google_auth-2.19.1-py2.py3-none-any.whl (181 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard->-r requirements.txt (line 5))\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Collecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 5))\n","  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting protobuf<4,>=3.9.2 (from tensorboard->-r requirements.txt (line 5))\n","  Downloading protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting requests<3,>=2.21.0 (from tensorboard->-r requirements.txt (line 5))\n","  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 5)) (67.8.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard->-r requirements.txt (line 5))\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard->-r requirements.txt (line 5))\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting werkzeug>=1.0.1 (from tensorboard->-r requirements.txt (line 5))\n","  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r requirements.txt (line 5)) (0.40.0)\n","Collecting packaging (from tensorboardX->-r requirements.txt (line 6))\n","  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting PyYAML (from ml-collections->-r requirements.txt (line 7))\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting contextlib2 (from ml-collections->-r requirements.txt (line 7))\n","  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n","INFO: pip is looking at multiple versions of timm to determine which version is compatible with other requirements. This could take a while.\n","Collecting timm (from -r requirements.txt (line 12))\n","  Downloading timm-0.9.1-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading timm-0.9.0-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading timm-0.6.13-py3-none-any.whl (549 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading timm-0.6.11-py3-none-any.whl (548 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.7/548.7 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5))\n","  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n","Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5))\n","  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5))\n","  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n","Collecting urllib3<2.0 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5))\n","  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 5))\n","  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n","Collecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 5))\n","  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n","Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 5))\n","  Downloading charset_normalizer-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (171 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.0/171.0 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 5))\n","  Downloading idna-3.4-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 5))\n","  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.0/157.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 5))\n","  Downloading MarkupSafe-2.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Collecting zipp>=0.5 (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 5))\n","  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n","Collecting typing-extensions>=3.6.4 (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 5))\n","  Downloading typing_extensions-4.6.3-py3-none-any.whl (31 kB)\n","Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5))\n","  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 5))\n","  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: ml-collections, medpy\n","  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94506 sha256=7de890f9330ce50174878d7b3cf262f8f8e7c9135a129928333dcec1d24733ab\n","  Stored in directory: /root/.cache/pip/wheels/b7/da/64/33c926a1b10ff19791081b705879561b715a8341a856a3bbd2\n","  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for medpy: filename=MedPy-0.4.0-py3-none-any.whl size=214946 sha256=ab0b275b577eace9f4e55000f12595695727c7bb661892d0fba6a47b7b260563\n","  Stored in directory: /root/.cache/pip/wheels/b0/57/3a/da1183f22a6afb42e11138daa6a759de233fd977a984333602\n","Successfully built ml-collections medpy\n","Installing collected packages: tensorboard-plugin-wit, SimpleITK, zipp, urllib3, typing-extensions, tqdm, torch, tensorboard-data-server, six, PyYAML, pyasn1, protobuf, pillow, packaging, oauthlib, numpy, MarkupSafe, idna, grpcio, einops, contextlib2, charset-normalizer, certifi, cachetools, absl-py, yacs, werkzeug, torchvision, tensorboardX, scipy, rsa, requests, pyasn1-modules, ml-collections, importlib-metadata, h5py, timm, requests-oauthlib, medpy, markdown, google-auth, google-auth-oauthlib, tensorboard\n","Successfully installed MarkupSafe-2.1.3 PyYAML-6.0 SimpleITK-2.2.1 absl-py-1.4.0 cachetools-5.3.1 certifi-2023.5.7 charset-normalizer-3.1.0 contextlib2-21.6.0 einops-0.6.1 google-auth-2.19.1 google-auth-oauthlib-0.4.6 grpcio-1.54.2 h5py-3.8.0 idna-3.4 importlib-metadata-6.6.0 markdown-3.4.3 medpy-0.4.0 ml-collections-0.1.1 numpy-1.21.6 oauthlib-3.2.2 packaging-23.1 pillow-9.5.0 protobuf-3.20.3 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 scipy-1.7.3 six-1.16.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorboardX-2.6 timm-0.6.7 torch-1.4.0 torchvision-0.5.0 tqdm-4.65.0 typing-extensions-4.6.3 urllib3-1.26.16 werkzeug-2.2.3 yacs-0.1.8 zipp-3.15.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google","six"]}}},"metadata":{}}]},{"cell_type":"markdown","source":["GPU used"],"metadata":{"id":"Xd1QWU4t4PU9"}},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JwqHcJNMOCAz","executionInfo":{"status":"ok","timestamp":1686458686549,"user_tz":-180,"elapsed":5,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"7494fa97-7aae-459c-bdea-fb820b1f9617"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Jun 11 04:44:45 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P8     8W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["!python train.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xXHzZ61n4THU","executionInfo":{"status":"ok","timestamp":1686458690871,"user_tz":-180,"elapsed":3027,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"7d691386-1c2f-44e2-9c71-ef5abe0350df"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["usage: train.py [-h] [--root_path ROOT_PATH] [--dataset DATASET]\n","                [--list_dir LIST_DIR] [--num_classes NUM_CLASSES]\n","                [--output_dir OUTPUT_DIR] [--max_iterations MAX_ITERATIONS]\n","                [--max_epochs MAX_EPOCHS] [--batch_size BATCH_SIZE]\n","                [--n_gpu N_GPU] [--deterministic DETERMINISTIC]\n","                [--base_lr BASE_LR] [--img_size IMG_SIZE] [--seed SEED] --cfg\n","                FILE [--opts OPTS [OPTS ...]] [--zip]\n","                [--cache-mode {no,full,part}] [--resume RESUME]\n","                [--accumulation-steps ACCUMULATION_STEPS] [--use-checkpoint]\n","                [--amp-opt-level {O0,O1,O2}] [--tag TAG] [--eval]\n","                [--throughput]\n","train.py: error: the following arguments are required: --cfg\n"]}]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2IOiMvi4_dcZ","executionInfo":{"status":"ok","timestamp":1686458701878,"user_tz":-180,"elapsed":447,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"fc76cd2a-f237-40d9-a09f-a51929515d5b"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/itu/PhD/BLG 641E - Medical Image Computing/final project/code/Swin-Unet\n"]}]},{"cell_type":"code","source":["!pip show torch torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SYUX6M6yVZZd","executionInfo":{"status":"ok","timestamp":1686458704472,"user_tz":-180,"elapsed":1060,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"7d926775-32b5-46c2-b5fe-85ce45f91571"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Name: torch\n","Version: 1.4.0\n","Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n","Home-page: https://pytorch.org/\n","Author: PyTorch Team\n","Author-email: packages@pytorch.org\n","License: BSD-3\n","Location: /usr/local/lib/python3.7/dist-packages\n","Requires: \n","Required-by: timm, torchvision\n","---\n","Name: torchvision\n","Version: 0.5.0\n","Summary: image and video datasets and models for torch deep learning\n","Home-page: https://github.com/pytorch/vision\n","Author: PyTorch Core Team\n","Author-email: soumith@pytorch.org\n","License: BSD\n","Location: /usr/local/lib/python3.7/dist-packages\n","Requires: numpy, pillow, six, torch\n","Required-by: timm\n"]}]},{"cell_type":"code","source":["!pip install --upgrade torch torchvision\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sRhA9ByBWelc","executionInfo":{"status":"ok","timestamp":1686458800540,"user_tz":-180,"elapsed":95518,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"9a62fd64-eb8d-4f49-d6a7-fc7a70bfbc86"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.4.0)\n","Collecting torch\n","  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.5.0)\n","Collecting torchvision\n","  Downloading torchvision-0.14.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.6.3)\n","Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch)\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch)\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch)\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.8.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (9.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2023.5.7)\n","Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, torchvision\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.4.0\n","    Uninstalling torch-1.4.0:\n","      Successfully uninstalled torch-1.4.0\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.5.0\n","    Uninstalling torchvision-0.5.0:\n","      Successfully uninstalled torchvision-0.5.0\n","Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1 torchvision-0.14.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip show torch torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wq04CwPJW200","executionInfo":{"status":"ok","timestamp":1686458800540,"user_tz":-180,"elapsed":10,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"819da98b-f02c-4e15-bcc7-b8164e683bbe"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Name: torch\n","Version: 1.13.1\n","Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n","Home-page: https://pytorch.org/\n","Author: PyTorch Team\n","Author-email: packages@pytorch.org\n","License: BSD-3\n","Location: /usr/local/lib/python3.7/dist-packages\n","Requires: nvidia-cublas-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-runtime-cu11, nvidia-cudnn-cu11, typing-extensions\n","Required-by: timm, torchvision\n","---\n","Name: torchvision\n","Version: 0.14.1\n","Summary: image and video datasets and models for torch deep learning\n","Home-page: https://github.com/pytorch/vision\n","Author: PyTorch Core Team\n","Author-email: soumith@pytorch.org\n","License: BSD\n","Location: /usr/local/lib/python3.7/dist-packages\n","Requires: numpy, pillow, requests, torch, typing-extensions\n","Required-by: timm\n"]}]},{"cell_type":"code","source":["DATA_DIR, OUT_DIR"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e5RP8rd2X45p","executionInfo":{"status":"ok","timestamp":1686458800540,"user_tz":-180,"elapsed":6,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"cee1ef19-6824-40b4-f9b1-84d4006452cb"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/My Drive/data/project_TransUNet/data/Synapse/',\n"," '/content/drive/My Drive/itu/PhD/BLG 641E - Medical Image Computing/final project/saved/')"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["# Train Model"],"metadata":{"id":"BOXrkuMsxKBX"}},{"cell_type":"code","source":["!python train.py --dataset Synapse --cfg configs/swin_tiny_patch4_window7_224_lite.yaml --root_path \"/content/drive/MyDrive/data/project_TransUNet/data/Synapse\" --max_epochs 150 --output_dir \"/content/drive/My Drive/itu/PhD/BLG 641E - Medical Image Computing/final project/saved/\"  --img_size 224 --base_lr 0.05 --batch_size 24"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yEV6m0ZqMgUD","executionInfo":{"status":"ok","timestamp":1686443210219,"user_tz":-180,"elapsed":11789286,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"e185a78c-5bc0-4cdd-d486-18b14813ed38"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","iteration 8955 : loss : 0.174352, loss_ce: 0.043166\n","iteration 8956 : loss : 0.236971, loss_ce: 0.043098\n","iteration 8957 : loss : 0.251106, loss_ce: 0.052018\n","iteration 8958 : loss : 0.267334, loss_ce: 0.047828\n","iteration 8959 : loss : 0.161929, loss_ce: 0.050304\n","iteration 8960 : loss : 0.204493, loss_ce: 0.062115\n","iteration 8961 : loss : 0.172326, loss_ce: 0.043752\n","iteration 8962 : loss : 0.182896, loss_ce: 0.042966\n","iteration 8963 : loss : 0.197484, loss_ce: 0.039583\n","iteration 8964 : loss : 0.251644, loss_ce: 0.034720\n","iteration 8965 : loss : 0.179002, loss_ce: 0.066760\n","iteration 8966 : loss : 0.196896, loss_ce: 0.083213\n","iteration 8967 : loss : 0.199361, loss_ce: 0.055860\n","iteration 8968 : loss : 0.177158, loss_ce: 0.073642\n","iteration 8969 : loss : 0.186543, loss_ce: 0.061548\n","iteration 8970 : loss : 0.163688, loss_ce: 0.051990\n","iteration 8971 : loss : 0.152932, loss_ce: 0.057079\n","iteration 8972 : loss : 0.196193, loss_ce: 0.059803\n","iteration 8973 : loss : 0.223739, loss_ce: 0.029857\n","iteration 8974 : loss : 0.195438, loss_ce: 0.082625\n","iteration 8975 : loss : 0.191326, loss_ce: 0.031408\n","iteration 8976 : loss : 0.149830, loss_ce: 0.050212\n","iteration 8977 : loss : 0.169706, loss_ce: 0.053725\n","iteration 8978 : loss : 0.167245, loss_ce: 0.068462\n","iteration 8979 : loss : 0.165503, loss_ce: 0.057984\n","iteration 8980 : loss : 0.148925, loss_ce: 0.056396\n","iteration 8981 : loss : 0.272345, loss_ce: 0.044450\n","iteration 8982 : loss : 0.163186, loss_ce: 0.058074\n","iteration 8983 : loss : 0.217294, loss_ce: 0.046330\n","iteration 8984 : loss : 0.174304, loss_ce: 0.042292\n","iteration 8985 : loss : 0.192175, loss_ce: 0.042489\n","iteration 8986 : loss : 0.164475, loss_ce: 0.075559\n","iteration 8987 : loss : 0.247291, loss_ce: 0.046260\n","iteration 8988 : loss : 0.183182, loss_ce: 0.068315\n","iteration 8989 : loss : 0.178485, loss_ce: 0.051540\n","iteration 8990 : loss : 0.229371, loss_ce: 0.076345\n","iteration 8991 : loss : 0.161251, loss_ce: 0.051978\n","iteration 8992 : loss : 0.191939, loss_ce: 0.062167\n","iteration 8993 : loss : 0.175503, loss_ce: 0.059295\n","iteration 8994 : loss : 0.184831, loss_ce: 0.069693\n","iteration 8995 : loss : 0.151382, loss_ce: 0.054082\n","iteration 8996 : loss : 0.199381, loss_ce: 0.056461\n","iteration 8997 : loss : 0.149928, loss_ce: 0.050553\n","iteration 8998 : loss : 0.180378, loss_ce: 0.050944\n","iteration 8999 : loss : 0.162783, loss_ce: 0.062489\n","iteration 9000 : loss : 0.174786, loss_ce: 0.049505\n","iteration 9001 : loss : 0.138621, loss_ce: 0.047895\n","iteration 9002 : loss : 0.180089, loss_ce: 0.059538\n","iteration 9003 : loss : 0.166045, loss_ce: 0.050886\n","iteration 9004 : loss : 0.142343, loss_ce: 0.055743\n","iteration 9005 : loss : 0.182809, loss_ce: 0.054299\n","iteration 9006 : loss : 0.144506, loss_ce: 0.037570\n","iteration 9007 : loss : 0.200602, loss_ce: 0.027463\n","iteration 9008 : loss : 0.128197, loss_ce: 0.037119\n","iteration 9009 : loss : 0.141439, loss_ce: 0.041588\n","iteration 9010 : loss : 0.207930, loss_ce: 0.064566\n","iteration 9011 : loss : 0.207528, loss_ce: 0.062695\n","iteration 9012 : loss : 0.186583, loss_ce: 0.069932\n","iteration 9013 : loss : 0.180302, loss_ce: 0.054469\n","iteration 9014 : loss : 0.181883, loss_ce: 0.068786\n","iteration 9015 : loss : 0.206590, loss_ce: 0.081913\n","iteration 9016 : loss : 0.172912, loss_ce: 0.063996\n","iteration 9017 : loss : 0.295322, loss_ce: 0.071057\n","iteration 9018 : loss : 0.217395, loss_ce: 0.072441\n","iteration 9019 : loss : 0.215835, loss_ce: 0.073668\n","iteration 9020 : loss : 0.157966, loss_ce: 0.058136\n","iteration 9021 : loss : 0.344834, loss_ce: 0.047600\n"," 65%|██████████████████          | 97/150 [2:06:37<1:10:08, 79.41s/it]iteration 9022 : loss : 0.181538, loss_ce: 0.026920\n","iteration 9023 : loss : 0.215474, loss_ce: 0.045408\n","iteration 9024 : loss : 0.190317, loss_ce: 0.058091\n","iteration 9025 : loss : 0.113310, loss_ce: 0.029755\n","iteration 9026 : loss : 0.174887, loss_ce: 0.060740\n","iteration 9027 : loss : 0.137354, loss_ce: 0.039488\n","iteration 9028 : loss : 0.233122, loss_ce: 0.051397\n","iteration 9029 : loss : 0.181457, loss_ce: 0.057524\n","iteration 9030 : loss : 0.166800, loss_ce: 0.032058\n","iteration 9031 : loss : 0.161842, loss_ce: 0.074775\n","iteration 9032 : loss : 0.201147, loss_ce: 0.063403\n","iteration 9033 : loss : 0.189320, loss_ce: 0.051848\n","iteration 9034 : loss : 0.202306, loss_ce: 0.041910\n","iteration 9035 : loss : 0.172545, loss_ce: 0.054707\n","iteration 9036 : loss : 0.122967, loss_ce: 0.051659\n","iteration 9037 : loss : 0.172552, loss_ce: 0.024822\n","iteration 9038 : loss : 0.188508, loss_ce: 0.043641\n","iteration 9039 : loss : 0.184381, loss_ce: 0.069796\n","iteration 9040 : loss : 0.155494, loss_ce: 0.064695\n","iteration 9041 : loss : 0.184736, loss_ce: 0.047834\n","iteration 9042 : loss : 0.147200, loss_ce: 0.049454\n","iteration 9043 : loss : 0.149869, loss_ce: 0.050017\n","iteration 9044 : loss : 0.193635, loss_ce: 0.053911\n","iteration 9045 : loss : 0.174854, loss_ce: 0.050023\n","iteration 9046 : loss : 0.176116, loss_ce: 0.036906\n","iteration 9047 : loss : 0.227062, loss_ce: 0.045612\n","iteration 9048 : loss : 0.211775, loss_ce: 0.070888\n","iteration 9049 : loss : 0.216422, loss_ce: 0.042825\n","iteration 9050 : loss : 0.179247, loss_ce: 0.037341\n","iteration 9051 : loss : 0.141323, loss_ce: 0.051197\n","iteration 9052 : loss : 0.224825, loss_ce: 0.078943\n","iteration 9053 : loss : 0.213182, loss_ce: 0.056246\n","iteration 9054 : loss : 0.154988, loss_ce: 0.040656\n","iteration 9055 : loss : 0.171836, loss_ce: 0.037981\n","iteration 9056 : loss : 0.151837, loss_ce: 0.031492\n","iteration 9057 : loss : 0.212682, loss_ce: 0.052387\n","iteration 9058 : loss : 0.235809, loss_ce: 0.037610\n","iteration 9059 : loss : 0.178688, loss_ce: 0.080483\n","iteration 9060 : loss : 0.138974, loss_ce: 0.037916\n","iteration 9061 : loss : 0.181191, loss_ce: 0.045118\n","iteration 9062 : loss : 0.167854, loss_ce: 0.065695\n","iteration 9063 : loss : 0.148980, loss_ce: 0.050165\n","iteration 9064 : loss : 0.153435, loss_ce: 0.050091\n","iteration 9065 : loss : 0.146480, loss_ce: 0.035781\n","iteration 9066 : loss : 0.174929, loss_ce: 0.053670\n","iteration 9067 : loss : 0.167669, loss_ce: 0.050312\n","iteration 9068 : loss : 0.156968, loss_ce: 0.065796\n","iteration 9069 : loss : 0.221960, loss_ce: 0.052144\n","iteration 9070 : loss : 0.198098, loss_ce: 0.063857\n","iteration 9071 : loss : 0.122824, loss_ce: 0.048423\n","iteration 9072 : loss : 0.139601, loss_ce: 0.035896\n","iteration 9073 : loss : 0.186258, loss_ce: 0.075983\n","iteration 9074 : loss : 0.155907, loss_ce: 0.035736\n","iteration 9075 : loss : 0.165125, loss_ce: 0.054844\n","iteration 9076 : loss : 0.189063, loss_ce: 0.046330\n","iteration 9077 : loss : 0.163592, loss_ce: 0.040993\n","iteration 9078 : loss : 0.241195, loss_ce: 0.054136\n","iteration 9079 : loss : 0.165031, loss_ce: 0.071449\n","iteration 9080 : loss : 0.177110, loss_ce: 0.042033\n","iteration 9081 : loss : 0.182125, loss_ce: 0.078024\n","iteration 9082 : loss : 0.184447, loss_ce: 0.043585\n","iteration 9083 : loss : 0.211038, loss_ce: 0.064059\n","iteration 9084 : loss : 0.130934, loss_ce: 0.025853\n","iteration 9085 : loss : 0.197902, loss_ce: 0.047571\n","iteration 9086 : loss : 0.233704, loss_ce: 0.084385\n","iteration 9087 : loss : 0.255257, loss_ce: 0.059995\n","iteration 9088 : loss : 0.144179, loss_ce: 0.055299\n","iteration 9089 : loss : 0.131205, loss_ce: 0.037595\n","iteration 9090 : loss : 0.182784, loss_ce: 0.053950\n","iteration 9091 : loss : 0.159960, loss_ce: 0.036679\n","iteration 9092 : loss : 0.201809, loss_ce: 0.050325\n","iteration 9093 : loss : 0.161757, loss_ce: 0.054295\n","iteration 9094 : loss : 0.217984, loss_ce: 0.075410\n","iteration 9095 : loss : 0.158435, loss_ce: 0.074383\n","iteration 9096 : loss : 0.152529, loss_ce: 0.037571\n","iteration 9097 : loss : 0.139385, loss_ce: 0.040972\n","iteration 9098 : loss : 0.227866, loss_ce: 0.047951\n","iteration 9099 : loss : 0.185779, loss_ce: 0.062410\n","iteration 9100 : loss : 0.223038, loss_ce: 0.053913\n","iteration 9101 : loss : 0.172302, loss_ce: 0.037193\n","iteration 9102 : loss : 0.157940, loss_ce: 0.063886\n","iteration 9103 : loss : 0.180899, loss_ce: 0.038441\n","iteration 9104 : loss : 0.186993, loss_ce: 0.062152\n","iteration 9105 : loss : 0.142416, loss_ce: 0.044064\n","iteration 9106 : loss : 0.204538, loss_ce: 0.035245\n","iteration 9107 : loss : 0.187905, loss_ce: 0.063021\n","iteration 9108 : loss : 0.163026, loss_ce: 0.055331\n","iteration 9109 : loss : 0.175565, loss_ce: 0.041675\n","iteration 9110 : loss : 0.241349, loss_ce: 0.045755\n","iteration 9111 : loss : 0.144200, loss_ce: 0.056372\n","iteration 9112 : loss : 0.161574, loss_ce: 0.057647\n","iteration 9113 : loss : 0.168069, loss_ce: 0.049500\n","iteration 9114 : loss : 0.536643, loss_ce: 0.008150\n"," 65%|██████████████████▎         | 98/150 [2:07:57<1:08:57, 79.57s/it]iteration 9115 : loss : 0.225827, loss_ce: 0.023549\n","iteration 9116 : loss : 0.178744, loss_ce: 0.053508\n","iteration 9117 : loss : 0.214533, loss_ce: 0.026004\n","iteration 9118 : loss : 0.143584, loss_ce: 0.044782\n","iteration 9119 : loss : 0.119580, loss_ce: 0.030402\n","iteration 9120 : loss : 0.164624, loss_ce: 0.047217\n","iteration 9121 : loss : 0.208772, loss_ce: 0.061637\n","iteration 9122 : loss : 0.194136, loss_ce: 0.040837\n","iteration 9123 : loss : 0.194465, loss_ce: 0.038682\n","iteration 9124 : loss : 0.164236, loss_ce: 0.065644\n","iteration 9125 : loss : 0.231452, loss_ce: 0.054692\n","iteration 9126 : loss : 0.179322, loss_ce: 0.064098\n","iteration 9127 : loss : 0.189275, loss_ce: 0.028309\n","iteration 9128 : loss : 0.170835, loss_ce: 0.051656\n","iteration 9129 : loss : 0.210886, loss_ce: 0.039857\n","iteration 9130 : loss : 0.160861, loss_ce: 0.045801\n","iteration 9131 : loss : 0.132829, loss_ce: 0.033556\n","iteration 9132 : loss : 0.260723, loss_ce: 0.056078\n","iteration 9133 : loss : 0.132815, loss_ce: 0.043743\n","iteration 9134 : loss : 0.193511, loss_ce: 0.045217\n","iteration 9135 : loss : 0.177308, loss_ce: 0.026961\n","iteration 9136 : loss : 0.162051, loss_ce: 0.056016\n","iteration 9137 : loss : 0.171889, loss_ce: 0.041149\n","iteration 9138 : loss : 0.206077, loss_ce: 0.063252\n","iteration 9139 : loss : 0.173872, loss_ce: 0.052400\n","iteration 9140 : loss : 0.182829, loss_ce: 0.061446\n","iteration 9141 : loss : 0.175109, loss_ce: 0.059432\n","iteration 9142 : loss : 0.138180, loss_ce: 0.036791\n","iteration 9143 : loss : 0.169668, loss_ce: 0.042069\n","iteration 9144 : loss : 0.170669, loss_ce: 0.070869\n","iteration 9145 : loss : 0.174445, loss_ce: 0.041327\n","iteration 9146 : loss : 0.232023, loss_ce: 0.093472\n","iteration 9147 : loss : 0.118934, loss_ce: 0.043010\n","iteration 9148 : loss : 0.168455, loss_ce: 0.062251\n","iteration 9149 : loss : 0.148126, loss_ce: 0.030335\n","iteration 9150 : loss : 0.219394, loss_ce: 0.033364\n","iteration 9151 : loss : 0.170614, loss_ce: 0.045653\n","iteration 9152 : loss : 0.226326, loss_ce: 0.045529\n","iteration 9153 : loss : 0.174092, loss_ce: 0.061008\n","iteration 9154 : loss : 0.142521, loss_ce: 0.036489\n","iteration 9155 : loss : 0.144539, loss_ce: 0.054700\n","iteration 9156 : loss : 0.161020, loss_ce: 0.059156\n","iteration 9157 : loss : 0.276302, loss_ce: 0.065294\n","iteration 9158 : loss : 0.145045, loss_ce: 0.043015\n","iteration 9159 : loss : 0.191848, loss_ce: 0.075924\n","iteration 9160 : loss : 0.171060, loss_ce: 0.050238\n","iteration 9161 : loss : 0.177097, loss_ce: 0.055129\n","iteration 9162 : loss : 0.139838, loss_ce: 0.045163\n","iteration 9163 : loss : 0.172963, loss_ce: 0.051889\n","iteration 9164 : loss : 0.195629, loss_ce: 0.069524\n","iteration 9165 : loss : 0.171617, loss_ce: 0.071484\n","iteration 9166 : loss : 0.145961, loss_ce: 0.052425\n","iteration 9167 : loss : 0.136381, loss_ce: 0.036202\n","iteration 9168 : loss : 0.139770, loss_ce: 0.043038\n","iteration 9169 : loss : 0.150879, loss_ce: 0.057548\n","iteration 9170 : loss : 0.215725, loss_ce: 0.058324\n","iteration 9171 : loss : 0.196225, loss_ce: 0.049504\n","iteration 9172 : loss : 0.195897, loss_ce: 0.066970\n","iteration 9173 : loss : 0.230358, loss_ce: 0.069592\n","iteration 9174 : loss : 0.158750, loss_ce: 0.043840\n","iteration 9175 : loss : 0.152629, loss_ce: 0.038117\n","iteration 9176 : loss : 0.239168, loss_ce: 0.038317\n","iteration 9177 : loss : 0.176675, loss_ce: 0.077422\n","iteration 9178 : loss : 0.133082, loss_ce: 0.027592\n","iteration 9179 : loss : 0.182286, loss_ce: 0.038506\n","iteration 9180 : loss : 0.189885, loss_ce: 0.050498\n","iteration 9181 : loss : 0.179311, loss_ce: 0.080689\n","iteration 9182 : loss : 0.138131, loss_ce: 0.050403\n","iteration 9183 : loss : 0.176688, loss_ce: 0.051999\n","iteration 9184 : loss : 0.150149, loss_ce: 0.057081\n","iteration 9185 : loss : 0.142178, loss_ce: 0.051240\n","iteration 9186 : loss : 0.147764, loss_ce: 0.047586\n","iteration 9187 : loss : 0.196023, loss_ce: 0.065494\n","iteration 9188 : loss : 0.204570, loss_ce: 0.038923\n","iteration 9189 : loss : 0.168714, loss_ce: 0.050882\n","iteration 9190 : loss : 0.122041, loss_ce: 0.048576\n","iteration 9191 : loss : 0.158753, loss_ce: 0.069535\n","iteration 9192 : loss : 0.238216, loss_ce: 0.050935\n","iteration 9193 : loss : 0.220359, loss_ce: 0.052758\n","iteration 9194 : loss : 0.139967, loss_ce: 0.038864\n","iteration 9195 : loss : 0.127846, loss_ce: 0.024752\n","iteration 9196 : loss : 0.213721, loss_ce: 0.045320\n","iteration 9197 : loss : 0.158855, loss_ce: 0.038524\n","iteration 9198 : loss : 0.157859, loss_ce: 0.051629\n","iteration 9199 : loss : 0.165534, loss_ce: 0.055828\n","iteration 9200 : loss : 0.158658, loss_ce: 0.045284\n","iteration 9201 : loss : 0.188149, loss_ce: 0.045700\n","iteration 9202 : loss : 0.141724, loss_ce: 0.034555\n","iteration 9203 : loss : 0.156106, loss_ce: 0.036453\n","iteration 9204 : loss : 0.284546, loss_ce: 0.045109\n","iteration 9205 : loss : 0.137398, loss_ce: 0.051814\n","iteration 9206 : loss : 0.126273, loss_ce: 0.021889\n","iteration 9207 : loss : 0.185611, loss_ce: 0.043941\n"," 66%|██████████████████▍         | 99/150 [2:09:12<1:06:26, 78.16s/it]iteration 9208 : loss : 0.170414, loss_ce: 0.072666\n","iteration 9209 : loss : 0.146521, loss_ce: 0.026969\n","iteration 9210 : loss : 0.154264, loss_ce: 0.062186\n","iteration 9211 : loss : 0.134350, loss_ce: 0.045529\n","iteration 9212 : loss : 0.164064, loss_ce: 0.030739\n","iteration 9213 : loss : 0.165430, loss_ce: 0.055546\n","iteration 9214 : loss : 0.204032, loss_ce: 0.071658\n","iteration 9215 : loss : 0.186133, loss_ce: 0.058928\n","iteration 9216 : loss : 0.153680, loss_ce: 0.049209\n","iteration 9217 : loss : 0.202544, loss_ce: 0.046253\n","iteration 9218 : loss : 0.197117, loss_ce: 0.040533\n","iteration 9219 : loss : 0.119950, loss_ce: 0.028353\n","iteration 9220 : loss : 0.166356, loss_ce: 0.056859\n","iteration 9221 : loss : 0.201616, loss_ce: 0.036751\n","iteration 9222 : loss : 0.125929, loss_ce: 0.018461\n","iteration 9223 : loss : 0.176061, loss_ce: 0.043332\n","iteration 9224 : loss : 0.139332, loss_ce: 0.065338\n","iteration 9225 : loss : 0.172929, loss_ce: 0.079007\n","iteration 9226 : loss : 0.263185, loss_ce: 0.027020\n","iteration 9227 : loss : 0.136891, loss_ce: 0.050757\n","iteration 9228 : loss : 0.129182, loss_ce: 0.035020\n","iteration 9229 : loss : 0.111151, loss_ce: 0.031831\n","iteration 9230 : loss : 0.191963, loss_ce: 0.072789\n","iteration 9231 : loss : 0.158082, loss_ce: 0.074417\n","iteration 9232 : loss : 0.214384, loss_ce: 0.059966\n","iteration 9233 : loss : 0.186019, loss_ce: 0.040081\n","iteration 9234 : loss : 0.201768, loss_ce: 0.028241\n","iteration 9235 : loss : 0.147929, loss_ce: 0.044843\n","iteration 9236 : loss : 0.201506, loss_ce: 0.033709\n","iteration 9237 : loss : 0.159939, loss_ce: 0.051898\n","iteration 9238 : loss : 0.145583, loss_ce: 0.033541\n","iteration 9239 : loss : 0.181441, loss_ce: 0.039098\n","iteration 9240 : loss : 0.159479, loss_ce: 0.061002\n","iteration 9241 : loss : 0.159819, loss_ce: 0.054799\n","iteration 9242 : loss : 0.175407, loss_ce: 0.063015\n","iteration 9243 : loss : 0.141018, loss_ce: 0.053166\n","iteration 9244 : loss : 0.159302, loss_ce: 0.063395\n","iteration 9245 : loss : 0.201033, loss_ce: 0.050491\n","iteration 9246 : loss : 0.143199, loss_ce: 0.039836\n","iteration 9247 : loss : 0.203318, loss_ce: 0.063624\n","iteration 9248 : loss : 0.140642, loss_ce: 0.060054\n","iteration 9249 : loss : 0.127032, loss_ce: 0.040423\n","iteration 9250 : loss : 0.153929, loss_ce: 0.034365\n","iteration 9251 : loss : 0.169958, loss_ce: 0.047538\n","iteration 9252 : loss : 0.137960, loss_ce: 0.075134\n","iteration 9253 : loss : 0.199909, loss_ce: 0.037207\n","iteration 9254 : loss : 0.265034, loss_ce: 0.050726\n","iteration 9255 : loss : 0.154930, loss_ce: 0.052726\n","iteration 9256 : loss : 0.178838, loss_ce: 0.030453\n","iteration 9257 : loss : 0.228259, loss_ce: 0.097447\n","iteration 9258 : loss : 0.171085, loss_ce: 0.065001\n","iteration 9259 : loss : 0.161801, loss_ce: 0.053477\n","iteration 9260 : loss : 0.189626, loss_ce: 0.028705\n","iteration 9261 : loss : 0.187850, loss_ce: 0.031504\n","iteration 9262 : loss : 0.162025, loss_ce: 0.047856\n","iteration 9263 : loss : 0.291322, loss_ce: 0.025609\n","iteration 9264 : loss : 0.184487, loss_ce: 0.036036\n","iteration 9265 : loss : 0.181580, loss_ce: 0.025378\n","iteration 9266 : loss : 0.204929, loss_ce: 0.051726\n","iteration 9267 : loss : 0.163040, loss_ce: 0.059665\n","iteration 9268 : loss : 0.179376, loss_ce: 0.039127\n","iteration 9269 : loss : 0.164921, loss_ce: 0.057794\n","iteration 9270 : loss : 0.189820, loss_ce: 0.049510\n","iteration 9271 : loss : 0.178789, loss_ce: 0.043900\n","iteration 9272 : loss : 0.192846, loss_ce: 0.067834\n","iteration 9273 : loss : 0.199162, loss_ce: 0.058317\n","iteration 9274 : loss : 0.203939, loss_ce: 0.031830\n","iteration 9275 : loss : 0.206168, loss_ce: 0.051203\n","iteration 9276 : loss : 0.207118, loss_ce: 0.039677\n","iteration 9277 : loss : 0.237077, loss_ce: 0.041789\n","iteration 9278 : loss : 0.159883, loss_ce: 0.048201\n","iteration 9279 : loss : 0.139207, loss_ce: 0.039988\n","iteration 9280 : loss : 0.123613, loss_ce: 0.045251\n","iteration 9281 : loss : 0.156378, loss_ce: 0.057998\n","iteration 9282 : loss : 0.151283, loss_ce: 0.054527\n","iteration 9283 : loss : 0.133299, loss_ce: 0.032838\n","iteration 9284 : loss : 0.167707, loss_ce: 0.044853\n","iteration 9285 : loss : 0.162860, loss_ce: 0.041241\n","iteration 9286 : loss : 0.148644, loss_ce: 0.055995\n","iteration 9287 : loss : 0.178995, loss_ce: 0.027361\n","iteration 9288 : loss : 0.184223, loss_ce: 0.055636\n","iteration 9289 : loss : 0.148384, loss_ce: 0.047257\n","iteration 9290 : loss : 0.128810, loss_ce: 0.053430\n","iteration 9291 : loss : 0.260277, loss_ce: 0.066884\n","iteration 9292 : loss : 0.132283, loss_ce: 0.049158\n","iteration 9293 : loss : 0.161416, loss_ce: 0.042398\n","iteration 9294 : loss : 0.230683, loss_ce: 0.076772\n","iteration 9295 : loss : 0.225861, loss_ce: 0.050531\n","iteration 9296 : loss : 0.160201, loss_ce: 0.062155\n","iteration 9297 : loss : 0.163459, loss_ce: 0.056177\n","iteration 9298 : loss : 0.154447, loss_ce: 0.053104\n","iteration 9299 : loss : 0.182602, loss_ce: 0.082222\n","iteration 9300 : loss : 0.462837, loss_ce: 0.023819\n","save model to /content/drive/My Drive/itu/PhD/BLG 641E - Medical Image Computing/final project/saved/epoch_99.pth\n"," 67%|██████████████████         | 100/150 [2:10:33<1:05:41, 78.83s/it]iteration 9301 : loss : 0.250353, loss_ce: 0.035790\n","iteration 9302 : loss : 0.162563, loss_ce: 0.065077\n","iteration 9303 : loss : 0.167204, loss_ce: 0.061670\n","iteration 9304 : loss : 0.272550, loss_ce: 0.042104\n","iteration 9305 : loss : 0.129615, loss_ce: 0.032727\n","iteration 9306 : loss : 0.140026, loss_ce: 0.052185\n","iteration 9307 : loss : 0.222011, loss_ce: 0.063397\n","iteration 9308 : loss : 0.183999, loss_ce: 0.049132\n","iteration 9309 : loss : 0.107722, loss_ce: 0.021106\n","iteration 9310 : loss : 0.173136, loss_ce: 0.043554\n","iteration 9311 : loss : 0.194755, loss_ce: 0.041565\n","iteration 9312 : loss : 0.259307, loss_ce: 0.054472\n","iteration 9313 : loss : 0.170563, loss_ce: 0.041006\n","iteration 9314 : loss : 0.145705, loss_ce: 0.063790\n","iteration 9315 : loss : 0.154192, loss_ce: 0.039146\n","iteration 9316 : loss : 0.170271, loss_ce: 0.041341\n","iteration 9317 : loss : 0.226699, loss_ce: 0.043892\n","iteration 9318 : loss : 0.145647, loss_ce: 0.034478\n","iteration 9319 : loss : 0.202748, loss_ce: 0.081952\n","iteration 9320 : loss : 0.144738, loss_ce: 0.048053\n","iteration 9321 : loss : 0.131755, loss_ce: 0.042971\n","iteration 9322 : loss : 0.165804, loss_ce: 0.033456\n","iteration 9323 : loss : 0.169041, loss_ce: 0.068426\n","iteration 9324 : loss : 0.184472, loss_ce: 0.049645\n","iteration 9325 : loss : 0.139208, loss_ce: 0.039025\n","iteration 9326 : loss : 0.163067, loss_ce: 0.052317\n","iteration 9327 : loss : 0.128138, loss_ce: 0.043842\n","iteration 9328 : loss : 0.189072, loss_ce: 0.060657\n","iteration 9329 : loss : 0.184813, loss_ce: 0.038296\n","iteration 9330 : loss : 0.128837, loss_ce: 0.041665\n","iteration 9331 : loss : 0.161792, loss_ce: 0.056683\n","iteration 9332 : loss : 0.155815, loss_ce: 0.038256\n","iteration 9333 : loss : 0.121111, loss_ce: 0.024177\n","iteration 9334 : loss : 0.168563, loss_ce: 0.037517\n","iteration 9335 : loss : 0.179110, loss_ce: 0.046074\n","iteration 9336 : loss : 0.167226, loss_ce: 0.039361\n","iteration 9337 : loss : 0.187878, loss_ce: 0.030378\n","iteration 9338 : loss : 0.208806, loss_ce: 0.027177\n","iteration 9339 : loss : 0.177909, loss_ce: 0.057161\n","iteration 9340 : loss : 0.227077, loss_ce: 0.041053\n","iteration 9341 : loss : 0.142285, loss_ce: 0.064751\n","iteration 9342 : loss : 0.220770, loss_ce: 0.031679\n","iteration 9343 : loss : 0.175350, loss_ce: 0.050833\n","iteration 9344 : loss : 0.146232, loss_ce: 0.044634\n","iteration 9345 : loss : 0.203468, loss_ce: 0.030065\n","iteration 9346 : loss : 0.234293, loss_ce: 0.062813\n","iteration 9347 : loss : 0.137515, loss_ce: 0.058963\n","iteration 9348 : loss : 0.197368, loss_ce: 0.034768\n","iteration 9349 : loss : 0.169170, loss_ce: 0.042784\n","iteration 9350 : loss : 0.145975, loss_ce: 0.035581\n","iteration 9351 : loss : 0.145786, loss_ce: 0.032619\n","iteration 9352 : loss : 0.156858, loss_ce: 0.055038\n","iteration 9353 : loss : 0.138150, loss_ce: 0.048518\n","iteration 9354 : loss : 0.219054, loss_ce: 0.060371\n","iteration 9355 : loss : 0.180950, loss_ce: 0.080964\n","iteration 9356 : loss : 0.174217, loss_ce: 0.065831\n","iteration 9357 : loss : 0.204906, loss_ce: 0.027335\n","iteration 9358 : loss : 0.167442, loss_ce: 0.047893\n","iteration 9359 : loss : 0.146119, loss_ce: 0.069739\n","iteration 9360 : loss : 0.129391, loss_ce: 0.031580\n","iteration 9361 : loss : 0.218189, loss_ce: 0.041788\n","iteration 9362 : loss : 0.159061, loss_ce: 0.039507\n","iteration 9363 : loss : 0.189433, loss_ce: 0.045365\n","iteration 9364 : loss : 0.171048, loss_ce: 0.048310\n","iteration 9365 : loss : 0.165669, loss_ce: 0.029408\n","iteration 9366 : loss : 0.131589, loss_ce: 0.046573\n","iteration 9367 : loss : 0.156225, loss_ce: 0.063109\n","iteration 9368 : loss : 0.151060, loss_ce: 0.038486\n","iteration 9369 : loss : 0.127001, loss_ce: 0.037269\n","iteration 9370 : loss : 0.253460, loss_ce: 0.062320\n","iteration 9371 : loss : 0.161494, loss_ce: 0.045059\n","iteration 9372 : loss : 0.210754, loss_ce: 0.062661\n","iteration 9373 : loss : 0.154033, loss_ce: 0.038094\n","iteration 9374 : loss : 0.227235, loss_ce: 0.061527\n","iteration 9375 : loss : 0.147296, loss_ce: 0.033778\n","iteration 9376 : loss : 0.154873, loss_ce: 0.053197\n","iteration 9377 : loss : 0.200346, loss_ce: 0.047842\n","iteration 9378 : loss : 0.163682, loss_ce: 0.033103\n","iteration 9379 : loss : 0.221098, loss_ce: 0.098204\n","iteration 9380 : loss : 0.160375, loss_ce: 0.049115\n","iteration 9381 : loss : 0.200357, loss_ce: 0.076305\n","iteration 9382 : loss : 0.151043, loss_ce: 0.048018\n","iteration 9383 : loss : 0.267724, loss_ce: 0.044816\n","iteration 9384 : loss : 0.168580, loss_ce: 0.044414\n","iteration 9385 : loss : 0.161530, loss_ce: 0.048317\n","iteration 9386 : loss : 0.185902, loss_ce: 0.083560\n","iteration 9387 : loss : 0.153587, loss_ce: 0.069232\n","iteration 9388 : loss : 0.164036, loss_ce: 0.047877\n","iteration 9389 : loss : 0.210495, loss_ce: 0.060775\n","iteration 9390 : loss : 0.180709, loss_ce: 0.059814\n","iteration 9391 : loss : 0.183983, loss_ce: 0.071807\n","iteration 9392 : loss : 0.131778, loss_ce: 0.038478\n","iteration 9393 : loss : 0.476224, loss_ce: 0.004390\n"," 67%|██████████████████▏        | 101/150 [2:11:53<1:04:50, 79.40s/it]iteration 9394 : loss : 0.170500, loss_ce: 0.054628\n","iteration 9395 : loss : 0.196851, loss_ce: 0.060221\n","iteration 9396 : loss : 0.157442, loss_ce: 0.054075\n","iteration 9397 : loss : 0.167373, loss_ce: 0.054325\n","iteration 9398 : loss : 0.217732, loss_ce: 0.044636\n","iteration 9399 : loss : 0.149778, loss_ce: 0.053194\n","iteration 9400 : loss : 0.131857, loss_ce: 0.041980\n","iteration 9401 : loss : 0.138504, loss_ce: 0.036777\n","iteration 9402 : loss : 0.208943, loss_ce: 0.055659\n","iteration 9403 : loss : 0.139400, loss_ce: 0.055347\n","iteration 9404 : loss : 0.147684, loss_ce: 0.060322\n","iteration 9405 : loss : 0.123393, loss_ce: 0.040756\n","iteration 9406 : loss : 0.170139, loss_ce: 0.050932\n","iteration 9407 : loss : 0.191806, loss_ce: 0.039402\n","iteration 9408 : loss : 0.094484, loss_ce: 0.031507\n","iteration 9409 : loss : 0.136922, loss_ce: 0.043700\n","iteration 9410 : loss : 0.147964, loss_ce: 0.048733\n","iteration 9411 : loss : 0.205828, loss_ce: 0.064948\n","iteration 9412 : loss : 0.162218, loss_ce: 0.061010\n","iteration 9413 : loss : 0.179686, loss_ce: 0.033562\n","iteration 9414 : loss : 0.143041, loss_ce: 0.055280\n","iteration 9415 : loss : 0.169815, loss_ce: 0.066473\n","iteration 9416 : loss : 0.158365, loss_ce: 0.059367\n","iteration 9417 : loss : 0.182976, loss_ce: 0.062888\n","iteration 9418 : loss : 0.162580, loss_ce: 0.045043\n","iteration 9419 : loss : 0.161232, loss_ce: 0.043886\n","iteration 9420 : loss : 0.170435, loss_ce: 0.032163\n","iteration 9421 : loss : 0.165524, loss_ce: 0.049072\n","iteration 9422 : loss : 0.133694, loss_ce: 0.042058\n","iteration 9423 : loss : 0.184601, loss_ce: 0.087276\n","iteration 9424 : loss : 0.152706, loss_ce: 0.036179\n","iteration 9425 : loss : 0.181307, loss_ce: 0.031613\n","iteration 9426 : loss : 0.140738, loss_ce: 0.031522\n","iteration 9427 : loss : 0.146740, loss_ce: 0.052334\n","iteration 9428 : loss : 0.228765, loss_ce: 0.050696\n","iteration 9429 : loss : 0.216879, loss_ce: 0.077109\n","iteration 9430 : loss : 0.242007, loss_ce: 0.066343\n","iteration 9431 : loss : 0.137099, loss_ce: 0.035429\n","iteration 9432 : loss : 0.132329, loss_ce: 0.049810\n","iteration 9433 : loss : 0.200451, loss_ce: 0.051208\n","iteration 9434 : loss : 0.178857, loss_ce: 0.054139\n","iteration 9435 : loss : 0.181745, loss_ce: 0.054784\n","iteration 9436 : loss : 0.205040, loss_ce: 0.046265\n","iteration 9437 : loss : 0.169844, loss_ce: 0.043424\n","iteration 9438 : loss : 0.151277, loss_ce: 0.049181\n","iteration 9439 : loss : 0.222959, loss_ce: 0.033614\n","iteration 9440 : loss : 0.138841, loss_ce: 0.035225\n","iteration 9441 : loss : 0.154316, loss_ce: 0.036579\n","iteration 9442 : loss : 0.205596, loss_ce: 0.049835\n","iteration 9443 : loss : 0.179243, loss_ce: 0.073569\n","iteration 9444 : loss : 0.153577, loss_ce: 0.071520\n","iteration 9445 : loss : 0.170376, loss_ce: 0.065705\n","iteration 9446 : loss : 0.138052, loss_ce: 0.049457\n","iteration 9447 : loss : 0.154762, loss_ce: 0.055189\n","iteration 9448 : loss : 0.125010, loss_ce: 0.053018\n","iteration 9449 : loss : 0.213528, loss_ce: 0.037516\n","iteration 9450 : loss : 0.199514, loss_ce: 0.076899\n","iteration 9451 : loss : 0.151820, loss_ce: 0.048465\n","iteration 9452 : loss : 0.135672, loss_ce: 0.042347\n","iteration 9453 : loss : 0.164418, loss_ce: 0.066280\n","iteration 9454 : loss : 0.177944, loss_ce: 0.047538\n","iteration 9455 : loss : 0.124833, loss_ce: 0.036362\n","iteration 9456 : loss : 0.226201, loss_ce: 0.056282\n","iteration 9457 : loss : 0.158661, loss_ce: 0.049295\n","iteration 9458 : loss : 0.176847, loss_ce: 0.046557\n","iteration 9459 : loss : 0.154071, loss_ce: 0.045917\n","iteration 9460 : loss : 0.130707, loss_ce: 0.056930\n","iteration 9461 : loss : 0.176031, loss_ce: 0.040170\n","iteration 9462 : loss : 0.204377, loss_ce: 0.032359\n","iteration 9463 : loss : 0.188337, loss_ce: 0.023006\n","iteration 9464 : loss : 0.192562, loss_ce: 0.038317\n","iteration 9465 : loss : 0.245616, loss_ce: 0.036318\n","iteration 9466 : loss : 0.151213, loss_ce: 0.044270\n","iteration 9467 : loss : 0.168548, loss_ce: 0.038283\n","iteration 9468 : loss : 0.177077, loss_ce: 0.039961\n","iteration 9469 : loss : 0.181577, loss_ce: 0.040466\n","iteration 9470 : loss : 0.156097, loss_ce: 0.038551\n","iteration 9471 : loss : 0.141005, loss_ce: 0.042570\n","iteration 9472 : loss : 0.157838, loss_ce: 0.048706\n","iteration 9473 : loss : 0.145336, loss_ce: 0.038118\n","iteration 9474 : loss : 0.185124, loss_ce: 0.024493\n","iteration 9475 : loss : 0.103375, loss_ce: 0.025962\n","iteration 9476 : loss : 0.129830, loss_ce: 0.035253\n","iteration 9477 : loss : 0.208884, loss_ce: 0.026450\n","iteration 9478 : loss : 0.144334, loss_ce: 0.034207\n","iteration 9479 : loss : 0.148655, loss_ce: 0.049379\n","iteration 9480 : loss : 0.197831, loss_ce: 0.045893\n","iteration 9481 : loss : 0.153998, loss_ce: 0.043577\n","iteration 9482 : loss : 0.189673, loss_ce: 0.049615\n","iteration 9483 : loss : 0.152232, loss_ce: 0.044627\n","iteration 9484 : loss : 0.136785, loss_ce: 0.029027\n","iteration 9485 : loss : 0.134548, loss_ce: 0.036186\n","iteration 9486 : loss : 0.294819, loss_ce: 0.124128\n"," 68%|██████████████████▎        | 102/150 [2:13:09<1:02:30, 78.14s/it]iteration 9487 : loss : 0.114536, loss_ce: 0.025072\n","iteration 9488 : loss : 0.208349, loss_ce: 0.037471\n","iteration 9489 : loss : 0.199922, loss_ce: 0.038126\n","iteration 9490 : loss : 0.190898, loss_ce: 0.074779\n","iteration 9491 : loss : 0.166631, loss_ce: 0.037718\n","iteration 9492 : loss : 0.138232, loss_ce: 0.059616\n","iteration 9493 : loss : 0.142361, loss_ce: 0.035788\n","iteration 9494 : loss : 0.134540, loss_ce: 0.057747\n","iteration 9495 : loss : 0.186423, loss_ce: 0.070493\n","iteration 9496 : loss : 0.150888, loss_ce: 0.038040\n","iteration 9497 : loss : 0.140248, loss_ce: 0.038695\n","iteration 9498 : loss : 0.121629, loss_ce: 0.033233\n","iteration 9499 : loss : 0.200983, loss_ce: 0.072695\n","iteration 9500 : loss : 0.176848, loss_ce: 0.053790\n","iteration 9501 : loss : 0.157598, loss_ce: 0.039281\n","iteration 9502 : loss : 0.151421, loss_ce: 0.047316\n","iteration 9503 : loss : 0.138108, loss_ce: 0.038996\n","iteration 9504 : loss : 0.160259, loss_ce: 0.034695\n","iteration 9505 : loss : 0.186955, loss_ce: 0.042791\n","iteration 9506 : loss : 0.133812, loss_ce: 0.064502\n","iteration 9507 : loss : 0.146933, loss_ce: 0.045749\n","iteration 9508 : loss : 0.200237, loss_ce: 0.031334\n","iteration 9509 : loss : 0.138398, loss_ce: 0.054756\n","iteration 9510 : loss : 0.195086, loss_ce: 0.055041\n","iteration 9511 : loss : 0.160737, loss_ce: 0.041522\n","iteration 9512 : loss : 0.135933, loss_ce: 0.046118\n","iteration 9513 : loss : 0.126059, loss_ce: 0.046925\n","iteration 9514 : loss : 0.139357, loss_ce: 0.042398\n","iteration 9515 : loss : 0.209974, loss_ce: 0.036433\n","iteration 9516 : loss : 0.167434, loss_ce: 0.035861\n","iteration 9517 : loss : 0.114645, loss_ce: 0.040735\n","iteration 9518 : loss : 0.126330, loss_ce: 0.049621\n","iteration 9519 : loss : 0.134940, loss_ce: 0.040645\n","iteration 9520 : loss : 0.138510, loss_ce: 0.066158\n","iteration 9521 : loss : 0.172075, loss_ce: 0.049476\n","iteration 9522 : loss : 0.170408, loss_ce: 0.072310\n","iteration 9523 : loss : 0.158844, loss_ce: 0.072103\n","iteration 9524 : loss : 0.111798, loss_ce: 0.023414\n","iteration 9525 : loss : 0.191687, loss_ce: 0.068860\n","iteration 9526 : loss : 0.149812, loss_ce: 0.037044\n","iteration 9527 : loss : 0.154434, loss_ce: 0.032507\n","iteration 9528 : loss : 0.156604, loss_ce: 0.046398\n","iteration 9529 : loss : 0.182796, loss_ce: 0.033998\n","iteration 9530 : loss : 0.163747, loss_ce: 0.033784\n","iteration 9531 : loss : 0.149319, loss_ce: 0.056399\n","iteration 9532 : loss : 0.180676, loss_ce: 0.055427\n","iteration 9533 : loss : 0.125259, loss_ce: 0.031553\n","iteration 9534 : loss : 0.199054, loss_ce: 0.014292\n","iteration 9535 : loss : 0.147703, loss_ce: 0.047913\n","iteration 9536 : loss : 0.200374, loss_ce: 0.042633\n","iteration 9537 : loss : 0.195564, loss_ce: 0.054546\n","iteration 9538 : loss : 0.101369, loss_ce: 0.027857\n","iteration 9539 : loss : 0.177403, loss_ce: 0.034392\n","iteration 9540 : loss : 0.145788, loss_ce: 0.033850\n","iteration 9541 : loss : 0.169454, loss_ce: 0.031169\n","iteration 9542 : loss : 0.115238, loss_ce: 0.029944\n","iteration 9543 : loss : 0.153987, loss_ce: 0.043919\n","iteration 9544 : loss : 0.176617, loss_ce: 0.042722\n","iteration 9545 : loss : 0.178573, loss_ce: 0.069102\n","iteration 9546 : loss : 0.166664, loss_ce: 0.051629\n","iteration 9547 : loss : 0.151051, loss_ce: 0.039360\n","iteration 9548 : loss : 0.154897, loss_ce: 0.047694\n","iteration 9549 : loss : 0.160256, loss_ce: 0.051353\n","iteration 9550 : loss : 0.203139, loss_ce: 0.029640\n","iteration 9551 : loss : 0.215927, loss_ce: 0.070359\n","iteration 9552 : loss : 0.155402, loss_ce: 0.065927\n","iteration 9553 : loss : 0.151519, loss_ce: 0.059935\n","iteration 9554 : loss : 0.196607, loss_ce: 0.047083\n","iteration 9555 : loss : 0.164840, loss_ce: 0.047580\n","iteration 9556 : loss : 0.121226, loss_ce: 0.024453\n","iteration 9557 : loss : 0.144911, loss_ce: 0.047711\n","iteration 9558 : loss : 0.166942, loss_ce: 0.040234\n","iteration 9559 : loss : 0.157134, loss_ce: 0.045049\n","iteration 9560 : loss : 0.154135, loss_ce: 0.037110\n","iteration 9561 : loss : 0.179916, loss_ce: 0.034036\n","iteration 9562 : loss : 0.203103, loss_ce: 0.072517\n","iteration 9563 : loss : 0.172610, loss_ce: 0.045526\n","iteration 9564 : loss : 0.132964, loss_ce: 0.060708\n","iteration 9565 : loss : 0.193603, loss_ce: 0.045034\n","iteration 9566 : loss : 0.156010, loss_ce: 0.062359\n","iteration 9567 : loss : 0.186680, loss_ce: 0.045037\n","iteration 9568 : loss : 0.130072, loss_ce: 0.045406\n","iteration 9569 : loss : 0.145011, loss_ce: 0.043019\n","iteration 9570 : loss : 0.178559, loss_ce: 0.040085\n","iteration 9571 : loss : 0.164753, loss_ce: 0.059393\n","iteration 9572 : loss : 0.213513, loss_ce: 0.068119\n","iteration 9573 : loss : 0.177935, loss_ce: 0.034038\n","iteration 9574 : loss : 0.155257, loss_ce: 0.041977\n","iteration 9575 : loss : 0.199838, loss_ce: 0.027363\n","iteration 9576 : loss : 0.163137, loss_ce: 0.051894\n","iteration 9577 : loss : 0.154659, loss_ce: 0.048473\n","iteration 9578 : loss : 0.126521, loss_ce: 0.037656\n","iteration 9579 : loss : 0.281455, loss_ce: 0.051268\n"," 69%|██████████████████▌        | 103/150 [2:14:29<1:01:45, 78.85s/it]iteration 9580 : loss : 0.195928, loss_ce: 0.035400\n","iteration 9581 : loss : 0.168951, loss_ce: 0.037009\n","iteration 9582 : loss : 0.175373, loss_ce: 0.056669\n","iteration 9583 : loss : 0.169042, loss_ce: 0.041407\n","iteration 9584 : loss : 0.174641, loss_ce: 0.056106\n","iteration 9585 : loss : 0.216375, loss_ce: 0.039515\n","iteration 9586 : loss : 0.167246, loss_ce: 0.058433\n","iteration 9587 : loss : 0.144183, loss_ce: 0.039191\n","iteration 9588 : loss : 0.166662, loss_ce: 0.049933\n","iteration 9589 : loss : 0.167295, loss_ce: 0.055291\n","iteration 9590 : loss : 0.151801, loss_ce: 0.070349\n","iteration 9591 : loss : 0.176576, loss_ce: 0.048637\n","iteration 9592 : loss : 0.184255, loss_ce: 0.048759\n","iteration 9593 : loss : 0.128102, loss_ce: 0.055790\n","iteration 9594 : loss : 0.167350, loss_ce: 0.051554\n","iteration 9595 : loss : 0.165384, loss_ce: 0.038419\n","iteration 9596 : loss : 0.165162, loss_ce: 0.042059\n","iteration 9597 : loss : 0.274721, loss_ce: 0.015538\n","iteration 9598 : loss : 0.151109, loss_ce: 0.059418\n","iteration 9599 : loss : 0.163542, loss_ce: 0.037249\n","iteration 9600 : loss : 0.173030, loss_ce: 0.040404\n","iteration 9601 : loss : 0.164439, loss_ce: 0.061145\n","iteration 9602 : loss : 0.149115, loss_ce: 0.030797\n","iteration 9603 : loss : 0.155766, loss_ce: 0.049640\n","iteration 9604 : loss : 0.140087, loss_ce: 0.029332\n","iteration 9605 : loss : 0.157763, loss_ce: 0.061805\n","iteration 9606 : loss : 0.156944, loss_ce: 0.052074\n","iteration 9607 : loss : 0.170737, loss_ce: 0.065693\n","iteration 9608 : loss : 0.158368, loss_ce: 0.048271\n","iteration 9609 : loss : 0.245258, loss_ce: 0.030359\n","iteration 9610 : loss : 0.141543, loss_ce: 0.037904\n","iteration 9611 : loss : 0.172047, loss_ce: 0.041921\n","iteration 9612 : loss : 0.132251, loss_ce: 0.040712\n","iteration 9613 : loss : 0.154114, loss_ce: 0.043386\n","iteration 9614 : loss : 0.170948, loss_ce: 0.047605\n","iteration 9615 : loss : 0.181648, loss_ce: 0.062416\n","iteration 9616 : loss : 0.187251, loss_ce: 0.045090\n","iteration 9617 : loss : 0.181351, loss_ce: 0.050125\n","iteration 9618 : loss : 0.186237, loss_ce: 0.040086\n","iteration 9619 : loss : 0.129703, loss_ce: 0.039081\n","iteration 9620 : loss : 0.164294, loss_ce: 0.032596\n","iteration 9621 : loss : 0.155892, loss_ce: 0.070710\n","iteration 9622 : loss : 0.147899, loss_ce: 0.052354\n","iteration 9623 : loss : 0.184390, loss_ce: 0.058965\n","iteration 9624 : loss : 0.195558, loss_ce: 0.036141\n","iteration 9625 : loss : 0.137672, loss_ce: 0.049336\n","iteration 9626 : loss : 0.140997, loss_ce: 0.048695\n","iteration 9627 : loss : 0.157669, loss_ce: 0.033684\n","iteration 9628 : loss : 0.155784, loss_ce: 0.048104\n","iteration 9629 : loss : 0.198686, loss_ce: 0.033612\n","iteration 9630 : loss : 0.179018, loss_ce: 0.027315\n","iteration 9631 : loss : 0.118266, loss_ce: 0.046540\n","iteration 9632 : loss : 0.225045, loss_ce: 0.078137\n","iteration 9633 : loss : 0.206263, loss_ce: 0.066929\n","iteration 9634 : loss : 0.180356, loss_ce: 0.048745\n","iteration 9635 : loss : 0.152107, loss_ce: 0.043647\n","iteration 9636 : loss : 0.177006, loss_ce: 0.048450\n","iteration 9637 : loss : 0.156998, loss_ce: 0.048828\n","iteration 9638 : loss : 0.154637, loss_ce: 0.051493\n","iteration 9639 : loss : 0.238387, loss_ce: 0.044012\n","iteration 9640 : loss : 0.158197, loss_ce: 0.038954\n","iteration 9641 : loss : 0.162189, loss_ce: 0.057320\n","iteration 9642 : loss : 0.128468, loss_ce: 0.043447\n","iteration 9643 : loss : 0.122623, loss_ce: 0.047418\n","iteration 9644 : loss : 0.180212, loss_ce: 0.050823\n","iteration 9645 : loss : 0.158807, loss_ce: 0.046392\n","iteration 9646 : loss : 0.165728, loss_ce: 0.074246\n","iteration 9647 : loss : 0.154911, loss_ce: 0.041808\n","iteration 9648 : loss : 0.165108, loss_ce: 0.053500\n","iteration 9649 : loss : 0.191526, loss_ce: 0.051142\n","iteration 9650 : loss : 0.140310, loss_ce: 0.031214\n","iteration 9651 : loss : 0.117803, loss_ce: 0.040299\n","iteration 9652 : loss : 0.160425, loss_ce: 0.032508\n","iteration 9653 : loss : 0.144401, loss_ce: 0.034898\n","iteration 9654 : loss : 0.225577, loss_ce: 0.024070\n","iteration 9655 : loss : 0.143023, loss_ce: 0.047278\n","iteration 9656 : loss : 0.126551, loss_ce: 0.040888\n","iteration 9657 : loss : 0.119499, loss_ce: 0.032837\n","iteration 9658 : loss : 0.197048, loss_ce: 0.059944\n","iteration 9659 : loss : 0.120854, loss_ce: 0.042029\n","iteration 9660 : loss : 0.130685, loss_ce: 0.061043\n","iteration 9661 : loss : 0.144216, loss_ce: 0.043795\n","iteration 9662 : loss : 0.222748, loss_ce: 0.033213\n","iteration 9663 : loss : 0.179683, loss_ce: 0.064549\n","iteration 9664 : loss : 0.133984, loss_ce: 0.027616\n","iteration 9665 : loss : 0.215865, loss_ce: 0.033944\n","iteration 9666 : loss : 0.214096, loss_ce: 0.036127\n","iteration 9667 : loss : 0.191873, loss_ce: 0.072322\n","iteration 9668 : loss : 0.135364, loss_ce: 0.043895\n","iteration 9669 : loss : 0.134920, loss_ce: 0.049017\n","iteration 9670 : loss : 0.198869, loss_ce: 0.041919\n","iteration 9671 : loss : 0.123304, loss_ce: 0.036918\n","iteration 9672 : loss : 0.369193, loss_ce: 0.034085\n"," 69%|██████████████████▋        | 104/150 [2:15:51<1:01:14, 79.89s/it]iteration 9673 : loss : 0.172100, loss_ce: 0.021981\n","iteration 9674 : loss : 0.139144, loss_ce: 0.040239\n","iteration 9675 : loss : 0.235889, loss_ce: 0.041049\n","iteration 9676 : loss : 0.171847, loss_ce: 0.056627\n","iteration 9677 : loss : 0.141980, loss_ce: 0.054605\n","iteration 9678 : loss : 0.190318, loss_ce: 0.050923\n","iteration 9679 : loss : 0.171480, loss_ce: 0.065190\n","iteration 9680 : loss : 0.149928, loss_ce: 0.046292\n","iteration 9681 : loss : 0.140335, loss_ce: 0.061540\n","iteration 9682 : loss : 0.123784, loss_ce: 0.036849\n","iteration 9683 : loss : 0.203087, loss_ce: 0.048447\n","iteration 9684 : loss : 0.143360, loss_ce: 0.034748\n","iteration 9685 : loss : 0.114854, loss_ce: 0.036891\n","iteration 9686 : loss : 0.168622, loss_ce: 0.042297\n","iteration 9687 : loss : 0.179495, loss_ce: 0.036485\n","iteration 9688 : loss : 0.136494, loss_ce: 0.024148\n","iteration 9689 : loss : 0.131283, loss_ce: 0.030777\n","iteration 9690 : loss : 0.174225, loss_ce: 0.038474\n","iteration 9691 : loss : 0.132243, loss_ce: 0.036589\n","iteration 9692 : loss : 0.179628, loss_ce: 0.038920\n","iteration 9693 : loss : 0.113748, loss_ce: 0.039661\n","iteration 9694 : loss : 0.156418, loss_ce: 0.044048\n","iteration 9695 : loss : 0.160446, loss_ce: 0.041883\n","iteration 9696 : loss : 0.188852, loss_ce: 0.065108\n","iteration 9697 : loss : 0.206496, loss_ce: 0.023755\n","iteration 9698 : loss : 0.124012, loss_ce: 0.042183\n","iteration 9699 : loss : 0.139922, loss_ce: 0.044429\n","iteration 9700 : loss : 0.205033, loss_ce: 0.056491\n","iteration 9701 : loss : 0.200466, loss_ce: 0.050999\n","iteration 9702 : loss : 0.173096, loss_ce: 0.042097\n","iteration 9703 : loss : 0.154779, loss_ce: 0.041645\n","iteration 9704 : loss : 0.168616, loss_ce: 0.052586\n","iteration 9705 : loss : 0.141991, loss_ce: 0.034996\n","iteration 9706 : loss : 0.150254, loss_ce: 0.045034\n","iteration 9707 : loss : 0.145942, loss_ce: 0.032941\n","iteration 9708 : loss : 0.145720, loss_ce: 0.025492\n","iteration 9709 : loss : 0.156025, loss_ce: 0.065569\n","iteration 9710 : loss : 0.109878, loss_ce: 0.044518\n","iteration 9711 : loss : 0.149382, loss_ce: 0.039860\n","iteration 9712 : loss : 0.155444, loss_ce: 0.051790\n","iteration 9713 : loss : 0.157511, loss_ce: 0.051511\n","iteration 9714 : loss : 0.143594, loss_ce: 0.061109\n","iteration 9715 : loss : 0.207071, loss_ce: 0.044872\n","iteration 9716 : loss : 0.150975, loss_ce: 0.046532\n","iteration 9717 : loss : 0.141519, loss_ce: 0.043652\n","iteration 9718 : loss : 0.174856, loss_ce: 0.046921\n","iteration 9719 : loss : 0.121476, loss_ce: 0.032566\n","iteration 9720 : loss : 0.149131, loss_ce: 0.063578\n","iteration 9721 : loss : 0.158579, loss_ce: 0.045686\n","iteration 9722 : loss : 0.149779, loss_ce: 0.038031\n","iteration 9723 : loss : 0.150820, loss_ce: 0.053025\n","iteration 9724 : loss : 0.132899, loss_ce: 0.032862\n","iteration 9725 : loss : 0.181431, loss_ce: 0.050167\n","iteration 9726 : loss : 0.150881, loss_ce: 0.048533\n","iteration 9727 : loss : 0.199698, loss_ce: 0.039059\n","iteration 9728 : loss : 0.100665, loss_ce: 0.028796\n","iteration 9729 : loss : 0.189549, loss_ce: 0.040597\n","iteration 9730 : loss : 0.187422, loss_ce: 0.053317\n","iteration 9731 : loss : 0.170790, loss_ce: 0.046344\n","iteration 9732 : loss : 0.150847, loss_ce: 0.036232\n","iteration 9733 : loss : 0.115465, loss_ce: 0.037653\n","iteration 9734 : loss : 0.122125, loss_ce: 0.041253\n","iteration 9735 : loss : 0.134478, loss_ce: 0.050501\n","iteration 9736 : loss : 0.110645, loss_ce: 0.038072\n","iteration 9737 : loss : 0.147208, loss_ce: 0.047873\n","iteration 9738 : loss : 0.147951, loss_ce: 0.042867\n","iteration 9739 : loss : 0.144435, loss_ce: 0.054289\n","iteration 9740 : loss : 0.157566, loss_ce: 0.029992\n","iteration 9741 : loss : 0.111986, loss_ce: 0.039418\n","iteration 9742 : loss : 0.216153, loss_ce: 0.029368\n","iteration 9743 : loss : 0.130082, loss_ce: 0.038644\n","iteration 9744 : loss : 0.131462, loss_ce: 0.042088\n","iteration 9745 : loss : 0.148369, loss_ce: 0.041250\n","iteration 9746 : loss : 0.120685, loss_ce: 0.037392\n","iteration 9747 : loss : 0.252610, loss_ce: 0.027379\n","iteration 9748 : loss : 0.179690, loss_ce: 0.042500\n","iteration 9749 : loss : 0.126635, loss_ce: 0.047380\n","iteration 9750 : loss : 0.169552, loss_ce: 0.069635\n","iteration 9751 : loss : 0.160828, loss_ce: 0.061107\n","iteration 9752 : loss : 0.165192, loss_ce: 0.064160\n","iteration 9753 : loss : 0.143780, loss_ce: 0.054415\n","iteration 9754 : loss : 0.188272, loss_ce: 0.032622\n","iteration 9755 : loss : 0.166483, loss_ce: 0.042747\n","iteration 9756 : loss : 0.117408, loss_ce: 0.049971\n","iteration 9757 : loss : 0.270380, loss_ce: 0.034866\n","iteration 9758 : loss : 0.204924, loss_ce: 0.036879\n","iteration 9759 : loss : 0.174282, loss_ce: 0.028826\n","iteration 9760 : loss : 0.136757, loss_ce: 0.052460\n","iteration 9761 : loss : 0.190794, loss_ce: 0.041562\n","iteration 9762 : loss : 0.144834, loss_ce: 0.049321\n","iteration 9763 : loss : 0.131095, loss_ce: 0.034174\n","iteration 9764 : loss : 0.111722, loss_ce: 0.026628\n","iteration 9765 : loss : 0.464633, loss_ce: 0.006834\n"," 70%|████████████████████▎        | 105/150 [2:17:07<58:54, 78.54s/it]iteration 9766 : loss : 0.177012, loss_ce: 0.067634\n","iteration 9767 : loss : 0.233271, loss_ce: 0.035762\n","iteration 9768 : loss : 0.214076, loss_ce: 0.046003\n","iteration 9769 : loss : 0.132917, loss_ce: 0.059279\n","iteration 9770 : loss : 0.173062, loss_ce: 0.037235\n","iteration 9771 : loss : 0.132922, loss_ce: 0.040297\n","iteration 9772 : loss : 0.180585, loss_ce: 0.038051\n","iteration 9773 : loss : 0.149610, loss_ce: 0.025210\n","iteration 9774 : loss : 0.154356, loss_ce: 0.048127\n","iteration 9775 : loss : 0.141780, loss_ce: 0.027704\n","iteration 9776 : loss : 0.142058, loss_ce: 0.031100\n","iteration 9777 : loss : 0.216339, loss_ce: 0.049763\n","iteration 9778 : loss : 0.151620, loss_ce: 0.039508\n","iteration 9779 : loss : 0.125464, loss_ce: 0.042790\n","iteration 9780 : loss : 0.123380, loss_ce: 0.023594\n","iteration 9781 : loss : 0.138637, loss_ce: 0.036325\n","iteration 9782 : loss : 0.144607, loss_ce: 0.051590\n","iteration 9783 : loss : 0.151989, loss_ce: 0.039355\n","iteration 9784 : loss : 0.149318, loss_ce: 0.045840\n","iteration 9785 : loss : 0.162746, loss_ce: 0.042599\n","iteration 9786 : loss : 0.128901, loss_ce: 0.041022\n","iteration 9787 : loss : 0.124989, loss_ce: 0.021071\n","iteration 9788 : loss : 0.177576, loss_ce: 0.062159\n","iteration 9789 : loss : 0.143134, loss_ce: 0.054350\n","iteration 9790 : loss : 0.143135, loss_ce: 0.048183\n","iteration 9791 : loss : 0.148792, loss_ce: 0.054723\n","iteration 9792 : loss : 0.126171, loss_ce: 0.043179\n","iteration 9793 : loss : 0.160659, loss_ce: 0.057254\n","iteration 9794 : loss : 0.136642, loss_ce: 0.046041\n","iteration 9795 : loss : 0.114228, loss_ce: 0.032960\n","iteration 9796 : loss : 0.164759, loss_ce: 0.042411\n","iteration 9797 : loss : 0.157929, loss_ce: 0.055324\n","iteration 9798 : loss : 0.116309, loss_ce: 0.042047\n","iteration 9799 : loss : 0.159524, loss_ce: 0.039492\n","iteration 9800 : loss : 0.168162, loss_ce: 0.039013\n","iteration 9801 : loss : 0.128124, loss_ce: 0.030656\n","iteration 9802 : loss : 0.155447, loss_ce: 0.039493\n","iteration 9803 : loss : 0.174604, loss_ce: 0.035782\n","iteration 9804 : loss : 0.136427, loss_ce: 0.043993\n","iteration 9805 : loss : 0.238166, loss_ce: 0.036509\n","iteration 9806 : loss : 0.144061, loss_ce: 0.039680\n","iteration 9807 : loss : 0.107416, loss_ce: 0.024145\n","iteration 9808 : loss : 0.148797, loss_ce: 0.037184\n","iteration 9809 : loss : 0.108609, loss_ce: 0.049322\n","iteration 9810 : loss : 0.209787, loss_ce: 0.032213\n","iteration 9811 : loss : 0.148713, loss_ce: 0.038066\n","iteration 9812 : loss : 0.148480, loss_ce: 0.033824\n","iteration 9813 : loss : 0.209469, loss_ce: 0.050787\n","iteration 9814 : loss : 0.206972, loss_ce: 0.051802\n","iteration 9815 : loss : 0.180855, loss_ce: 0.041003\n","iteration 9816 : loss : 0.155857, loss_ce: 0.048027\n","iteration 9817 : loss : 0.117211, loss_ce: 0.034089\n","iteration 9818 : loss : 0.134169, loss_ce: 0.042092\n","iteration 9819 : loss : 0.129833, loss_ce: 0.050649\n","iteration 9820 : loss : 0.131239, loss_ce: 0.040253\n","iteration 9821 : loss : 0.147792, loss_ce: 0.039836\n","iteration 9822 : loss : 0.136860, loss_ce: 0.044858\n","iteration 9823 : loss : 0.108231, loss_ce: 0.034173\n","iteration 9824 : loss : 0.127613, loss_ce: 0.041238\n","iteration 9825 : loss : 0.101897, loss_ce: 0.033252\n","iteration 9826 : loss : 0.145707, loss_ce: 0.037912\n","iteration 9827 : loss : 0.258304, loss_ce: 0.036786\n","iteration 9828 : loss : 0.153382, loss_ce: 0.037087\n","iteration 9829 : loss : 0.118035, loss_ce: 0.045474\n","iteration 9830 : loss : 0.114533, loss_ce: 0.028245\n","iteration 9831 : loss : 0.173568, loss_ce: 0.022056\n","iteration 9832 : loss : 0.182892, loss_ce: 0.044667\n","iteration 9833 : loss : 0.128593, loss_ce: 0.055536\n","iteration 9834 : loss : 0.152885, loss_ce: 0.058327\n","iteration 9835 : loss : 0.156000, loss_ce: 0.038519\n","iteration 9836 : loss : 0.129917, loss_ce: 0.055546\n","iteration 9837 : loss : 0.121048, loss_ce: 0.035937\n","iteration 9838 : loss : 0.162933, loss_ce: 0.080533\n","iteration 9839 : loss : 0.183842, loss_ce: 0.055576\n","iteration 9840 : loss : 0.128532, loss_ce: 0.045074\n","iteration 9841 : loss : 0.128197, loss_ce: 0.038352\n","iteration 9842 : loss : 0.138052, loss_ce: 0.046359\n","iteration 9843 : loss : 0.120370, loss_ce: 0.034967\n","iteration 9844 : loss : 0.122146, loss_ce: 0.039474\n","iteration 9845 : loss : 0.132237, loss_ce: 0.059304\n","iteration 9846 : loss : 0.124680, loss_ce: 0.043836\n","iteration 9847 : loss : 0.097993, loss_ce: 0.031984\n","iteration 9848 : loss : 0.137986, loss_ce: 0.044619\n","iteration 9849 : loss : 0.121468, loss_ce: 0.035715\n","iteration 9850 : loss : 0.199106, loss_ce: 0.052213\n","iteration 9851 : loss : 0.164567, loss_ce: 0.040186\n","iteration 9852 : loss : 0.162188, loss_ce: 0.066412\n","iteration 9853 : loss : 0.142603, loss_ce: 0.046785\n","iteration 9854 : loss : 0.192974, loss_ce: 0.046589\n","iteration 9855 : loss : 0.134888, loss_ce: 0.028806\n","iteration 9856 : loss : 0.170268, loss_ce: 0.051113\n","iteration 9857 : loss : 0.153391, loss_ce: 0.050880\n","iteration 9858 : loss : 0.246162, loss_ce: 0.034034\n"," 71%|████████████████████▍        | 106/150 [2:18:28<58:07, 79.26s/it]iteration 9859 : loss : 0.113382, loss_ce: 0.046404\n","iteration 9860 : loss : 0.128702, loss_ce: 0.059648\n","iteration 9861 : loss : 0.149896, loss_ce: 0.063141\n","iteration 9862 : loss : 0.203566, loss_ce: 0.023101\n","iteration 9863 : loss : 0.202193, loss_ce: 0.043207\n","iteration 9864 : loss : 0.110892, loss_ce: 0.030186\n","iteration 9865 : loss : 0.128461, loss_ce: 0.049734\n","iteration 9866 : loss : 0.123644, loss_ce: 0.030332\n","iteration 9867 : loss : 0.178091, loss_ce: 0.017304\n","iteration 9868 : loss : 0.128526, loss_ce: 0.040419\n","iteration 9869 : loss : 0.110995, loss_ce: 0.041055\n","iteration 9870 : loss : 0.145862, loss_ce: 0.050061\n","iteration 9871 : loss : 0.146385, loss_ce: 0.032839\n","iteration 9872 : loss : 0.166635, loss_ce: 0.056846\n","iteration 9873 : loss : 0.154112, loss_ce: 0.043823\n","iteration 9874 : loss : 0.130544, loss_ce: 0.041582\n","iteration 9875 : loss : 0.137953, loss_ce: 0.027621\n","iteration 9876 : loss : 0.163137, loss_ce: 0.041344\n","iteration 9877 : loss : 0.128172, loss_ce: 0.027861\n","iteration 9878 : loss : 0.183690, loss_ce: 0.033888\n","iteration 9879 : loss : 0.114454, loss_ce: 0.049857\n","iteration 9880 : loss : 0.171943, loss_ce: 0.062050\n","iteration 9881 : loss : 0.145801, loss_ce: 0.031218\n","iteration 9882 : loss : 0.132021, loss_ce: 0.040330\n","iteration 9883 : loss : 0.150025, loss_ce: 0.036506\n","iteration 9884 : loss : 0.136265, loss_ce: 0.030058\n","iteration 9885 : loss : 0.136908, loss_ce: 0.058381\n","iteration 9886 : loss : 0.123503, loss_ce: 0.038984\n","iteration 9887 : loss : 0.122271, loss_ce: 0.048978\n","iteration 9888 : loss : 0.138585, loss_ce: 0.048101\n","iteration 9889 : loss : 0.172431, loss_ce: 0.033005\n","iteration 9890 : loss : 0.139561, loss_ce: 0.070807\n","iteration 9891 : loss : 0.144497, loss_ce: 0.053631\n","iteration 9892 : loss : 0.170365, loss_ce: 0.049252\n","iteration 9893 : loss : 0.131103, loss_ce: 0.042792\n","iteration 9894 : loss : 0.140817, loss_ce: 0.061830\n","iteration 9895 : loss : 0.109980, loss_ce: 0.032565\n","iteration 9896 : loss : 0.156535, loss_ce: 0.018013\n","iteration 9897 : loss : 0.125266, loss_ce: 0.026000\n","iteration 9898 : loss : 0.134108, loss_ce: 0.044745\n","iteration 9899 : loss : 0.154174, loss_ce: 0.047310\n","iteration 9900 : loss : 0.225827, loss_ce: 0.052151\n","iteration 9901 : loss : 0.127574, loss_ce: 0.027425\n","iteration 9902 : loss : 0.127052, loss_ce: 0.050179\n","iteration 9903 : loss : 0.126755, loss_ce: 0.021355\n","iteration 9904 : loss : 0.168187, loss_ce: 0.024507\n","iteration 9905 : loss : 0.098146, loss_ce: 0.022375\n","iteration 9906 : loss : 0.137435, loss_ce: 0.041301\n","iteration 9907 : loss : 0.205923, loss_ce: 0.030610\n","iteration 9908 : loss : 0.119134, loss_ce: 0.025717\n","iteration 9909 : loss : 0.132943, loss_ce: 0.044184\n","iteration 9910 : loss : 0.138921, loss_ce: 0.040927\n","iteration 9911 : loss : 0.150874, loss_ce: 0.037531\n","iteration 9912 : loss : 0.110231, loss_ce: 0.036437\n","iteration 9913 : loss : 0.145059, loss_ce: 0.049704\n","iteration 9914 : loss : 0.152252, loss_ce: 0.047932\n","iteration 9915 : loss : 0.205681, loss_ce: 0.036366\n","iteration 9916 : loss : 0.126043, loss_ce: 0.044224\n","iteration 9917 : loss : 0.167025, loss_ce: 0.042295\n","iteration 9918 : loss : 0.211246, loss_ce: 0.060840\n","iteration 9919 : loss : 0.154490, loss_ce: 0.041847\n","iteration 9920 : loss : 0.176040, loss_ce: 0.053505\n","iteration 9921 : loss : 0.123664, loss_ce: 0.048262\n","iteration 9922 : loss : 0.134718, loss_ce: 0.056551\n","iteration 9923 : loss : 0.155646, loss_ce: 0.021645\n","iteration 9924 : loss : 0.106804, loss_ce: 0.031960\n","iteration 9925 : loss : 0.113777, loss_ce: 0.028860\n","iteration 9926 : loss : 0.106710, loss_ce: 0.035599\n","iteration 9927 : loss : 0.132439, loss_ce: 0.046069\n","iteration 9928 : loss : 0.137305, loss_ce: 0.043600\n","iteration 9929 : loss : 0.130261, loss_ce: 0.034339\n","iteration 9930 : loss : 0.100391, loss_ce: 0.030821\n","iteration 9931 : loss : 0.138266, loss_ce: 0.026735\n","iteration 9932 : loss : 0.184014, loss_ce: 0.040480\n","iteration 9933 : loss : 0.132088, loss_ce: 0.051007\n","iteration 9934 : loss : 0.094629, loss_ce: 0.024642\n","iteration 9935 : loss : 0.118997, loss_ce: 0.048011\n","iteration 9936 : loss : 0.144533, loss_ce: 0.037392\n","iteration 9937 : loss : 0.155779, loss_ce: 0.028268\n","iteration 9938 : loss : 0.122814, loss_ce: 0.035332\n","iteration 9939 : loss : 0.142276, loss_ce: 0.053205\n","iteration 9940 : loss : 0.125657, loss_ce: 0.062787\n","iteration 9941 : loss : 0.139663, loss_ce: 0.054795\n","iteration 9942 : loss : 0.116493, loss_ce: 0.045167\n","iteration 9943 : loss : 0.154682, loss_ce: 0.034510\n","iteration 9944 : loss : 0.211134, loss_ce: 0.043142\n","iteration 9945 : loss : 0.183990, loss_ce: 0.046119\n","iteration 9946 : loss : 0.091309, loss_ce: 0.020904\n","iteration 9947 : loss : 0.203042, loss_ce: 0.044455\n","iteration 9948 : loss : 0.127022, loss_ce: 0.040334\n","iteration 9949 : loss : 0.147802, loss_ce: 0.042069\n","iteration 9950 : loss : 0.111906, loss_ce: 0.028522\n","iteration 9951 : loss : 0.371179, loss_ce: 0.028314\n"," 71%|████████████████████▋        | 107/150 [2:19:50<57:26, 80.16s/it]iteration 9952 : loss : 0.149153, loss_ce: 0.047006\n","iteration 9953 : loss : 0.156421, loss_ce: 0.061006\n","iteration 9954 : loss : 0.133739, loss_ce: 0.037804\n","iteration 9955 : loss : 0.108745, loss_ce: 0.042207\n","iteration 9956 : loss : 0.216357, loss_ce: 0.038724\n","iteration 9957 : loss : 0.127304, loss_ce: 0.041040\n","iteration 9958 : loss : 0.148893, loss_ce: 0.045138\n","iteration 9959 : loss : 0.089231, loss_ce: 0.037136\n","iteration 9960 : loss : 0.130914, loss_ce: 0.049403\n","iteration 9961 : loss : 0.116667, loss_ce: 0.053845\n","iteration 9962 : loss : 0.123868, loss_ce: 0.038757\n","iteration 9963 : loss : 0.152593, loss_ce: 0.058586\n","iteration 9964 : loss : 0.117626, loss_ce: 0.033393\n","iteration 9965 : loss : 0.114165, loss_ce: 0.038849\n","iteration 9966 : loss : 0.154391, loss_ce: 0.033666\n","iteration 9967 : loss : 0.116486, loss_ce: 0.037680\n","iteration 9968 : loss : 0.156548, loss_ce: 0.061186\n","iteration 9969 : loss : 0.253910, loss_ce: 0.028212\n","iteration 9970 : loss : 0.109199, loss_ce: 0.030058\n","iteration 9971 : loss : 0.114962, loss_ce: 0.024532\n","iteration 9972 : loss : 0.142078, loss_ce: 0.027482\n","iteration 9973 : loss : 0.167659, loss_ce: 0.050477\n","iteration 9974 : loss : 0.148427, loss_ce: 0.048235\n","iteration 9975 : loss : 0.194796, loss_ce: 0.022385\n","iteration 9976 : loss : 0.150198, loss_ce: 0.031662\n","iteration 9977 : loss : 0.125440, loss_ce: 0.043735\n","iteration 9978 : loss : 0.134294, loss_ce: 0.040528\n","iteration 9979 : loss : 0.186643, loss_ce: 0.032549\n","iteration 9980 : loss : 0.148921, loss_ce: 0.050588\n","iteration 9981 : loss : 0.151709, loss_ce: 0.040079\n","iteration 9982 : loss : 0.122665, loss_ce: 0.029259\n","iteration 9983 : loss : 0.125833, loss_ce: 0.043054\n","iteration 9984 : loss : 0.146604, loss_ce: 0.053838\n","iteration 9985 : loss : 0.140469, loss_ce: 0.047566\n","iteration 9986 : loss : 0.165204, loss_ce: 0.027242\n","iteration 9987 : loss : 0.125782, loss_ce: 0.062166\n","iteration 9988 : loss : 0.133995, loss_ce: 0.052676\n","iteration 9989 : loss : 0.187745, loss_ce: 0.031969\n","iteration 9990 : loss : 0.166339, loss_ce: 0.044144\n","iteration 9991 : loss : 0.144113, loss_ce: 0.051796\n","iteration 9992 : loss : 0.183144, loss_ce: 0.011764\n","iteration 9993 : loss : 0.114229, loss_ce: 0.038226\n","iteration 9994 : loss : 0.110577, loss_ce: 0.034405\n","iteration 9995 : loss : 0.151970, loss_ce: 0.034386\n","iteration 9996 : loss : 0.131552, loss_ce: 0.036263\n","iteration 9997 : loss : 0.157671, loss_ce: 0.018706\n","iteration 9998 : loss : 0.122606, loss_ce: 0.031201\n","iteration 9999 : loss : 0.111773, loss_ce: 0.025102\n","iteration 10000 : loss : 0.137806, loss_ce: 0.062269\n","iteration 10001 : loss : 0.107216, loss_ce: 0.032161\n","iteration 10002 : loss : 0.184961, loss_ce: 0.045374\n","iteration 10003 : loss : 0.119655, loss_ce: 0.052517\n","iteration 10004 : loss : 0.126930, loss_ce: 0.029226\n","iteration 10005 : loss : 0.102923, loss_ce: 0.035962\n","iteration 10006 : loss : 0.208363, loss_ce: 0.027399\n","iteration 10007 : loss : 0.150335, loss_ce: 0.050239\n","iteration 10008 : loss : 0.123420, loss_ce: 0.035414\n","iteration 10009 : loss : 0.120569, loss_ce: 0.043922\n","iteration 10010 : loss : 0.139389, loss_ce: 0.029579\n","iteration 10011 : loss : 0.101509, loss_ce: 0.022464\n","iteration 10012 : loss : 0.123331, loss_ce: 0.027750\n","iteration 10013 : loss : 0.129006, loss_ce: 0.044921\n","iteration 10014 : loss : 0.177389, loss_ce: 0.036751\n","iteration 10015 : loss : 0.160730, loss_ce: 0.031557\n","iteration 10016 : loss : 0.158670, loss_ce: 0.044355\n","iteration 10017 : loss : 0.195620, loss_ce: 0.044412\n","iteration 10018 : loss : 0.151861, loss_ce: 0.052690\n","iteration 10019 : loss : 0.135893, loss_ce: 0.048441\n","iteration 10020 : loss : 0.214441, loss_ce: 0.047941\n","iteration 10021 : loss : 0.159244, loss_ce: 0.051503\n","iteration 10022 : loss : 0.214930, loss_ce: 0.035705\n","iteration 10023 : loss : 0.159987, loss_ce: 0.039608\n","iteration 10024 : loss : 0.134101, loss_ce: 0.039643\n","iteration 10025 : loss : 0.126345, loss_ce: 0.034624\n","iteration 10026 : loss : 0.133843, loss_ce: 0.027594\n","iteration 10027 : loss : 0.137967, loss_ce: 0.038068\n","iteration 10028 : loss : 0.257588, loss_ce: 0.022139\n","iteration 10029 : loss : 0.192626, loss_ce: 0.038380\n","iteration 10030 : loss : 0.115287, loss_ce: 0.038394\n","iteration 10031 : loss : 0.158972, loss_ce: 0.033154\n","iteration 10032 : loss : 0.117438, loss_ce: 0.026728\n","iteration 10033 : loss : 0.115659, loss_ce: 0.037804\n","iteration 10034 : loss : 0.137056, loss_ce: 0.042094\n","iteration 10035 : loss : 0.129075, loss_ce: 0.048825\n","iteration 10036 : loss : 0.130893, loss_ce: 0.027444\n","iteration 10037 : loss : 0.125645, loss_ce: 0.042225\n","iteration 10038 : loss : 0.115025, loss_ce: 0.029424\n","iteration 10039 : loss : 0.138883, loss_ce: 0.045901\n","iteration 10040 : loss : 0.156782, loss_ce: 0.056758\n","iteration 10041 : loss : 0.126716, loss_ce: 0.043164\n","iteration 10042 : loss : 0.166487, loss_ce: 0.044830\n","iteration 10043 : loss : 0.124030, loss_ce: 0.033814\n","iteration 10044 : loss : 0.110549, loss_ce: 0.052566\n"," 72%|████████████████████▉        | 108/150 [2:21:06<55:10, 78.81s/it]iteration 10045 : loss : 0.161861, loss_ce: 0.024547\n","iteration 10046 : loss : 0.128514, loss_ce: 0.035121\n","iteration 10047 : loss : 0.101929, loss_ce: 0.027396\n","iteration 10048 : loss : 0.137306, loss_ce: 0.043521\n","iteration 10049 : loss : 0.171822, loss_ce: 0.046220\n","iteration 10050 : loss : 0.109669, loss_ce: 0.017520\n","iteration 10051 : loss : 0.202798, loss_ce: 0.034478\n","iteration 10052 : loss : 0.172772, loss_ce: 0.047273\n","iteration 10053 : loss : 0.205087, loss_ce: 0.045066\n","iteration 10054 : loss : 0.135378, loss_ce: 0.041091\n","iteration 10055 : loss : 0.132322, loss_ce: 0.055952\n","iteration 10056 : loss : 0.177304, loss_ce: 0.036466\n","iteration 10057 : loss : 0.194762, loss_ce: 0.023002\n","iteration 10058 : loss : 0.203537, loss_ce: 0.037112\n","iteration 10059 : loss : 0.130480, loss_ce: 0.062214\n","iteration 10060 : loss : 0.127078, loss_ce: 0.044393\n","iteration 10061 : loss : 0.109206, loss_ce: 0.038806\n","iteration 10062 : loss : 0.123659, loss_ce: 0.041653\n","iteration 10063 : loss : 0.110600, loss_ce: 0.042773\n","iteration 10064 : loss : 0.127034, loss_ce: 0.027519\n","iteration 10065 : loss : 0.131610, loss_ce: 0.044110\n","iteration 10066 : loss : 0.093493, loss_ce: 0.029968\n","iteration 10067 : loss : 0.131564, loss_ce: 0.030349\n","iteration 10068 : loss : 0.124407, loss_ce: 0.032282\n","iteration 10069 : loss : 0.243538, loss_ce: 0.021423\n","iteration 10070 : loss : 0.146303, loss_ce: 0.043442\n","iteration 10071 : loss : 0.175444, loss_ce: 0.036147\n","iteration 10072 : loss : 0.177542, loss_ce: 0.048774\n","iteration 10073 : loss : 0.201035, loss_ce: 0.022271\n","iteration 10074 : loss : 0.178128, loss_ce: 0.070857\n","iteration 10075 : loss : 0.111176, loss_ce: 0.032264\n","iteration 10076 : loss : 0.146008, loss_ce: 0.034965\n","iteration 10077 : loss : 0.151094, loss_ce: 0.044521\n","iteration 10078 : loss : 0.117450, loss_ce: 0.023164\n","iteration 10079 : loss : 0.124223, loss_ce: 0.041710\n","iteration 10080 : loss : 0.121027, loss_ce: 0.040931\n","iteration 10081 : loss : 0.160077, loss_ce: 0.035976\n","iteration 10082 : loss : 0.154242, loss_ce: 0.027075\n","iteration 10083 : loss : 0.173036, loss_ce: 0.046614\n","iteration 10084 : loss : 0.147991, loss_ce: 0.046764\n","iteration 10085 : loss : 0.132516, loss_ce: 0.029501\n","iteration 10086 : loss : 0.123138, loss_ce: 0.036683\n","iteration 10087 : loss : 0.121470, loss_ce: 0.034917\n","iteration 10088 : loss : 0.126714, loss_ce: 0.026005\n","iteration 10089 : loss : 0.130603, loss_ce: 0.033232\n","iteration 10090 : loss : 0.153656, loss_ce: 0.036943\n","iteration 10091 : loss : 0.145695, loss_ce: 0.049859\n","iteration 10092 : loss : 0.125270, loss_ce: 0.052133\n","iteration 10093 : loss : 0.107626, loss_ce: 0.034526\n","iteration 10094 : loss : 0.114819, loss_ce: 0.039209\n","iteration 10095 : loss : 0.137702, loss_ce: 0.041415\n","iteration 10096 : loss : 0.189103, loss_ce: 0.042777\n","iteration 10097 : loss : 0.120669, loss_ce: 0.045628\n","iteration 10098 : loss : 0.160365, loss_ce: 0.045554\n","iteration 10099 : loss : 0.109487, loss_ce: 0.031762\n","iteration 10100 : loss : 0.176064, loss_ce: 0.057524\n","iteration 10101 : loss : 0.113857, loss_ce: 0.039774\n","iteration 10102 : loss : 0.108266, loss_ce: 0.027726\n","iteration 10103 : loss : 0.161439, loss_ce: 0.030266\n","iteration 10104 : loss : 0.137885, loss_ce: 0.049460\n","iteration 10105 : loss : 0.118731, loss_ce: 0.051626\n","iteration 10106 : loss : 0.168213, loss_ce: 0.036735\n","iteration 10107 : loss : 0.148464, loss_ce: 0.041088\n","iteration 10108 : loss : 0.147653, loss_ce: 0.053182\n","iteration 10109 : loss : 0.184638, loss_ce: 0.051550\n","iteration 10110 : loss : 0.147048, loss_ce: 0.047120\n","iteration 10111 : loss : 0.157957, loss_ce: 0.043628\n","iteration 10112 : loss : 0.165266, loss_ce: 0.039813\n","iteration 10113 : loss : 0.182720, loss_ce: 0.032245\n","iteration 10114 : loss : 0.136082, loss_ce: 0.052628\n","iteration 10115 : loss : 0.122896, loss_ce: 0.034483\n","iteration 10116 : loss : 0.120650, loss_ce: 0.040969\n","iteration 10117 : loss : 0.125541, loss_ce: 0.052299\n","iteration 10118 : loss : 0.159307, loss_ce: 0.022650\n","iteration 10119 : loss : 0.153672, loss_ce: 0.043757\n","iteration 10120 : loss : 0.150157, loss_ce: 0.038825\n","iteration 10121 : loss : 0.132052, loss_ce: 0.043374\n","iteration 10122 : loss : 0.132153, loss_ce: 0.038297\n","iteration 10123 : loss : 0.111780, loss_ce: 0.035041\n","iteration 10124 : loss : 0.120394, loss_ce: 0.033432\n","iteration 10125 : loss : 0.104218, loss_ce: 0.030775\n","iteration 10126 : loss : 0.123467, loss_ce: 0.043776\n","iteration 10127 : loss : 0.124306, loss_ce: 0.043480\n","iteration 10128 : loss : 0.115661, loss_ce: 0.036721\n","iteration 10129 : loss : 0.222899, loss_ce: 0.040999\n","iteration 10130 : loss : 0.211170, loss_ce: 0.036374\n","iteration 10131 : loss : 0.131005, loss_ce: 0.044435\n","iteration 10132 : loss : 0.141191, loss_ce: 0.045093\n","iteration 10133 : loss : 0.140092, loss_ce: 0.061636\n","iteration 10134 : loss : 0.116012, loss_ce: 0.040414\n","iteration 10135 : loss : 0.100585, loss_ce: 0.041571\n","iteration 10136 : loss : 0.128217, loss_ce: 0.044475\n","iteration 10137 : loss : 0.497823, loss_ce: 0.009760\n"," 73%|█████████████████████        | 109/150 [2:22:24<53:43, 78.63s/it]iteration 10138 : loss : 0.179124, loss_ce: 0.046480\n","iteration 10139 : loss : 0.112431, loss_ce: 0.028615\n","iteration 10140 : loss : 0.196999, loss_ce: 0.062768\n","iteration 10141 : loss : 0.158785, loss_ce: 0.046165\n","iteration 10142 : loss : 0.128569, loss_ce: 0.053160\n","iteration 10143 : loss : 0.154449, loss_ce: 0.032429\n","iteration 10144 : loss : 0.102789, loss_ce: 0.026542\n","iteration 10145 : loss : 0.140441, loss_ce: 0.024436\n","iteration 10146 : loss : 0.113038, loss_ce: 0.050141\n","iteration 10147 : loss : 0.158928, loss_ce: 0.027414\n","iteration 10148 : loss : 0.105575, loss_ce: 0.031186\n","iteration 10149 : loss : 0.174036, loss_ce: 0.030144\n","iteration 10150 : loss : 0.143475, loss_ce: 0.054993\n","iteration 10151 : loss : 0.154113, loss_ce: 0.035558\n","iteration 10152 : loss : 0.202158, loss_ce: 0.044167\n","iteration 10153 : loss : 0.107077, loss_ce: 0.031062\n","iteration 10154 : loss : 0.146190, loss_ce: 0.041131\n","iteration 10155 : loss : 0.101178, loss_ce: 0.030117\n","iteration 10156 : loss : 0.114207, loss_ce: 0.034113\n","iteration 10157 : loss : 0.146585, loss_ce: 0.026498\n","iteration 10158 : loss : 0.092895, loss_ce: 0.035154\n","iteration 10159 : loss : 0.184479, loss_ce: 0.039160\n","iteration 10160 : loss : 0.158703, loss_ce: 0.044796\n","iteration 10161 : loss : 0.110530, loss_ce: 0.057764\n","iteration 10162 : loss : 0.131618, loss_ce: 0.055201\n","iteration 10163 : loss : 0.148290, loss_ce: 0.047200\n","iteration 10164 : loss : 0.144270, loss_ce: 0.035426\n","iteration 10165 : loss : 0.216384, loss_ce: 0.033324\n","iteration 10166 : loss : 0.151884, loss_ce: 0.045991\n","iteration 10167 : loss : 0.130735, loss_ce: 0.029650\n","iteration 10168 : loss : 0.143924, loss_ce: 0.032638\n","iteration 10169 : loss : 0.117905, loss_ce: 0.033842\n","iteration 10170 : loss : 0.126151, loss_ce: 0.034211\n","iteration 10171 : loss : 0.117433, loss_ce: 0.032142\n","iteration 10172 : loss : 0.148806, loss_ce: 0.046734\n","iteration 10173 : loss : 0.137632, loss_ce: 0.022491\n","iteration 10174 : loss : 0.100661, loss_ce: 0.033415\n","iteration 10175 : loss : 0.116492, loss_ce: 0.023117\n","iteration 10176 : loss : 0.144576, loss_ce: 0.059250\n","iteration 10177 : loss : 0.187314, loss_ce: 0.045693\n","iteration 10178 : loss : 0.126822, loss_ce: 0.024616\n","iteration 10179 : loss : 0.160815, loss_ce: 0.035668\n","iteration 10180 : loss : 0.149265, loss_ce: 0.041097\n","iteration 10181 : loss : 0.143805, loss_ce: 0.043128\n","iteration 10182 : loss : 0.118669, loss_ce: 0.033317\n","iteration 10183 : loss : 0.121284, loss_ce: 0.024016\n","iteration 10184 : loss : 0.107463, loss_ce: 0.034450\n","iteration 10185 : loss : 0.222472, loss_ce: 0.020342\n","iteration 10186 : loss : 0.133922, loss_ce: 0.038681\n","iteration 10187 : loss : 0.191649, loss_ce: 0.022866\n","iteration 10188 : loss : 0.172786, loss_ce: 0.062475\n","iteration 10189 : loss : 0.210989, loss_ce: 0.046899\n","iteration 10190 : loss : 0.158633, loss_ce: 0.054239\n","iteration 10191 : loss : 0.125747, loss_ce: 0.051659\n","iteration 10192 : loss : 0.184284, loss_ce: 0.032266\n","iteration 10193 : loss : 0.130290, loss_ce: 0.046474\n","iteration 10194 : loss : 0.124588, loss_ce: 0.052954\n","iteration 10195 : loss : 0.149758, loss_ce: 0.039328\n","iteration 10196 : loss : 0.193851, loss_ce: 0.049359\n","iteration 10197 : loss : 0.128034, loss_ce: 0.038901\n","iteration 10198 : loss : 0.143232, loss_ce: 0.044512\n","iteration 10199 : loss : 0.153101, loss_ce: 0.040910\n","iteration 10200 : loss : 0.118325, loss_ce: 0.030380\n","iteration 10201 : loss : 0.121050, loss_ce: 0.023583\n","iteration 10202 : loss : 0.118003, loss_ce: 0.027963\n","iteration 10203 : loss : 0.137824, loss_ce: 0.051755\n","iteration 10204 : loss : 0.132531, loss_ce: 0.044158\n","iteration 10205 : loss : 0.116132, loss_ce: 0.050753\n","iteration 10206 : loss : 0.167526, loss_ce: 0.020179\n","iteration 10207 : loss : 0.211245, loss_ce: 0.034039\n","iteration 10208 : loss : 0.143972, loss_ce: 0.054880\n","iteration 10209 : loss : 0.172779, loss_ce: 0.043359\n","iteration 10210 : loss : 0.155376, loss_ce: 0.048640\n","iteration 10211 : loss : 0.146553, loss_ce: 0.049554\n","iteration 10212 : loss : 0.148503, loss_ce: 0.049836\n","iteration 10213 : loss : 0.306046, loss_ce: 0.026646\n","iteration 10214 : loss : 0.168688, loss_ce: 0.040965\n","iteration 10215 : loss : 0.147543, loss_ce: 0.037448\n","iteration 10216 : loss : 0.139218, loss_ce: 0.041645\n","iteration 10217 : loss : 0.144392, loss_ce: 0.044591\n","iteration 10218 : loss : 0.154481, loss_ce: 0.041057\n","iteration 10219 : loss : 0.134221, loss_ce: 0.029433\n","iteration 10220 : loss : 0.156334, loss_ce: 0.042848\n","iteration 10221 : loss : 0.140139, loss_ce: 0.038404\n","iteration 10222 : loss : 0.134489, loss_ce: 0.056009\n","iteration 10223 : loss : 0.117752, loss_ce: 0.040542\n","iteration 10224 : loss : 0.136510, loss_ce: 0.053146\n","iteration 10225 : loss : 0.143950, loss_ce: 0.054612\n","iteration 10226 : loss : 0.167439, loss_ce: 0.045143\n","iteration 10227 : loss : 0.131410, loss_ce: 0.030467\n","iteration 10228 : loss : 0.102694, loss_ce: 0.035539\n","iteration 10229 : loss : 0.111263, loss_ce: 0.047527\n","iteration 10230 : loss : 0.307587, loss_ce: 0.017623\n"," 73%|█████████████████████▎       | 110/150 [2:23:43<52:34, 78.86s/it]iteration 10231 : loss : 0.117267, loss_ce: 0.048039\n","iteration 10232 : loss : 0.104072, loss_ce: 0.025732\n","iteration 10233 : loss : 0.133080, loss_ce: 0.043831\n","iteration 10234 : loss : 0.133271, loss_ce: 0.026064\n","iteration 10235 : loss : 0.144536, loss_ce: 0.043431\n","iteration 10236 : loss : 0.124411, loss_ce: 0.039064\n","iteration 10237 : loss : 0.203208, loss_ce: 0.035593\n","iteration 10238 : loss : 0.116896, loss_ce: 0.029119\n","iteration 10239 : loss : 0.142547, loss_ce: 0.051003\n","iteration 10240 : loss : 0.125790, loss_ce: 0.037530\n","iteration 10241 : loss : 0.176461, loss_ce: 0.051412\n","iteration 10242 : loss : 0.094218, loss_ce: 0.025402\n","iteration 10243 : loss : 0.128171, loss_ce: 0.020067\n","iteration 10244 : loss : 0.118685, loss_ce: 0.032182\n","iteration 10245 : loss : 0.184072, loss_ce: 0.037004\n","iteration 10246 : loss : 0.104050, loss_ce: 0.049019\n","iteration 10247 : loss : 0.143039, loss_ce: 0.037124\n","iteration 10248 : loss : 0.100650, loss_ce: 0.029010\n","iteration 10249 : loss : 0.136526, loss_ce: 0.037415\n","iteration 10250 : loss : 0.187145, loss_ce: 0.025649\n","iteration 10251 : loss : 0.082612, loss_ce: 0.029825\n","iteration 10252 : loss : 0.163225, loss_ce: 0.041809\n","iteration 10253 : loss : 0.109117, loss_ce: 0.042290\n","iteration 10254 : loss : 0.163706, loss_ce: 0.047277\n","iteration 10255 : loss : 0.131041, loss_ce: 0.064291\n","iteration 10256 : loss : 0.111493, loss_ce: 0.032253\n","iteration 10257 : loss : 0.195339, loss_ce: 0.046286\n","iteration 10258 : loss : 0.129681, loss_ce: 0.043423\n","iteration 10259 : loss : 0.129280, loss_ce: 0.041927\n","iteration 10260 : loss : 0.179260, loss_ce: 0.048589\n","iteration 10261 : loss : 0.127671, loss_ce: 0.043774\n","iteration 10262 : loss : 0.145866, loss_ce: 0.041905\n","iteration 10263 : loss : 0.160244, loss_ce: 0.051571\n","iteration 10264 : loss : 0.124529, loss_ce: 0.032213\n","iteration 10265 : loss : 0.134749, loss_ce: 0.031763\n","iteration 10266 : loss : 0.089599, loss_ce: 0.024459\n","iteration 10267 : loss : 0.135483, loss_ce: 0.025416\n","iteration 10268 : loss : 0.135390, loss_ce: 0.036345\n","iteration 10269 : loss : 0.120010, loss_ce: 0.043088\n","iteration 10270 : loss : 0.125313, loss_ce: 0.044708\n","iteration 10271 : loss : 0.160988, loss_ce: 0.044100\n","iteration 10272 : loss : 0.109111, loss_ce: 0.045169\n","iteration 10273 : loss : 0.157706, loss_ce: 0.040475\n","iteration 10274 : loss : 0.157478, loss_ce: 0.026862\n","iteration 10275 : loss : 0.160131, loss_ce: 0.036761\n","iteration 10276 : loss : 0.110044, loss_ce: 0.040237\n","iteration 10277 : loss : 0.174281, loss_ce: 0.030609\n","iteration 10278 : loss : 0.118161, loss_ce: 0.041014\n","iteration 10279 : loss : 0.137167, loss_ce: 0.030676\n","iteration 10280 : loss : 0.123339, loss_ce: 0.044803\n","iteration 10281 : loss : 0.135146, loss_ce: 0.047996\n","iteration 10282 : loss : 0.125634, loss_ce: 0.036316\n","iteration 10283 : loss : 0.172752, loss_ce: 0.042587\n","iteration 10284 : loss : 0.105724, loss_ce: 0.032436\n","iteration 10285 : loss : 0.118342, loss_ce: 0.053358\n","iteration 10286 : loss : 0.180041, loss_ce: 0.020707\n","iteration 10287 : loss : 0.126752, loss_ce: 0.054243\n","iteration 10288 : loss : 0.097036, loss_ce: 0.032702\n","iteration 10289 : loss : 0.169804, loss_ce: 0.039569\n","iteration 10290 : loss : 0.189509, loss_ce: 0.035262\n","iteration 10291 : loss : 0.138613, loss_ce: 0.049347\n","iteration 10292 : loss : 0.157744, loss_ce: 0.043734\n","iteration 10293 : loss : 0.117686, loss_ce: 0.029057\n","iteration 10294 : loss : 0.115575, loss_ce: 0.033432\n","iteration 10295 : loss : 0.143663, loss_ce: 0.043211\n","iteration 10296 : loss : 0.104225, loss_ce: 0.044449\n","iteration 10297 : loss : 0.105774, loss_ce: 0.032851\n","iteration 10298 : loss : 0.107132, loss_ce: 0.030851\n","iteration 10299 : loss : 0.127624, loss_ce: 0.040193\n","iteration 10300 : loss : 0.164180, loss_ce: 0.045595\n","iteration 10301 : loss : 0.170856, loss_ce: 0.040354\n","iteration 10302 : loss : 0.198987, loss_ce: 0.034264\n","iteration 10303 : loss : 0.136121, loss_ce: 0.036146\n","iteration 10304 : loss : 0.125608, loss_ce: 0.033806\n","iteration 10305 : loss : 0.186211, loss_ce: 0.032751\n","iteration 10306 : loss : 0.115079, loss_ce: 0.018314\n","iteration 10307 : loss : 0.171159, loss_ce: 0.028368\n","iteration 10308 : loss : 0.121839, loss_ce: 0.041761\n","iteration 10309 : loss : 0.166807, loss_ce: 0.038376\n","iteration 10310 : loss : 0.110455, loss_ce: 0.028954\n","iteration 10311 : loss : 0.131383, loss_ce: 0.052517\n","iteration 10312 : loss : 0.117331, loss_ce: 0.030894\n","iteration 10313 : loss : 0.157056, loss_ce: 0.043324\n","iteration 10314 : loss : 0.177958, loss_ce: 0.035611\n","iteration 10315 : loss : 0.143662, loss_ce: 0.050400\n","iteration 10316 : loss : 0.165316, loss_ce: 0.056174\n","iteration 10317 : loss : 0.186656, loss_ce: 0.035176\n","iteration 10318 : loss : 0.176962, loss_ce: 0.027098\n","iteration 10319 : loss : 0.141400, loss_ce: 0.039455\n","iteration 10320 : loss : 0.124882, loss_ce: 0.037561\n","iteration 10321 : loss : 0.140490, loss_ce: 0.039965\n","iteration 10322 : loss : 0.137041, loss_ce: 0.037318\n","iteration 10323 : loss : 0.515726, loss_ce: 0.015391\n"," 74%|█████████████████████▍       | 111/150 [2:25:01<50:57, 78.40s/it]iteration 10324 : loss : 0.115257, loss_ce: 0.030025\n","iteration 10325 : loss : 0.159201, loss_ce: 0.066506\n","iteration 10326 : loss : 0.200239, loss_ce: 0.036329\n","iteration 10327 : loss : 0.139436, loss_ce: 0.054149\n","iteration 10328 : loss : 0.150498, loss_ce: 0.042425\n","iteration 10329 : loss : 0.160670, loss_ce: 0.063980\n","iteration 10330 : loss : 0.100281, loss_ce: 0.030511\n","iteration 10331 : loss : 0.119254, loss_ce: 0.048640\n","iteration 10332 : loss : 0.094454, loss_ce: 0.032056\n","iteration 10333 : loss : 0.110567, loss_ce: 0.040441\n","iteration 10334 : loss : 0.121012, loss_ce: 0.033034\n","iteration 10335 : loss : 0.169152, loss_ce: 0.041050\n","iteration 10336 : loss : 0.141881, loss_ce: 0.030751\n","iteration 10337 : loss : 0.121972, loss_ce: 0.023278\n","iteration 10338 : loss : 0.126032, loss_ce: 0.054053\n","iteration 10339 : loss : 0.096842, loss_ce: 0.042913\n","iteration 10340 : loss : 0.111136, loss_ce: 0.034372\n","iteration 10341 : loss : 0.112227, loss_ce: 0.015355\n","iteration 10342 : loss : 0.117961, loss_ce: 0.030305\n","iteration 10343 : loss : 0.170757, loss_ce: 0.022126\n","iteration 10344 : loss : 0.142302, loss_ce: 0.040778\n","iteration 10345 : loss : 0.146676, loss_ce: 0.024438\n","iteration 10346 : loss : 0.137616, loss_ce: 0.047782\n","iteration 10347 : loss : 0.128416, loss_ce: 0.040582\n","iteration 10348 : loss : 0.125880, loss_ce: 0.023773\n","iteration 10349 : loss : 0.135484, loss_ce: 0.023107\n","iteration 10350 : loss : 0.109802, loss_ce: 0.039454\n","iteration 10351 : loss : 0.162103, loss_ce: 0.046474\n","iteration 10352 : loss : 0.134909, loss_ce: 0.022231\n","iteration 10353 : loss : 0.145160, loss_ce: 0.032963\n","iteration 10354 : loss : 0.132652, loss_ce: 0.037750\n","iteration 10355 : loss : 0.154289, loss_ce: 0.045149\n","iteration 10356 : loss : 0.116674, loss_ce: 0.032099\n","iteration 10357 : loss : 0.133526, loss_ce: 0.028318\n","iteration 10358 : loss : 0.099348, loss_ce: 0.032121\n","iteration 10359 : loss : 0.121875, loss_ce: 0.027973\n","iteration 10360 : loss : 0.177111, loss_ce: 0.027273\n","iteration 10361 : loss : 0.136224, loss_ce: 0.033989\n","iteration 10362 : loss : 0.159498, loss_ce: 0.034342\n","iteration 10363 : loss : 0.130417, loss_ce: 0.055300\n","iteration 10364 : loss : 0.101555, loss_ce: 0.032332\n","iteration 10365 : loss : 0.136664, loss_ce: 0.048341\n","iteration 10366 : loss : 0.112917, loss_ce: 0.034105\n","iteration 10367 : loss : 0.135835, loss_ce: 0.032166\n","iteration 10368 : loss : 0.121153, loss_ce: 0.041607\n","iteration 10369 : loss : 0.139250, loss_ce: 0.023216\n","iteration 10370 : loss : 0.134298, loss_ce: 0.026243\n","iteration 10371 : loss : 0.100922, loss_ce: 0.036205\n","iteration 10372 : loss : 0.135876, loss_ce: 0.028795\n","iteration 10373 : loss : 0.177063, loss_ce: 0.028060\n","iteration 10374 : loss : 0.121397, loss_ce: 0.047458\n","iteration 10375 : loss : 0.148346, loss_ce: 0.034837\n","iteration 10376 : loss : 0.138359, loss_ce: 0.039470\n","iteration 10377 : loss : 0.122823, loss_ce: 0.037378\n","iteration 10378 : loss : 0.152624, loss_ce: 0.033005\n","iteration 10379 : loss : 0.106468, loss_ce: 0.032943\n","iteration 10380 : loss : 0.104145, loss_ce: 0.040409\n","iteration 10381 : loss : 0.126194, loss_ce: 0.039236\n","iteration 10382 : loss : 0.112783, loss_ce: 0.036876\n","iteration 10383 : loss : 0.129838, loss_ce: 0.040525\n","iteration 10384 : loss : 0.100725, loss_ce: 0.043037\n","iteration 10385 : loss : 0.122501, loss_ce: 0.040211\n","iteration 10386 : loss : 0.134682, loss_ce: 0.042900\n","iteration 10387 : loss : 0.131320, loss_ce: 0.026541\n","iteration 10388 : loss : 0.195058, loss_ce: 0.039994\n","iteration 10389 : loss : 0.159310, loss_ce: 0.055968\n","iteration 10390 : loss : 0.151521, loss_ce: 0.053501\n","iteration 10391 : loss : 0.143580, loss_ce: 0.027059\n","iteration 10392 : loss : 0.104569, loss_ce: 0.034305\n","iteration 10393 : loss : 0.138396, loss_ce: 0.047975\n","iteration 10394 : loss : 0.130236, loss_ce: 0.057086\n","iteration 10395 : loss : 0.110069, loss_ce: 0.033441\n","iteration 10396 : loss : 0.122778, loss_ce: 0.042858\n","iteration 10397 : loss : 0.105900, loss_ce: 0.041198\n","iteration 10398 : loss : 0.132400, loss_ce: 0.039213\n","iteration 10399 : loss : 0.153647, loss_ce: 0.042142\n","iteration 10400 : loss : 0.118790, loss_ce: 0.040854\n","iteration 10401 : loss : 0.222242, loss_ce: 0.041754\n","iteration 10402 : loss : 0.112960, loss_ce: 0.042794\n","iteration 10403 : loss : 0.113226, loss_ce: 0.030057\n","iteration 10404 : loss : 0.146376, loss_ce: 0.037819\n","iteration 10405 : loss : 0.172213, loss_ce: 0.053151\n","iteration 10406 : loss : 0.125206, loss_ce: 0.053745\n","iteration 10407 : loss : 0.173115, loss_ce: 0.029386\n","iteration 10408 : loss : 0.101251, loss_ce: 0.045033\n","iteration 10409 : loss : 0.147314, loss_ce: 0.063918\n","iteration 10410 : loss : 0.124362, loss_ce: 0.033210\n","iteration 10411 : loss : 0.222533, loss_ce: 0.042302\n","iteration 10412 : loss : 0.175040, loss_ce: 0.024095\n","iteration 10413 : loss : 0.120405, loss_ce: 0.028837\n","iteration 10414 : loss : 0.169799, loss_ce: 0.048258\n","iteration 10415 : loss : 0.199953, loss_ce: 0.035314\n","iteration 10416 : loss : 0.499685, loss_ce: 0.021384\n"," 75%|█████████████████████▋       | 112/150 [2:26:22<50:14, 79.32s/it]iteration 10417 : loss : 0.145435, loss_ce: 0.051207\n","iteration 10418 : loss : 0.152224, loss_ce: 0.017357\n","iteration 10419 : loss : 0.106950, loss_ce: 0.038325\n","iteration 10420 : loss : 0.230443, loss_ce: 0.061375\n","iteration 10421 : loss : 0.117185, loss_ce: 0.031077\n","iteration 10422 : loss : 0.108740, loss_ce: 0.039086\n","iteration 10423 : loss : 0.188974, loss_ce: 0.040275\n","iteration 10424 : loss : 0.133358, loss_ce: 0.025542\n","iteration 10425 : loss : 0.133060, loss_ce: 0.062721\n","iteration 10426 : loss : 0.111765, loss_ce: 0.026562\n","iteration 10427 : loss : 0.161387, loss_ce: 0.031094\n","iteration 10428 : loss : 0.130228, loss_ce: 0.037849\n","iteration 10429 : loss : 0.122611, loss_ce: 0.053461\n","iteration 10430 : loss : 0.119910, loss_ce: 0.029480\n","iteration 10431 : loss : 0.121310, loss_ce: 0.022536\n","iteration 10432 : loss : 0.111544, loss_ce: 0.022689\n","iteration 10433 : loss : 0.115454, loss_ce: 0.034001\n","iteration 10434 : loss : 0.112929, loss_ce: 0.033032\n","iteration 10435 : loss : 0.120867, loss_ce: 0.045118\n","iteration 10436 : loss : 0.114038, loss_ce: 0.027662\n","iteration 10437 : loss : 0.090614, loss_ce: 0.026096\n","iteration 10438 : loss : 0.124372, loss_ce: 0.048612\n","iteration 10439 : loss : 0.123155, loss_ce: 0.035694\n","iteration 10440 : loss : 0.157502, loss_ce: 0.033347\n","iteration 10441 : loss : 0.139186, loss_ce: 0.023339\n","iteration 10442 : loss : 0.121425, loss_ce: 0.044620\n","iteration 10443 : loss : 0.148298, loss_ce: 0.046661\n","iteration 10444 : loss : 0.146855, loss_ce: 0.023812\n","iteration 10445 : loss : 0.138028, loss_ce: 0.044163\n","iteration 10446 : loss : 0.135676, loss_ce: 0.017744\n","iteration 10447 : loss : 0.133480, loss_ce: 0.033494\n","iteration 10448 : loss : 0.134465, loss_ce: 0.044429\n","iteration 10449 : loss : 0.119236, loss_ce: 0.029794\n","iteration 10450 : loss : 0.167220, loss_ce: 0.047975\n","iteration 10451 : loss : 0.147602, loss_ce: 0.050928\n","iteration 10452 : loss : 0.170563, loss_ce: 0.047478\n","iteration 10453 : loss : 0.098513, loss_ce: 0.038598\n","iteration 10454 : loss : 0.194667, loss_ce: 0.029350\n","iteration 10455 : loss : 0.112102, loss_ce: 0.034559\n","iteration 10456 : loss : 0.116889, loss_ce: 0.037458\n","iteration 10457 : loss : 0.121601, loss_ce: 0.048632\n","iteration 10458 : loss : 0.153060, loss_ce: 0.029756\n","iteration 10459 : loss : 0.141195, loss_ce: 0.048907\n","iteration 10460 : loss : 0.126469, loss_ce: 0.037955\n","iteration 10461 : loss : 0.187608, loss_ce: 0.038652\n","iteration 10462 : loss : 0.137989, loss_ce: 0.029005\n","iteration 10463 : loss : 0.193810, loss_ce: 0.041353\n","iteration 10464 : loss : 0.153931, loss_ce: 0.039309\n","iteration 10465 : loss : 0.140785, loss_ce: 0.052708\n","iteration 10466 : loss : 0.089787, loss_ce: 0.024730\n","iteration 10467 : loss : 0.115461, loss_ce: 0.036738\n","iteration 10468 : loss : 0.160093, loss_ce: 0.027008\n","iteration 10469 : loss : 0.152635, loss_ce: 0.051214\n","iteration 10470 : loss : 0.116373, loss_ce: 0.042993\n","iteration 10471 : loss : 0.160121, loss_ce: 0.046822\n","iteration 10472 : loss : 0.108075, loss_ce: 0.038130\n","iteration 10473 : loss : 0.168927, loss_ce: 0.026534\n","iteration 10474 : loss : 0.117808, loss_ce: 0.036799\n","iteration 10475 : loss : 0.130666, loss_ce: 0.047175\n","iteration 10476 : loss : 0.115480, loss_ce: 0.042946\n","iteration 10477 : loss : 0.142865, loss_ce: 0.041029\n","iteration 10478 : loss : 0.122021, loss_ce: 0.037416\n","iteration 10479 : loss : 0.130554, loss_ce: 0.045959\n","iteration 10480 : loss : 0.133195, loss_ce: 0.046796\n","iteration 10481 : loss : 0.102866, loss_ce: 0.033340\n","iteration 10482 : loss : 0.180117, loss_ce: 0.025175\n","iteration 10483 : loss : 0.139895, loss_ce: 0.028788\n","iteration 10484 : loss : 0.213381, loss_ce: 0.031410\n","iteration 10485 : loss : 0.104528, loss_ce: 0.038632\n","iteration 10486 : loss : 0.115637, loss_ce: 0.033587\n","iteration 10487 : loss : 0.134848, loss_ce: 0.024662\n","iteration 10488 : loss : 0.148716, loss_ce: 0.043634\n","iteration 10489 : loss : 0.123178, loss_ce: 0.036341\n","iteration 10490 : loss : 0.112096, loss_ce: 0.026050\n","iteration 10491 : loss : 0.120665, loss_ce: 0.055600\n","iteration 10492 : loss : 0.094617, loss_ce: 0.025685\n","iteration 10493 : loss : 0.122643, loss_ce: 0.030505\n","iteration 10494 : loss : 0.111494, loss_ce: 0.029500\n","iteration 10495 : loss : 0.148695, loss_ce: 0.032461\n","iteration 10496 : loss : 0.194687, loss_ce: 0.057740\n","iteration 10497 : loss : 0.112828, loss_ce: 0.034314\n","iteration 10498 : loss : 0.093102, loss_ce: 0.029786\n","iteration 10499 : loss : 0.189647, loss_ce: 0.023508\n","iteration 10500 : loss : 0.105666, loss_ce: 0.053468\n","iteration 10501 : loss : 0.109778, loss_ce: 0.035403\n","iteration 10502 : loss : 0.096171, loss_ce: 0.041579\n","iteration 10503 : loss : 0.140245, loss_ce: 0.036106\n","iteration 10504 : loss : 0.115704, loss_ce: 0.020671\n","iteration 10505 : loss : 0.114468, loss_ce: 0.030711\n","iteration 10506 : loss : 0.141329, loss_ce: 0.041571\n","iteration 10507 : loss : 0.103604, loss_ce: 0.028915\n","iteration 10508 : loss : 0.134956, loss_ce: 0.028648\n","iteration 10509 : loss : 0.460543, loss_ce: 0.000378\n"," 75%|█████████████████████▊       | 113/150 [2:27:47<49:54, 80.94s/it]iteration 10510 : loss : 0.102862, loss_ce: 0.018575\n","iteration 10511 : loss : 0.115003, loss_ce: 0.028214\n","iteration 10512 : loss : 0.218511, loss_ce: 0.023280\n","iteration 10513 : loss : 0.153794, loss_ce: 0.034824\n","iteration 10514 : loss : 0.186687, loss_ce: 0.037376\n","iteration 10515 : loss : 0.182692, loss_ce: 0.048005\n","iteration 10516 : loss : 0.111169, loss_ce: 0.043809\n","iteration 10517 : loss : 0.111781, loss_ce: 0.041178\n","iteration 10518 : loss : 0.114345, loss_ce: 0.042194\n","iteration 10519 : loss : 0.133098, loss_ce: 0.039934\n","iteration 10520 : loss : 0.202163, loss_ce: 0.034906\n","iteration 10521 : loss : 0.121703, loss_ce: 0.040137\n","iteration 10522 : loss : 0.148204, loss_ce: 0.028050\n","iteration 10523 : loss : 0.222359, loss_ce: 0.036394\n","iteration 10524 : loss : 0.126863, loss_ce: 0.052406\n","iteration 10525 : loss : 0.146654, loss_ce: 0.038292\n","iteration 10526 : loss : 0.152622, loss_ce: 0.031665\n","iteration 10527 : loss : 0.106445, loss_ce: 0.025257\n","iteration 10528 : loss : 0.167462, loss_ce: 0.027847\n","iteration 10529 : loss : 0.120597, loss_ce: 0.042481\n","iteration 10530 : loss : 0.120919, loss_ce: 0.039347\n","iteration 10531 : loss : 0.091987, loss_ce: 0.022649\n","iteration 10532 : loss : 0.119005, loss_ce: 0.018656\n","iteration 10533 : loss : 0.159363, loss_ce: 0.046387\n","iteration 10534 : loss : 0.121908, loss_ce: 0.029873\n","iteration 10535 : loss : 0.100508, loss_ce: 0.039771\n","iteration 10536 : loss : 0.130402, loss_ce: 0.043984\n","iteration 10537 : loss : 0.150420, loss_ce: 0.065141\n","iteration 10538 : loss : 0.127276, loss_ce: 0.033693\n","iteration 10539 : loss : 0.137585, loss_ce: 0.032293\n","iteration 10540 : loss : 0.241254, loss_ce: 0.013970\n","iteration 10541 : loss : 0.191469, loss_ce: 0.026378\n","iteration 10542 : loss : 0.144937, loss_ce: 0.025918\n","iteration 10543 : loss : 0.092406, loss_ce: 0.033873\n","iteration 10544 : loss : 0.099940, loss_ce: 0.043842\n","iteration 10545 : loss : 0.137842, loss_ce: 0.044239\n","iteration 10546 : loss : 0.114568, loss_ce: 0.034649\n","iteration 10547 : loss : 0.124107, loss_ce: 0.040882\n","iteration 10548 : loss : 0.141502, loss_ce: 0.035049\n","iteration 10549 : loss : 0.100005, loss_ce: 0.036947\n","iteration 10550 : loss : 0.178007, loss_ce: 0.025895\n","iteration 10551 : loss : 0.143926, loss_ce: 0.049566\n","iteration 10552 : loss : 0.108319, loss_ce: 0.043753\n","iteration 10553 : loss : 0.125187, loss_ce: 0.030792\n","iteration 10554 : loss : 0.101831, loss_ce: 0.037465\n","iteration 10555 : loss : 0.155300, loss_ce: 0.032929\n","iteration 10556 : loss : 0.120553, loss_ce: 0.033576\n","iteration 10557 : loss : 0.109325, loss_ce: 0.037873\n","iteration 10558 : loss : 0.131534, loss_ce: 0.023940\n","iteration 10559 : loss : 0.169922, loss_ce: 0.032171\n","iteration 10560 : loss : 0.122420, loss_ce: 0.043793\n","iteration 10561 : loss : 0.101965, loss_ce: 0.041664\n","iteration 10562 : loss : 0.123955, loss_ce: 0.033852\n","iteration 10563 : loss : 0.096910, loss_ce: 0.032536\n","iteration 10564 : loss : 0.095681, loss_ce: 0.033363\n","iteration 10565 : loss : 0.122497, loss_ce: 0.038439\n","iteration 10566 : loss : 0.113791, loss_ce: 0.037156\n","iteration 10567 : loss : 0.116914, loss_ce: 0.052634\n","iteration 10568 : loss : 0.120892, loss_ce: 0.038215\n","iteration 10569 : loss : 0.110978, loss_ce: 0.052760\n","iteration 10570 : loss : 0.138252, loss_ce: 0.033890\n","iteration 10571 : loss : 0.186056, loss_ce: 0.039581\n","iteration 10572 : loss : 0.112696, loss_ce: 0.034399\n","iteration 10573 : loss : 0.105556, loss_ce: 0.025407\n","iteration 10574 : loss : 0.117783, loss_ce: 0.040677\n","iteration 10575 : loss : 0.179951, loss_ce: 0.035456\n","iteration 10576 : loss : 0.091318, loss_ce: 0.037994\n","iteration 10577 : loss : 0.115234, loss_ce: 0.040980\n","iteration 10578 : loss : 0.110058, loss_ce: 0.037640\n","iteration 10579 : loss : 0.100498, loss_ce: 0.043289\n","iteration 10580 : loss : 0.124084, loss_ce: 0.021460\n","iteration 10581 : loss : 0.098803, loss_ce: 0.038111\n","iteration 10582 : loss : 0.128345, loss_ce: 0.023475\n","iteration 10583 : loss : 0.129009, loss_ce: 0.046311\n","iteration 10584 : loss : 0.098344, loss_ce: 0.033069\n","iteration 10585 : loss : 0.099505, loss_ce: 0.028056\n","iteration 10586 : loss : 0.109580, loss_ce: 0.022375\n","iteration 10587 : loss : 0.101880, loss_ce: 0.040440\n","iteration 10588 : loss : 0.164760, loss_ce: 0.027022\n","iteration 10589 : loss : 0.151542, loss_ce: 0.041181\n","iteration 10590 : loss : 0.117046, loss_ce: 0.037238\n","iteration 10591 : loss : 0.159937, loss_ce: 0.031459\n","iteration 10592 : loss : 0.117142, loss_ce: 0.020557\n","iteration 10593 : loss : 0.118861, loss_ce: 0.038226\n","iteration 10594 : loss : 0.221582, loss_ce: 0.022112\n","iteration 10595 : loss : 0.120628, loss_ce: 0.032206\n","iteration 10596 : loss : 0.137354, loss_ce: 0.022794\n","iteration 10597 : loss : 0.164644, loss_ce: 0.038965\n","iteration 10598 : loss : 0.126637, loss_ce: 0.035699\n","iteration 10599 : loss : 0.148756, loss_ce: 0.058198\n","iteration 10600 : loss : 0.117860, loss_ce: 0.043657\n","iteration 10601 : loss : 0.167720, loss_ce: 0.033160\n","iteration 10602 : loss : 0.154813, loss_ce: 0.045266\n"," 76%|██████████████████████       | 114/150 [2:29:04<47:50, 79.73s/it]iteration 10603 : loss : 0.125907, loss_ce: 0.032369\n","iteration 10604 : loss : 0.122579, loss_ce: 0.036342\n","iteration 10605 : loss : 0.124877, loss_ce: 0.042432\n","iteration 10606 : loss : 0.106104, loss_ce: 0.026799\n","iteration 10607 : loss : 0.122849, loss_ce: 0.029612\n","iteration 10608 : loss : 0.109572, loss_ce: 0.030084\n","iteration 10609 : loss : 0.115807, loss_ce: 0.026856\n","iteration 10610 : loss : 0.149905, loss_ce: 0.038697\n","iteration 10611 : loss : 0.120443, loss_ce: 0.037588\n","iteration 10612 : loss : 0.130809, loss_ce: 0.022055\n","iteration 10613 : loss : 0.095523, loss_ce: 0.034869\n","iteration 10614 : loss : 0.112729, loss_ce: 0.041372\n","iteration 10615 : loss : 0.140688, loss_ce: 0.041756\n","iteration 10616 : loss : 0.136590, loss_ce: 0.019977\n","iteration 10617 : loss : 0.101108, loss_ce: 0.039147\n","iteration 10618 : loss : 0.106182, loss_ce: 0.036144\n","iteration 10619 : loss : 0.107988, loss_ce: 0.028455\n","iteration 10620 : loss : 0.112024, loss_ce: 0.041121\n","iteration 10621 : loss : 0.105283, loss_ce: 0.031590\n","iteration 10622 : loss : 0.128040, loss_ce: 0.048345\n","iteration 10623 : loss : 0.126765, loss_ce: 0.027182\n","iteration 10624 : loss : 0.115146, loss_ce: 0.039308\n","iteration 10625 : loss : 0.175362, loss_ce: 0.039970\n","iteration 10626 : loss : 0.118930, loss_ce: 0.047061\n","iteration 10627 : loss : 0.145178, loss_ce: 0.028860\n","iteration 10628 : loss : 0.117119, loss_ce: 0.033649\n","iteration 10629 : loss : 0.109796, loss_ce: 0.032268\n","iteration 10630 : loss : 0.132150, loss_ce: 0.045316\n","iteration 10631 : loss : 0.106029, loss_ce: 0.040777\n","iteration 10632 : loss : 0.104673, loss_ce: 0.042286\n","iteration 10633 : loss : 0.110645, loss_ce: 0.037141\n","iteration 10634 : loss : 0.175212, loss_ce: 0.048313\n","iteration 10635 : loss : 0.148046, loss_ce: 0.019947\n","iteration 10636 : loss : 0.114881, loss_ce: 0.039022\n","iteration 10637 : loss : 0.107378, loss_ce: 0.039304\n","iteration 10638 : loss : 0.147415, loss_ce: 0.064973\n","iteration 10639 : loss : 0.136945, loss_ce: 0.039803\n","iteration 10640 : loss : 0.095117, loss_ce: 0.034982\n","iteration 10641 : loss : 0.111674, loss_ce: 0.033774\n","iteration 10642 : loss : 0.115917, loss_ce: 0.053579\n","iteration 10643 : loss : 0.105028, loss_ce: 0.039942\n","iteration 10644 : loss : 0.097910, loss_ce: 0.032057\n","iteration 10645 : loss : 0.118744, loss_ce: 0.024060\n","iteration 10646 : loss : 0.183138, loss_ce: 0.030503\n","iteration 10647 : loss : 0.107859, loss_ce: 0.039941\n","iteration 10648 : loss : 0.155220, loss_ce: 0.029670\n","iteration 10649 : loss : 0.196572, loss_ce: 0.035502\n","iteration 10650 : loss : 0.113639, loss_ce: 0.034354\n","iteration 10651 : loss : 0.132097, loss_ce: 0.044965\n","iteration 10652 : loss : 0.133929, loss_ce: 0.036234\n","iteration 10653 : loss : 0.122226, loss_ce: 0.037495\n","iteration 10654 : loss : 0.147647, loss_ce: 0.054111\n","iteration 10655 : loss : 0.131916, loss_ce: 0.039715\n","iteration 10656 : loss : 0.139789, loss_ce: 0.023587\n","iteration 10657 : loss : 0.108898, loss_ce: 0.028566\n","iteration 10658 : loss : 0.114842, loss_ce: 0.045511\n","iteration 10659 : loss : 0.141523, loss_ce: 0.039215\n","iteration 10660 : loss : 0.110821, loss_ce: 0.022849\n","iteration 10661 : loss : 0.155611, loss_ce: 0.033421\n","iteration 10662 : loss : 0.104826, loss_ce: 0.031945\n","iteration 10663 : loss : 0.169576, loss_ce: 0.038896\n","iteration 10664 : loss : 0.108761, loss_ce: 0.028017\n","iteration 10665 : loss : 0.121360, loss_ce: 0.028724\n","iteration 10666 : loss : 0.112225, loss_ce: 0.040339\n","iteration 10667 : loss : 0.138671, loss_ce: 0.043643\n","iteration 10668 : loss : 0.129111, loss_ce: 0.042735\n","iteration 10669 : loss : 0.146376, loss_ce: 0.021094\n","iteration 10670 : loss : 0.107981, loss_ce: 0.026353\n","iteration 10671 : loss : 0.191583, loss_ce: 0.054588\n","iteration 10672 : loss : 0.118841, loss_ce: 0.045500\n","iteration 10673 : loss : 0.138479, loss_ce: 0.028264\n","iteration 10674 : loss : 0.122173, loss_ce: 0.034711\n","iteration 10675 : loss : 0.103757, loss_ce: 0.024446\n","iteration 10676 : loss : 0.167892, loss_ce: 0.032010\n","iteration 10677 : loss : 0.190417, loss_ce: 0.018796\n","iteration 10678 : loss : 0.164684, loss_ce: 0.031310\n","iteration 10679 : loss : 0.147071, loss_ce: 0.065554\n","iteration 10680 : loss : 0.159587, loss_ce: 0.030888\n","iteration 10681 : loss : 0.117008, loss_ce: 0.038281\n","iteration 10682 : loss : 0.109784, loss_ce: 0.029469\n","iteration 10683 : loss : 0.127319, loss_ce: 0.032845\n","iteration 10684 : loss : 0.100496, loss_ce: 0.022388\n","iteration 10685 : loss : 0.145034, loss_ce: 0.046756\n","iteration 10686 : loss : 0.173298, loss_ce: 0.023329\n","iteration 10687 : loss : 0.120002, loss_ce: 0.044409\n","iteration 10688 : loss : 0.112617, loss_ce: 0.038420\n","iteration 10689 : loss : 0.107363, loss_ce: 0.033466\n","iteration 10690 : loss : 0.095443, loss_ce: 0.028430\n","iteration 10691 : loss : 0.124872, loss_ce: 0.036198\n","iteration 10692 : loss : 0.129842, loss_ce: 0.031125\n","iteration 10693 : loss : 0.170953, loss_ce: 0.029695\n","iteration 10694 : loss : 0.092909, loss_ce: 0.029161\n","iteration 10695 : loss : 0.303328, loss_ce: 0.034629\n"," 77%|██████████████████████▏      | 115/150 [2:30:25<46:53, 80.37s/it]iteration 10696 : loss : 0.113707, loss_ce: 0.038878\n","iteration 10697 : loss : 0.134878, loss_ce: 0.035227\n","iteration 10698 : loss : 0.141886, loss_ce: 0.027598\n","iteration 10699 : loss : 0.137914, loss_ce: 0.020203\n","iteration 10700 : loss : 0.104324, loss_ce: 0.019399\n","iteration 10701 : loss : 0.195386, loss_ce: 0.024501\n","iteration 10702 : loss : 0.122408, loss_ce: 0.023537\n","iteration 10703 : loss : 0.128974, loss_ce: 0.041827\n","iteration 10704 : loss : 0.147056, loss_ce: 0.030102\n","iteration 10705 : loss : 0.113960, loss_ce: 0.034586\n","iteration 10706 : loss : 0.113963, loss_ce: 0.048473\n","iteration 10707 : loss : 0.097046, loss_ce: 0.028242\n","iteration 10708 : loss : 0.143551, loss_ce: 0.051379\n","iteration 10709 : loss : 0.160603, loss_ce: 0.038196\n","iteration 10710 : loss : 0.113573, loss_ce: 0.041880\n","iteration 10711 : loss : 0.119537, loss_ce: 0.027727\n","iteration 10712 : loss : 0.126333, loss_ce: 0.048962\n","iteration 10713 : loss : 0.133123, loss_ce: 0.040184\n","iteration 10714 : loss : 0.127229, loss_ce: 0.029722\n","iteration 10715 : loss : 0.093897, loss_ce: 0.027098\n","iteration 10716 : loss : 0.159260, loss_ce: 0.028330\n","iteration 10717 : loss : 0.108785, loss_ce: 0.039544\n","iteration 10718 : loss : 0.117208, loss_ce: 0.040472\n","iteration 10719 : loss : 0.114732, loss_ce: 0.048511\n","iteration 10720 : loss : 0.121786, loss_ce: 0.032556\n","iteration 10721 : loss : 0.134515, loss_ce: 0.040746\n","iteration 10722 : loss : 0.082393, loss_ce: 0.022772\n","iteration 10723 : loss : 0.155680, loss_ce: 0.052345\n","iteration 10724 : loss : 0.128738, loss_ce: 0.034117\n","iteration 10725 : loss : 0.117960, loss_ce: 0.042280\n","iteration 10726 : loss : 0.115095, loss_ce: 0.040858\n","iteration 10727 : loss : 0.128568, loss_ce: 0.040551\n","iteration 10728 : loss : 0.109957, loss_ce: 0.033158\n","iteration 10729 : loss : 0.148060, loss_ce: 0.013295\n","iteration 10730 : loss : 0.120242, loss_ce: 0.048395\n","iteration 10731 : loss : 0.117172, loss_ce: 0.034981\n","iteration 10732 : loss : 0.140939, loss_ce: 0.034169\n","iteration 10733 : loss : 0.108068, loss_ce: 0.033308\n","iteration 10734 : loss : 0.098131, loss_ce: 0.037212\n","iteration 10735 : loss : 0.183049, loss_ce: 0.037966\n","iteration 10736 : loss : 0.092230, loss_ce: 0.030634\n","iteration 10737 : loss : 0.209934, loss_ce: 0.036714\n","iteration 10738 : loss : 0.204711, loss_ce: 0.024630\n","iteration 10739 : loss : 0.146267, loss_ce: 0.020642\n","iteration 10740 : loss : 0.099129, loss_ce: 0.028848\n","iteration 10741 : loss : 0.121196, loss_ce: 0.027169\n","iteration 10742 : loss : 0.120871, loss_ce: 0.026423\n","iteration 10743 : loss : 0.094287, loss_ce: 0.035506\n","iteration 10744 : loss : 0.202818, loss_ce: 0.022926\n","iteration 10745 : loss : 0.141395, loss_ce: 0.041718\n","iteration 10746 : loss : 0.120820, loss_ce: 0.033136\n","iteration 10747 : loss : 0.156338, loss_ce: 0.031941\n","iteration 10748 : loss : 0.168291, loss_ce: 0.034680\n","iteration 10749 : loss : 0.130665, loss_ce: 0.050669\n","iteration 10750 : loss : 0.094741, loss_ce: 0.034263\n","iteration 10751 : loss : 0.127229, loss_ce: 0.030259\n","iteration 10752 : loss : 0.141634, loss_ce: 0.028009\n","iteration 10753 : loss : 0.129932, loss_ce: 0.034446\n","iteration 10754 : loss : 0.125553, loss_ce: 0.038537\n","iteration 10755 : loss : 0.123583, loss_ce: 0.050389\n","iteration 10756 : loss : 0.227725, loss_ce: 0.023942\n","iteration 10757 : loss : 0.144927, loss_ce: 0.030380\n","iteration 10758 : loss : 0.105416, loss_ce: 0.039432\n","iteration 10759 : loss : 0.133492, loss_ce: 0.027080\n","iteration 10760 : loss : 0.098298, loss_ce: 0.048925\n","iteration 10761 : loss : 0.096247, loss_ce: 0.031351\n","iteration 10762 : loss : 0.113077, loss_ce: 0.047202\n","iteration 10763 : loss : 0.208500, loss_ce: 0.033049\n","iteration 10764 : loss : 0.102248, loss_ce: 0.017093\n","iteration 10765 : loss : 0.122892, loss_ce: 0.040217\n","iteration 10766 : loss : 0.089980, loss_ce: 0.028369\n","iteration 10767 : loss : 0.116977, loss_ce: 0.061571\n","iteration 10768 : loss : 0.121339, loss_ce: 0.034517\n","iteration 10769 : loss : 0.189307, loss_ce: 0.032359\n","iteration 10770 : loss : 0.113402, loss_ce: 0.029851\n","iteration 10771 : loss : 0.117053, loss_ce: 0.024634\n","iteration 10772 : loss : 0.134113, loss_ce: 0.035527\n","iteration 10773 : loss : 0.135268, loss_ce: 0.041969\n","iteration 10774 : loss : 0.102847, loss_ce: 0.028980\n","iteration 10775 : loss : 0.134468, loss_ce: 0.042825\n","iteration 10776 : loss : 0.124036, loss_ce: 0.035550\n","iteration 10777 : loss : 0.207494, loss_ce: 0.033579\n","iteration 10778 : loss : 0.166680, loss_ce: 0.032712\n","iteration 10779 : loss : 0.111484, loss_ce: 0.050946\n","iteration 10780 : loss : 0.107656, loss_ce: 0.032028\n","iteration 10781 : loss : 0.123085, loss_ce: 0.030778\n","iteration 10782 : loss : 0.115088, loss_ce: 0.046624\n","iteration 10783 : loss : 0.112035, loss_ce: 0.042522\n","iteration 10784 : loss : 0.093889, loss_ce: 0.026348\n","iteration 10785 : loss : 0.124251, loss_ce: 0.034229\n","iteration 10786 : loss : 0.121133, loss_ce: 0.046731\n","iteration 10787 : loss : 0.157711, loss_ce: 0.033566\n","iteration 10788 : loss : 0.165515, loss_ce: 0.083738\n"," 77%|██████████████████████▍      | 116/150 [2:31:45<45:28, 80.24s/it]iteration 10789 : loss : 0.119410, loss_ce: 0.055755\n","iteration 10790 : loss : 0.098592, loss_ce: 0.037800\n","iteration 10791 : loss : 0.104096, loss_ce: 0.023655\n","iteration 10792 : loss : 0.180514, loss_ce: 0.046764\n","iteration 10793 : loss : 0.207839, loss_ce: 0.028340\n","iteration 10794 : loss : 0.088188, loss_ce: 0.031957\n","iteration 10795 : loss : 0.096668, loss_ce: 0.026220\n","iteration 10796 : loss : 0.106102, loss_ce: 0.040158\n","iteration 10797 : loss : 0.108253, loss_ce: 0.040566\n","iteration 10798 : loss : 0.147326, loss_ce: 0.041765\n","iteration 10799 : loss : 0.134160, loss_ce: 0.055482\n","iteration 10800 : loss : 0.113347, loss_ce: 0.040111\n","iteration 10801 : loss : 0.122496, loss_ce: 0.044004\n","iteration 10802 : loss : 0.127129, loss_ce: 0.024187\n","iteration 10803 : loss : 0.108817, loss_ce: 0.037961\n","iteration 10804 : loss : 0.145954, loss_ce: 0.022467\n","iteration 10805 : loss : 0.151673, loss_ce: 0.025119\n","iteration 10806 : loss : 0.101508, loss_ce: 0.026888\n","iteration 10807 : loss : 0.086820, loss_ce: 0.019706\n","iteration 10808 : loss : 0.100016, loss_ce: 0.027377\n","iteration 10809 : loss : 0.119670, loss_ce: 0.039602\n","iteration 10810 : loss : 0.107425, loss_ce: 0.025516\n","iteration 10811 : loss : 0.129736, loss_ce: 0.033201\n","iteration 10812 : loss : 0.101307, loss_ce: 0.027318\n","iteration 10813 : loss : 0.141799, loss_ce: 0.051899\n","iteration 10814 : loss : 0.139870, loss_ce: 0.033357\n","iteration 10815 : loss : 0.109133, loss_ce: 0.040223\n","iteration 10816 : loss : 0.140479, loss_ce: 0.035383\n","iteration 10817 : loss : 0.116620, loss_ce: 0.017658\n","iteration 10818 : loss : 0.092446, loss_ce: 0.025750\n","iteration 10819 : loss : 0.120651, loss_ce: 0.027869\n","iteration 10820 : loss : 0.091771, loss_ce: 0.036199\n","iteration 10821 : loss : 0.097843, loss_ce: 0.025334\n","iteration 10822 : loss : 0.110840, loss_ce: 0.027566\n","iteration 10823 : loss : 0.150494, loss_ce: 0.019787\n","iteration 10824 : loss : 0.141677, loss_ce: 0.033568\n","iteration 10825 : loss : 0.136217, loss_ce: 0.027934\n","iteration 10826 : loss : 0.121182, loss_ce: 0.054517\n","iteration 10827 : loss : 0.142877, loss_ce: 0.025275\n","iteration 10828 : loss : 0.087773, loss_ce: 0.036898\n","iteration 10829 : loss : 0.113495, loss_ce: 0.039830\n","iteration 10830 : loss : 0.149196, loss_ce: 0.053694\n","iteration 10831 : loss : 0.098764, loss_ce: 0.041750\n","iteration 10832 : loss : 0.188033, loss_ce: 0.020469\n","iteration 10833 : loss : 0.094562, loss_ce: 0.031455\n","iteration 10834 : loss : 0.118326, loss_ce: 0.045291\n","iteration 10835 : loss : 0.119200, loss_ce: 0.035066\n","iteration 10836 : loss : 0.185551, loss_ce: 0.024067\n","iteration 10837 : loss : 0.119116, loss_ce: 0.056044\n","iteration 10838 : loss : 0.104626, loss_ce: 0.026826\n","iteration 10839 : loss : 0.163741, loss_ce: 0.023311\n","iteration 10840 : loss : 0.102939, loss_ce: 0.046850\n","iteration 10841 : loss : 0.154574, loss_ce: 0.025782\n","iteration 10842 : loss : 0.116694, loss_ce: 0.023483\n","iteration 10843 : loss : 0.097200, loss_ce: 0.037992\n","iteration 10844 : loss : 0.108653, loss_ce: 0.034097\n","iteration 10845 : loss : 0.127046, loss_ce: 0.026547\n","iteration 10846 : loss : 0.132030, loss_ce: 0.033013\n","iteration 10847 : loss : 0.142438, loss_ce: 0.043437\n","iteration 10848 : loss : 0.099472, loss_ce: 0.039003\n","iteration 10849 : loss : 0.138158, loss_ce: 0.028082\n","iteration 10850 : loss : 0.101204, loss_ce: 0.031489\n","iteration 10851 : loss : 0.163969, loss_ce: 0.032413\n","iteration 10852 : loss : 0.124375, loss_ce: 0.041389\n","iteration 10853 : loss : 0.114668, loss_ce: 0.016688\n","iteration 10854 : loss : 0.127585, loss_ce: 0.038831\n","iteration 10855 : loss : 0.127059, loss_ce: 0.046702\n","iteration 10856 : loss : 0.099317, loss_ce: 0.037833\n","iteration 10857 : loss : 0.138782, loss_ce: 0.036705\n","iteration 10858 : loss : 0.091644, loss_ce: 0.029848\n","iteration 10859 : loss : 0.140122, loss_ce: 0.036505\n","iteration 10860 : loss : 0.109925, loss_ce: 0.030340\n","iteration 10861 : loss : 0.146281, loss_ce: 0.036491\n","iteration 10862 : loss : 0.118402, loss_ce: 0.029164\n","iteration 10863 : loss : 0.094788, loss_ce: 0.028377\n","iteration 10864 : loss : 0.118140, loss_ce: 0.027260\n","iteration 10865 : loss : 0.102481, loss_ce: 0.032037\n","iteration 10866 : loss : 0.136538, loss_ce: 0.065484\n","iteration 10867 : loss : 0.107433, loss_ce: 0.018727\n","iteration 10868 : loss : 0.194114, loss_ce: 0.021117\n","iteration 10869 : loss : 0.090566, loss_ce: 0.034961\n","iteration 10870 : loss : 0.094114, loss_ce: 0.024887\n","iteration 10871 : loss : 0.095160, loss_ce: 0.039979\n","iteration 10872 : loss : 0.166279, loss_ce: 0.035264\n","iteration 10873 : loss : 0.124836, loss_ce: 0.034709\n","iteration 10874 : loss : 0.118888, loss_ce: 0.048588\n","iteration 10875 : loss : 0.163297, loss_ce: 0.038422\n","iteration 10876 : loss : 0.140862, loss_ce: 0.016550\n","iteration 10877 : loss : 0.183654, loss_ce: 0.020002\n","iteration 10878 : loss : 0.119984, loss_ce: 0.030559\n","iteration 10879 : loss : 0.134833, loss_ce: 0.044314\n","iteration 10880 : loss : 0.124184, loss_ce: 0.052702\n","iteration 10881 : loss : 0.251667, loss_ce: 0.035757\n"," 78%|██████████████████████▌      | 117/150 [2:33:03<43:37, 79.32s/it]iteration 10882 : loss : 0.108799, loss_ce: 0.035946\n","iteration 10883 : loss : 0.133721, loss_ce: 0.029010\n","iteration 10884 : loss : 0.136677, loss_ce: 0.027699\n","iteration 10885 : loss : 0.126030, loss_ce: 0.047402\n","iteration 10886 : loss : 0.106811, loss_ce: 0.031731\n","iteration 10887 : loss : 0.140949, loss_ce: 0.022796\n","iteration 10888 : loss : 0.119221, loss_ce: 0.043095\n","iteration 10889 : loss : 0.085993, loss_ce: 0.026101\n","iteration 10890 : loss : 0.100118, loss_ce: 0.034956\n","iteration 10891 : loss : 0.115322, loss_ce: 0.020736\n","iteration 10892 : loss : 0.126538, loss_ce: 0.045317\n","iteration 10893 : loss : 0.117105, loss_ce: 0.033316\n","iteration 10894 : loss : 0.217679, loss_ce: 0.036337\n","iteration 10895 : loss : 0.106066, loss_ce: 0.037039\n","iteration 10896 : loss : 0.100697, loss_ce: 0.040485\n","iteration 10897 : loss : 0.118913, loss_ce: 0.034621\n","iteration 10898 : loss : 0.092836, loss_ce: 0.034394\n","iteration 10899 : loss : 0.147315, loss_ce: 0.030676\n","iteration 10900 : loss : 0.150342, loss_ce: 0.032380\n","iteration 10901 : loss : 0.092306, loss_ce: 0.022292\n","iteration 10902 : loss : 0.103226, loss_ce: 0.039571\n","iteration 10903 : loss : 0.123670, loss_ce: 0.024903\n","iteration 10904 : loss : 0.167077, loss_ce: 0.049117\n","iteration 10905 : loss : 0.110467, loss_ce: 0.039211\n","iteration 10906 : loss : 0.110173, loss_ce: 0.040420\n","iteration 10907 : loss : 0.098633, loss_ce: 0.042200\n","iteration 10908 : loss : 0.106159, loss_ce: 0.047063\n","iteration 10909 : loss : 0.129839, loss_ce: 0.043587\n","iteration 10910 : loss : 0.119821, loss_ce: 0.040254\n","iteration 10911 : loss : 0.150483, loss_ce: 0.058189\n","iteration 10912 : loss : 0.076806, loss_ce: 0.035375\n","iteration 10913 : loss : 0.083624, loss_ce: 0.030026\n","iteration 10914 : loss : 0.127642, loss_ce: 0.033189\n","iteration 10915 : loss : 0.134297, loss_ce: 0.028694\n","iteration 10916 : loss : 0.094048, loss_ce: 0.031800\n","iteration 10917 : loss : 0.135778, loss_ce: 0.040121\n","iteration 10918 : loss : 0.117171, loss_ce: 0.029845\n","iteration 10919 : loss : 0.151199, loss_ce: 0.032572\n","iteration 10920 : loss : 0.139053, loss_ce: 0.032560\n","iteration 10921 : loss : 0.126735, loss_ce: 0.034049\n","iteration 10922 : loss : 0.120176, loss_ce: 0.040733\n","iteration 10923 : loss : 0.102918, loss_ce: 0.034398\n","iteration 10924 : loss : 0.105339, loss_ce: 0.026000\n","iteration 10925 : loss : 0.120028, loss_ce: 0.041920\n","iteration 10926 : loss : 0.127328, loss_ce: 0.015377\n","iteration 10927 : loss : 0.173070, loss_ce: 0.026877\n","iteration 10928 : loss : 0.090327, loss_ce: 0.028659\n","iteration 10929 : loss : 0.090843, loss_ce: 0.039496\n","iteration 10930 : loss : 0.163257, loss_ce: 0.028776\n","iteration 10931 : loss : 0.171070, loss_ce: 0.029920\n","iteration 10932 : loss : 0.119359, loss_ce: 0.034962\n","iteration 10933 : loss : 0.104772, loss_ce: 0.045276\n","iteration 10934 : loss : 0.126815, loss_ce: 0.035755\n","iteration 10935 : loss : 0.115074, loss_ce: 0.036824\n","iteration 10936 : loss : 0.166611, loss_ce: 0.041991\n","iteration 10937 : loss : 0.161730, loss_ce: 0.022866\n","iteration 10938 : loss : 0.109390, loss_ce: 0.037337\n","iteration 10939 : loss : 0.161461, loss_ce: 0.037656\n","iteration 10940 : loss : 0.192682, loss_ce: 0.030931\n","iteration 10941 : loss : 0.127968, loss_ce: 0.026852\n","iteration 10942 : loss : 0.120968, loss_ce: 0.034910\n","iteration 10943 : loss : 0.113348, loss_ce: 0.025655\n","iteration 10944 : loss : 0.139240, loss_ce: 0.036872\n","iteration 10945 : loss : 0.108770, loss_ce: 0.032929\n","iteration 10946 : loss : 0.104722, loss_ce: 0.029774\n","iteration 10947 : loss : 0.129629, loss_ce: 0.039685\n","iteration 10948 : loss : 0.200275, loss_ce: 0.028742\n","iteration 10949 : loss : 0.131365, loss_ce: 0.039368\n","iteration 10950 : loss : 0.103313, loss_ce: 0.023722\n","iteration 10951 : loss : 0.135943, loss_ce: 0.039635\n","iteration 10952 : loss : 0.110708, loss_ce: 0.025855\n","iteration 10953 : loss : 0.098444, loss_ce: 0.036416\n","iteration 10954 : loss : 0.215060, loss_ce: 0.029856\n","iteration 10955 : loss : 0.188446, loss_ce: 0.028580\n","iteration 10956 : loss : 0.093210, loss_ce: 0.025765\n","iteration 10957 : loss : 0.106525, loss_ce: 0.050981\n","iteration 10958 : loss : 0.212636, loss_ce: 0.020272\n","iteration 10959 : loss : 0.118612, loss_ce: 0.043325\n","iteration 10960 : loss : 0.107092, loss_ce: 0.046542\n","iteration 10961 : loss : 0.154830, loss_ce: 0.030427\n","iteration 10962 : loss : 0.104614, loss_ce: 0.027954\n","iteration 10963 : loss : 0.119451, loss_ce: 0.038620\n","iteration 10964 : loss : 0.127356, loss_ce: 0.037325\n","iteration 10965 : loss : 0.127455, loss_ce: 0.034653\n","iteration 10966 : loss : 0.111551, loss_ce: 0.027233\n","iteration 10967 : loss : 0.101570, loss_ce: 0.037603\n","iteration 10968 : loss : 0.124226, loss_ce: 0.028412\n","iteration 10969 : loss : 0.113181, loss_ce: 0.031107\n","iteration 10970 : loss : 0.134751, loss_ce: 0.033465\n","iteration 10971 : loss : 0.107826, loss_ce: 0.042560\n","iteration 10972 : loss : 0.098484, loss_ce: 0.036564\n","iteration 10973 : loss : 0.075034, loss_ce: 0.033456\n","iteration 10974 : loss : 0.492074, loss_ce: 0.000521\n"," 79%|██████████████████████▊      | 118/150 [2:34:22<42:17, 79.29s/it]iteration 10975 : loss : 0.103389, loss_ce: 0.047387\n","iteration 10976 : loss : 0.117007, loss_ce: 0.023040\n","iteration 10977 : loss : 0.116817, loss_ce: 0.047031\n","iteration 10978 : loss : 0.142716, loss_ce: 0.057059\n","iteration 10979 : loss : 0.121800, loss_ce: 0.027185\n","iteration 10980 : loss : 0.109714, loss_ce: 0.053611\n","iteration 10981 : loss : 0.144066, loss_ce: 0.030576\n","iteration 10982 : loss : 0.123466, loss_ce: 0.044422\n","iteration 10983 : loss : 0.108533, loss_ce: 0.035748\n","iteration 10984 : loss : 0.119538, loss_ce: 0.033295\n","iteration 10985 : loss : 0.142180, loss_ce: 0.030760\n","iteration 10986 : loss : 0.125573, loss_ce: 0.036169\n","iteration 10987 : loss : 0.130839, loss_ce: 0.045091\n","iteration 10988 : loss : 0.125766, loss_ce: 0.018303\n","iteration 10989 : loss : 0.120861, loss_ce: 0.026826\n","iteration 10990 : loss : 0.118942, loss_ce: 0.039227\n","iteration 10991 : loss : 0.127320, loss_ce: 0.035404\n","iteration 10992 : loss : 0.100641, loss_ce: 0.025050\n","iteration 10993 : loss : 0.128043, loss_ce: 0.032345\n","iteration 10994 : loss : 0.126881, loss_ce: 0.035537\n","iteration 10995 : loss : 0.132739, loss_ce: 0.035750\n","iteration 10996 : loss : 0.098386, loss_ce: 0.025130\n","iteration 10997 : loss : 0.124018, loss_ce: 0.031298\n","iteration 10998 : loss : 0.136216, loss_ce: 0.028462\n","iteration 10999 : loss : 0.125657, loss_ce: 0.040240\n","iteration 11000 : loss : 0.112285, loss_ce: 0.031256\n","iteration 11001 : loss : 0.114099, loss_ce: 0.016595\n","iteration 11002 : loss : 0.106410, loss_ce: 0.046429\n","iteration 11003 : loss : 0.147679, loss_ce: 0.033240\n","iteration 11004 : loss : 0.112234, loss_ce: 0.029652\n","iteration 11005 : loss : 0.101674, loss_ce: 0.028992\n","iteration 11006 : loss : 0.163470, loss_ce: 0.042452\n","iteration 11007 : loss : 0.108050, loss_ce: 0.042054\n","iteration 11008 : loss : 0.123373, loss_ce: 0.043744\n","iteration 11009 : loss : 0.116752, loss_ce: 0.033660\n","iteration 11010 : loss : 0.114921, loss_ce: 0.031601\n","iteration 11011 : loss : 0.115294, loss_ce: 0.021353\n","iteration 11012 : loss : 0.103979, loss_ce: 0.030021\n","iteration 11013 : loss : 0.092978, loss_ce: 0.028264\n","iteration 11014 : loss : 0.144366, loss_ce: 0.020521\n","iteration 11015 : loss : 0.150603, loss_ce: 0.026468\n","iteration 11016 : loss : 0.139295, loss_ce: 0.031010\n","iteration 11017 : loss : 0.146727, loss_ce: 0.016069\n","iteration 11018 : loss : 0.105149, loss_ce: 0.036518\n","iteration 11019 : loss : 0.131745, loss_ce: 0.044120\n","iteration 11020 : loss : 0.111874, loss_ce: 0.040534\n","iteration 11021 : loss : 0.108273, loss_ce: 0.034390\n","iteration 11022 : loss : 0.096713, loss_ce: 0.028283\n","iteration 11023 : loss : 0.105622, loss_ce: 0.024707\n","iteration 11024 : loss : 0.096365, loss_ce: 0.035604\n","iteration 11025 : loss : 0.133219, loss_ce: 0.024674\n","iteration 11026 : loss : 0.097118, loss_ce: 0.044293\n","iteration 11027 : loss : 0.112104, loss_ce: 0.040035\n","iteration 11028 : loss : 0.087291, loss_ce: 0.024937\n","iteration 11029 : loss : 0.091770, loss_ce: 0.029188\n","iteration 11030 : loss : 0.154110, loss_ce: 0.028654\n","iteration 11031 : loss : 0.163253, loss_ce: 0.020959\n","iteration 11032 : loss : 0.146402, loss_ce: 0.014878\n","iteration 11033 : loss : 0.142027, loss_ce: 0.022214\n","iteration 11034 : loss : 0.117049, loss_ce: 0.041708\n","iteration 11035 : loss : 0.110512, loss_ce: 0.027252\n","iteration 11036 : loss : 0.165459, loss_ce: 0.026771\n","iteration 11037 : loss : 0.104530, loss_ce: 0.033501\n","iteration 11038 : loss : 0.157799, loss_ce: 0.044030\n","iteration 11039 : loss : 0.117663, loss_ce: 0.039284\n","iteration 11040 : loss : 0.116043, loss_ce: 0.029937\n","iteration 11041 : loss : 0.164025, loss_ce: 0.031059\n","iteration 11042 : loss : 0.093145, loss_ce: 0.032260\n","iteration 11043 : loss : 0.104224, loss_ce: 0.043311\n","iteration 11044 : loss : 0.154341, loss_ce: 0.029380\n","iteration 11045 : loss : 0.157110, loss_ce: 0.027410\n","iteration 11046 : loss : 0.111584, loss_ce: 0.029352\n","iteration 11047 : loss : 0.144160, loss_ce: 0.044835\n","iteration 11048 : loss : 0.115389, loss_ce: 0.037513\n","iteration 11049 : loss : 0.102513, loss_ce: 0.038424\n","iteration 11050 : loss : 0.093505, loss_ce: 0.025742\n","iteration 11051 : loss : 0.121750, loss_ce: 0.027495\n","iteration 11052 : loss : 0.098324, loss_ce: 0.043692\n","iteration 11053 : loss : 0.105021, loss_ce: 0.028767\n","iteration 11054 : loss : 0.091916, loss_ce: 0.034064\n","iteration 11055 : loss : 0.098019, loss_ce: 0.037904\n","iteration 11056 : loss : 0.099005, loss_ce: 0.031633\n","iteration 11057 : loss : 0.122730, loss_ce: 0.040138\n","iteration 11058 : loss : 0.106041, loss_ce: 0.034841\n","iteration 11059 : loss : 0.172551, loss_ce: 0.048818\n","iteration 11060 : loss : 0.137567, loss_ce: 0.038388\n","iteration 11061 : loss : 0.192616, loss_ce: 0.042859\n","iteration 11062 : loss : 0.110038, loss_ce: 0.030992\n","iteration 11063 : loss : 0.173066, loss_ce: 0.029693\n","iteration 11064 : loss : 0.132556, loss_ce: 0.042811\n","iteration 11065 : loss : 0.130505, loss_ce: 0.020583\n","iteration 11066 : loss : 0.082451, loss_ce: 0.038484\n","iteration 11067 : loss : 0.313387, loss_ce: 0.033768\n"," 79%|███████████████████████      | 119/150 [2:35:41<40:55, 79.20s/it]iteration 11068 : loss : 0.099115, loss_ce: 0.024829\n","iteration 11069 : loss : 0.093904, loss_ce: 0.023154\n","iteration 11070 : loss : 0.082303, loss_ce: 0.026835\n","iteration 11071 : loss : 0.117690, loss_ce: 0.034842\n","iteration 11072 : loss : 0.134719, loss_ce: 0.037210\n","iteration 11073 : loss : 0.130529, loss_ce: 0.037381\n","iteration 11074 : loss : 0.126130, loss_ce: 0.037906\n","iteration 11075 : loss : 0.148591, loss_ce: 0.040093\n","iteration 11076 : loss : 0.125199, loss_ce: 0.034722\n","iteration 11077 : loss : 0.099547, loss_ce: 0.032125\n","iteration 11078 : loss : 0.111896, loss_ce: 0.033969\n","iteration 11079 : loss : 0.133525, loss_ce: 0.053526\n","iteration 11080 : loss : 0.092418, loss_ce: 0.037762\n","iteration 11081 : loss : 0.082715, loss_ce: 0.027255\n","iteration 11082 : loss : 0.095364, loss_ce: 0.029685\n","iteration 11083 : loss : 0.093047, loss_ce: 0.026885\n","iteration 11084 : loss : 0.090705, loss_ce: 0.017067\n","iteration 11085 : loss : 0.117176, loss_ce: 0.030374\n","iteration 11086 : loss : 0.096368, loss_ce: 0.037351\n","iteration 11087 : loss : 0.128777, loss_ce: 0.029065\n","iteration 11088 : loss : 0.138634, loss_ce: 0.052009\n","iteration 11089 : loss : 0.132732, loss_ce: 0.012170\n","iteration 11090 : loss : 0.139968, loss_ce: 0.023956\n","iteration 11091 : loss : 0.100924, loss_ce: 0.039944\n","iteration 11092 : loss : 0.127513, loss_ce: 0.046242\n","iteration 11093 : loss : 0.139197, loss_ce: 0.040044\n","iteration 11094 : loss : 0.131530, loss_ce: 0.024770\n","iteration 11095 : loss : 0.136142, loss_ce: 0.029637\n","iteration 11096 : loss : 0.102049, loss_ce: 0.040760\n","iteration 11097 : loss : 0.133814, loss_ce: 0.032671\n","iteration 11098 : loss : 0.103861, loss_ce: 0.035988\n","iteration 11099 : loss : 0.096559, loss_ce: 0.028413\n","iteration 11100 : loss : 0.162900, loss_ce: 0.022767\n","iteration 11101 : loss : 0.131735, loss_ce: 0.027241\n","iteration 11102 : loss : 0.107005, loss_ce: 0.042445\n","iteration 11103 : loss : 0.096037, loss_ce: 0.030618\n","iteration 11104 : loss : 0.116578, loss_ce: 0.031201\n","iteration 11105 : loss : 0.134996, loss_ce: 0.045473\n","iteration 11106 : loss : 0.106290, loss_ce: 0.038027\n","iteration 11107 : loss : 0.115018, loss_ce: 0.018561\n","iteration 11108 : loss : 0.093726, loss_ce: 0.027599\n","iteration 11109 : loss : 0.089392, loss_ce: 0.023090\n","iteration 11110 : loss : 0.116366, loss_ce: 0.033025\n","iteration 11111 : loss : 0.110076, loss_ce: 0.045178\n","iteration 11112 : loss : 0.129979, loss_ce: 0.044905\n","iteration 11113 : loss : 0.099227, loss_ce: 0.028816\n","iteration 11114 : loss : 0.128655, loss_ce: 0.026691\n","iteration 11115 : loss : 0.115801, loss_ce: 0.025344\n","iteration 11116 : loss : 0.133332, loss_ce: 0.030190\n","iteration 11117 : loss : 0.109308, loss_ce: 0.051181\n","iteration 11118 : loss : 0.104252, loss_ce: 0.048930\n","iteration 11119 : loss : 0.091857, loss_ce: 0.020108\n","iteration 11120 : loss : 0.092762, loss_ce: 0.041522\n","iteration 11121 : loss : 0.112491, loss_ce: 0.035346\n","iteration 11122 : loss : 0.098086, loss_ce: 0.016677\n","iteration 11123 : loss : 0.127894, loss_ce: 0.034627\n","iteration 11124 : loss : 0.095985, loss_ce: 0.026948\n","iteration 11125 : loss : 0.151583, loss_ce: 0.020036\n","iteration 11126 : loss : 0.107484, loss_ce: 0.043411\n","iteration 11127 : loss : 0.159404, loss_ce: 0.032497\n","iteration 11128 : loss : 0.147576, loss_ce: 0.041583\n","iteration 11129 : loss : 0.084557, loss_ce: 0.019296\n","iteration 11130 : loss : 0.091409, loss_ce: 0.031085\n","iteration 11131 : loss : 0.130926, loss_ce: 0.023020\n","iteration 11132 : loss : 0.136513, loss_ce: 0.046122\n","iteration 11133 : loss : 0.215759, loss_ce: 0.024594\n","iteration 11134 : loss : 0.113650, loss_ce: 0.034705\n","iteration 11135 : loss : 0.153564, loss_ce: 0.022627\n","iteration 11136 : loss : 0.181015, loss_ce: 0.033121\n","iteration 11137 : loss : 0.152777, loss_ce: 0.033893\n","iteration 11138 : loss : 0.135885, loss_ce: 0.022071\n","iteration 11139 : loss : 0.137102, loss_ce: 0.031093\n","iteration 11140 : loss : 0.136885, loss_ce: 0.039497\n","iteration 11141 : loss : 0.155301, loss_ce: 0.035428\n","iteration 11142 : loss : 0.095376, loss_ce: 0.028499\n","iteration 11143 : loss : 0.122456, loss_ce: 0.043437\n","iteration 11144 : loss : 0.152204, loss_ce: 0.022154\n","iteration 11145 : loss : 0.100702, loss_ce: 0.037125\n","iteration 11146 : loss : 0.107131, loss_ce: 0.035101\n","iteration 11147 : loss : 0.107855, loss_ce: 0.052714\n","iteration 11148 : loss : 0.110729, loss_ce: 0.030007\n","iteration 11149 : loss : 0.116707, loss_ce: 0.025343\n","iteration 11150 : loss : 0.175661, loss_ce: 0.044429\n","iteration 11151 : loss : 0.088715, loss_ce: 0.029905\n","iteration 11152 : loss : 0.108713, loss_ce: 0.037758\n","iteration 11153 : loss : 0.143790, loss_ce: 0.031679\n","iteration 11154 : loss : 0.133816, loss_ce: 0.034707\n","iteration 11155 : loss : 0.121260, loss_ce: 0.033177\n","iteration 11156 : loss : 0.124748, loss_ce: 0.047130\n","iteration 11157 : loss : 0.111733, loss_ce: 0.026289\n","iteration 11158 : loss : 0.188570, loss_ce: 0.030736\n","iteration 11159 : loss : 0.105414, loss_ce: 0.033019\n","iteration 11160 : loss : 0.152863, loss_ce: 0.046975\n"," 80%|███████████████████████▏     | 120/150 [2:36:57<39:07, 78.25s/it]iteration 11161 : loss : 0.118414, loss_ce: 0.027985\n","iteration 11162 : loss : 0.129173, loss_ce: 0.024310\n","iteration 11163 : loss : 0.111477, loss_ce: 0.030096\n","iteration 11164 : loss : 0.096076, loss_ce: 0.035409\n","iteration 11165 : loss : 0.156348, loss_ce: 0.038780\n","iteration 11166 : loss : 0.107257, loss_ce: 0.030314\n","iteration 11167 : loss : 0.166690, loss_ce: 0.025946\n","iteration 11168 : loss : 0.097181, loss_ce: 0.048236\n","iteration 11169 : loss : 0.087737, loss_ce: 0.032443\n","iteration 11170 : loss : 0.108020, loss_ce: 0.028184\n","iteration 11171 : loss : 0.192980, loss_ce: 0.013983\n","iteration 11172 : loss : 0.096454, loss_ce: 0.035693\n","iteration 11173 : loss : 0.092937, loss_ce: 0.024427\n","iteration 11174 : loss : 0.087923, loss_ce: 0.027178\n","iteration 11175 : loss : 0.096252, loss_ce: 0.022211\n","iteration 11176 : loss : 0.108918, loss_ce: 0.034421\n","iteration 11177 : loss : 0.102553, loss_ce: 0.028957\n","iteration 11178 : loss : 0.087019, loss_ce: 0.021123\n","iteration 11179 : loss : 0.076318, loss_ce: 0.023874\n","iteration 11180 : loss : 0.089409, loss_ce: 0.023879\n","iteration 11181 : loss : 0.107232, loss_ce: 0.031940\n","iteration 11182 : loss : 0.155338, loss_ce: 0.027336\n","iteration 11183 : loss : 0.091755, loss_ce: 0.030940\n","iteration 11184 : loss : 0.122570, loss_ce: 0.038420\n","iteration 11185 : loss : 0.105082, loss_ce: 0.029733\n","iteration 11186 : loss : 0.110399, loss_ce: 0.038414\n","iteration 11187 : loss : 0.087730, loss_ce: 0.023252\n","iteration 11188 : loss : 0.127953, loss_ce: 0.044257\n","iteration 11189 : loss : 0.088172, loss_ce: 0.037006\n","iteration 11190 : loss : 0.104542, loss_ce: 0.039756\n","iteration 11191 : loss : 0.096641, loss_ce: 0.027338\n","iteration 11192 : loss : 0.086892, loss_ce: 0.025114\n","iteration 11193 : loss : 0.092170, loss_ce: 0.031466\n","iteration 11194 : loss : 0.089774, loss_ce: 0.039562\n","iteration 11195 : loss : 0.128910, loss_ce: 0.033603\n","iteration 11196 : loss : 0.090203, loss_ce: 0.029238\n","iteration 11197 : loss : 0.109631, loss_ce: 0.028940\n","iteration 11198 : loss : 0.089015, loss_ce: 0.027870\n","iteration 11199 : loss : 0.112211, loss_ce: 0.036298\n","iteration 11200 : loss : 0.164921, loss_ce: 0.033066\n","iteration 11201 : loss : 0.108291, loss_ce: 0.041669\n","iteration 11202 : loss : 0.090829, loss_ce: 0.019783\n","iteration 11203 : loss : 0.095985, loss_ce: 0.025369\n","iteration 11204 : loss : 0.138143, loss_ce: 0.054853\n","iteration 11205 : loss : 0.094287, loss_ce: 0.019266\n","iteration 11206 : loss : 0.116713, loss_ce: 0.035405\n","iteration 11207 : loss : 0.138587, loss_ce: 0.036028\n","iteration 11208 : loss : 0.139623, loss_ce: 0.026931\n","iteration 11209 : loss : 0.137549, loss_ce: 0.034960\n","iteration 11210 : loss : 0.090008, loss_ce: 0.029564\n","iteration 11211 : loss : 0.102709, loss_ce: 0.018677\n","iteration 11212 : loss : 0.096660, loss_ce: 0.034261\n","iteration 11213 : loss : 0.176392, loss_ce: 0.024543\n","iteration 11214 : loss : 0.102848, loss_ce: 0.033994\n","iteration 11215 : loss : 0.090406, loss_ce: 0.033447\n","iteration 11216 : loss : 0.102864, loss_ce: 0.027340\n","iteration 11217 : loss : 0.157965, loss_ce: 0.023756\n","iteration 11218 : loss : 0.146071, loss_ce: 0.028577\n","iteration 11219 : loss : 0.099926, loss_ce: 0.027323\n","iteration 11220 : loss : 0.128722, loss_ce: 0.021353\n","iteration 11221 : loss : 0.123888, loss_ce: 0.060140\n","iteration 11222 : loss : 0.115683, loss_ce: 0.029575\n","iteration 11223 : loss : 0.099495, loss_ce: 0.036083\n","iteration 11224 : loss : 0.118427, loss_ce: 0.025296\n","iteration 11225 : loss : 0.096379, loss_ce: 0.032778\n","iteration 11226 : loss : 0.084706, loss_ce: 0.030424\n","iteration 11227 : loss : 0.119582, loss_ce: 0.023721\n","iteration 11228 : loss : 0.127782, loss_ce: 0.055313\n","iteration 11229 : loss : 0.100343, loss_ce: 0.038610\n","iteration 11230 : loss : 0.157376, loss_ce: 0.034516\n","iteration 11231 : loss : 0.137952, loss_ce: 0.038050\n","iteration 11232 : loss : 0.122425, loss_ce: 0.019004\n","iteration 11233 : loss : 0.129078, loss_ce: 0.037031\n","iteration 11234 : loss : 0.105953, loss_ce: 0.033831\n","iteration 11235 : loss : 0.113401, loss_ce: 0.048596\n","iteration 11236 : loss : 0.117780, loss_ce: 0.025700\n","iteration 11237 : loss : 0.156275, loss_ce: 0.028496\n","iteration 11238 : loss : 0.088495, loss_ce: 0.037452\n","iteration 11239 : loss : 0.130291, loss_ce: 0.029512\n","iteration 11240 : loss : 0.081783, loss_ce: 0.024492\n","iteration 11241 : loss : 0.099689, loss_ce: 0.025880\n","iteration 11242 : loss : 0.116037, loss_ce: 0.028179\n","iteration 11243 : loss : 0.107942, loss_ce: 0.035020\n","iteration 11244 : loss : 0.120964, loss_ce: 0.047330\n","iteration 11245 : loss : 0.130198, loss_ce: 0.032500\n","iteration 11246 : loss : 0.101798, loss_ce: 0.035871\n","iteration 11247 : loss : 0.099639, loss_ce: 0.042808\n","iteration 11248 : loss : 0.154821, loss_ce: 0.038653\n","iteration 11249 : loss : 0.106721, loss_ce: 0.039172\n","iteration 11250 : loss : 0.103871, loss_ce: 0.036790\n","iteration 11251 : loss : 0.102121, loss_ce: 0.018690\n","iteration 11252 : loss : 0.097490, loss_ce: 0.029741\n","iteration 11253 : loss : 0.372246, loss_ce: 0.023505\n"," 81%|███████████████████████▍     | 121/150 [2:38:17<38:07, 78.87s/it]iteration 11254 : loss : 0.126725, loss_ce: 0.043595\n","iteration 11255 : loss : 0.091540, loss_ce: 0.018543\n","iteration 11256 : loss : 0.166218, loss_ce: 0.031240\n","iteration 11257 : loss : 0.103602, loss_ce: 0.033267\n","iteration 11258 : loss : 0.143790, loss_ce: 0.038586\n","iteration 11259 : loss : 0.101839, loss_ce: 0.036950\n","iteration 11260 : loss : 0.139080, loss_ce: 0.033395\n","iteration 11261 : loss : 0.087113, loss_ce: 0.027926\n","iteration 11262 : loss : 0.102072, loss_ce: 0.034216\n","iteration 11263 : loss : 0.112117, loss_ce: 0.021628\n","iteration 11264 : loss : 0.110633, loss_ce: 0.038202\n","iteration 11265 : loss : 0.162916, loss_ce: 0.035543\n","iteration 11266 : loss : 0.103548, loss_ce: 0.035363\n","iteration 11267 : loss : 0.182319, loss_ce: 0.028052\n","iteration 11268 : loss : 0.098380, loss_ce: 0.022450\n","iteration 11269 : loss : 0.103927, loss_ce: 0.028829\n","iteration 11270 : loss : 0.095856, loss_ce: 0.027102\n","iteration 11271 : loss : 0.098878, loss_ce: 0.028146\n","iteration 11272 : loss : 0.127047, loss_ce: 0.017119\n","iteration 11273 : loss : 0.111440, loss_ce: 0.044867\n","iteration 11274 : loss : 0.102381, loss_ce: 0.029628\n","iteration 11275 : loss : 0.091413, loss_ce: 0.022448\n","iteration 11276 : loss : 0.140449, loss_ce: 0.055554\n","iteration 11277 : loss : 0.135361, loss_ce: 0.019822\n","iteration 11278 : loss : 0.125397, loss_ce: 0.027168\n","iteration 11279 : loss : 0.090861, loss_ce: 0.037397\n","iteration 11280 : loss : 0.112060, loss_ce: 0.029401\n","iteration 11281 : loss : 0.209941, loss_ce: 0.023675\n","iteration 11282 : loss : 0.102071, loss_ce: 0.031429\n","iteration 11283 : loss : 0.132493, loss_ce: 0.033590\n","iteration 11284 : loss : 0.141939, loss_ce: 0.013847\n","iteration 11285 : loss : 0.093696, loss_ce: 0.037589\n","iteration 11286 : loss : 0.268443, loss_ce: 0.029922\n","iteration 11287 : loss : 0.118148, loss_ce: 0.025274\n","iteration 11288 : loss : 0.107822, loss_ce: 0.033376\n","iteration 11289 : loss : 0.103806, loss_ce: 0.043431\n","iteration 11290 : loss : 0.114421, loss_ce: 0.033899\n","iteration 11291 : loss : 0.091018, loss_ce: 0.036084\n","iteration 11292 : loss : 0.125711, loss_ce: 0.039169\n","iteration 11293 : loss : 0.089015, loss_ce: 0.026136\n","iteration 11294 : loss : 0.111217, loss_ce: 0.033835\n","iteration 11295 : loss : 0.149362, loss_ce: 0.015528\n","iteration 11296 : loss : 0.105873, loss_ce: 0.037619\n","iteration 11297 : loss : 0.124543, loss_ce: 0.028992\n","iteration 11298 : loss : 0.117455, loss_ce: 0.048844\n","iteration 11299 : loss : 0.114324, loss_ce: 0.030288\n","iteration 11300 : loss : 0.101417, loss_ce: 0.033419\n","iteration 11301 : loss : 0.113149, loss_ce: 0.024879\n","iteration 11302 : loss : 0.137277, loss_ce: 0.049819\n","iteration 11303 : loss : 0.114675, loss_ce: 0.029602\n","iteration 11304 : loss : 0.110742, loss_ce: 0.048755\n","iteration 11305 : loss : 0.116188, loss_ce: 0.040979\n","iteration 11306 : loss : 0.105945, loss_ce: 0.031386\n","iteration 11307 : loss : 0.093859, loss_ce: 0.032801\n","iteration 11308 : loss : 0.125390, loss_ce: 0.027744\n","iteration 11309 : loss : 0.128721, loss_ce: 0.033368\n","iteration 11310 : loss : 0.170445, loss_ce: 0.033117\n","iteration 11311 : loss : 0.133383, loss_ce: 0.049705\n","iteration 11312 : loss : 0.140003, loss_ce: 0.036934\n","iteration 11313 : loss : 0.104847, loss_ce: 0.047290\n","iteration 11314 : loss : 0.116761, loss_ce: 0.037001\n","iteration 11315 : loss : 0.132057, loss_ce: 0.031945\n","iteration 11316 : loss : 0.087527, loss_ce: 0.032520\n","iteration 11317 : loss : 0.152316, loss_ce: 0.030216\n","iteration 11318 : loss : 0.111242, loss_ce: 0.040436\n","iteration 11319 : loss : 0.097683, loss_ce: 0.023005\n","iteration 11320 : loss : 0.090304, loss_ce: 0.026150\n","iteration 11321 : loss : 0.172751, loss_ce: 0.037121\n","iteration 11322 : loss : 0.155678, loss_ce: 0.037228\n","iteration 11323 : loss : 0.120683, loss_ce: 0.031993\n","iteration 11324 : loss : 0.076675, loss_ce: 0.019731\n","iteration 11325 : loss : 0.095752, loss_ce: 0.029305\n","iteration 11326 : loss : 0.122019, loss_ce: 0.050261\n","iteration 11327 : loss : 0.081678, loss_ce: 0.029429\n","iteration 11328 : loss : 0.121230, loss_ce: 0.025238\n","iteration 11329 : loss : 0.085636, loss_ce: 0.026645\n","iteration 11330 : loss : 0.114617, loss_ce: 0.028924\n","iteration 11331 : loss : 0.092802, loss_ce: 0.032816\n","iteration 11332 : loss : 0.121666, loss_ce: 0.029322\n","iteration 11333 : loss : 0.120540, loss_ce: 0.026211\n","iteration 11334 : loss : 0.107680, loss_ce: 0.040673\n","iteration 11335 : loss : 0.103904, loss_ce: 0.033840\n","iteration 11336 : loss : 0.099172, loss_ce: 0.020255\n","iteration 11337 : loss : 0.097020, loss_ce: 0.023112\n","iteration 11338 : loss : 0.163219, loss_ce: 0.017048\n","iteration 11339 : loss : 0.103928, loss_ce: 0.035612\n","iteration 11340 : loss : 0.102954, loss_ce: 0.031297\n","iteration 11341 : loss : 0.089372, loss_ce: 0.030434\n","iteration 11342 : loss : 0.117052, loss_ce: 0.035849\n","iteration 11343 : loss : 0.098823, loss_ce: 0.031597\n","iteration 11344 : loss : 0.086056, loss_ce: 0.030540\n","iteration 11345 : loss : 0.091473, loss_ce: 0.042109\n","iteration 11346 : loss : 0.432844, loss_ce: 0.022925\n"," 81%|███████████████████████▌     | 122/150 [2:39:39<37:14, 79.80s/it]iteration 11347 : loss : 0.100523, loss_ce: 0.028596\n","iteration 11348 : loss : 0.177207, loss_ce: 0.027351\n","iteration 11349 : loss : 0.106760, loss_ce: 0.038188\n","iteration 11350 : loss : 0.105721, loss_ce: 0.049308\n","iteration 11351 : loss : 0.122564, loss_ce: 0.028287\n","iteration 11352 : loss : 0.108126, loss_ce: 0.030259\n","iteration 11353 : loss : 0.100391, loss_ce: 0.031541\n","iteration 11354 : loss : 0.149732, loss_ce: 0.014401\n","iteration 11355 : loss : 0.082255, loss_ce: 0.026139\n","iteration 11356 : loss : 0.213141, loss_ce: 0.054346\n","iteration 11357 : loss : 0.109035, loss_ce: 0.054690\n","iteration 11358 : loss : 0.109060, loss_ce: 0.033892\n","iteration 11359 : loss : 0.128372, loss_ce: 0.041469\n","iteration 11360 : loss : 0.074916, loss_ce: 0.025089\n","iteration 11361 : loss : 0.126475, loss_ce: 0.032546\n","iteration 11362 : loss : 0.105545, loss_ce: 0.038934\n","iteration 11363 : loss : 0.138222, loss_ce: 0.037993\n","iteration 11364 : loss : 0.094861, loss_ce: 0.036819\n","iteration 11365 : loss : 0.098615, loss_ce: 0.013331\n","iteration 11366 : loss : 0.101598, loss_ce: 0.036521\n","iteration 11367 : loss : 0.071439, loss_ce: 0.024447\n","iteration 11368 : loss : 0.128190, loss_ce: 0.036303\n","iteration 11369 : loss : 0.156071, loss_ce: 0.029702\n","iteration 11370 : loss : 0.090082, loss_ce: 0.041448\n","iteration 11371 : loss : 0.097596, loss_ce: 0.039235\n","iteration 11372 : loss : 0.168896, loss_ce: 0.024030\n","iteration 11373 : loss : 0.108406, loss_ce: 0.027006\n","iteration 11374 : loss : 0.123978, loss_ce: 0.041092\n","iteration 11375 : loss : 0.100386, loss_ce: 0.023490\n","iteration 11376 : loss : 0.130974, loss_ce: 0.040480\n","iteration 11377 : loss : 0.120145, loss_ce: 0.035333\n","iteration 11378 : loss : 0.103304, loss_ce: 0.033044\n","iteration 11379 : loss : 0.114211, loss_ce: 0.022507\n","iteration 11380 : loss : 0.092888, loss_ce: 0.017992\n","iteration 11381 : loss : 0.086512, loss_ce: 0.017253\n","iteration 11382 : loss : 0.119159, loss_ce: 0.025390\n","iteration 11383 : loss : 0.108931, loss_ce: 0.046027\n","iteration 11384 : loss : 0.109271, loss_ce: 0.029380\n","iteration 11385 : loss : 0.143941, loss_ce: 0.035219\n","iteration 11386 : loss : 0.093526, loss_ce: 0.031654\n","iteration 11387 : loss : 0.096146, loss_ce: 0.024029\n","iteration 11388 : loss : 0.097209, loss_ce: 0.017685\n","iteration 11389 : loss : 0.108374, loss_ce: 0.033268\n","iteration 11390 : loss : 0.178485, loss_ce: 0.028963\n","iteration 11391 : loss : 0.084001, loss_ce: 0.033688\n","iteration 11392 : loss : 0.129832, loss_ce: 0.049440\n","iteration 11393 : loss : 0.095723, loss_ce: 0.034201\n","iteration 11394 : loss : 0.095102, loss_ce: 0.028922\n","iteration 11395 : loss : 0.139874, loss_ce: 0.038718\n","iteration 11396 : loss : 0.102972, loss_ce: 0.042948\n","iteration 11397 : loss : 0.141298, loss_ce: 0.026742\n","iteration 11398 : loss : 0.230093, loss_ce: 0.028117\n","iteration 11399 : loss : 0.115696, loss_ce: 0.042911\n","iteration 11400 : loss : 0.096760, loss_ce: 0.023232\n","iteration 11401 : loss : 0.086493, loss_ce: 0.041556\n","iteration 11402 : loss : 0.121142, loss_ce: 0.024507\n","iteration 11403 : loss : 0.087220, loss_ce: 0.018852\n","iteration 11404 : loss : 0.092036, loss_ce: 0.030871\n","iteration 11405 : loss : 0.119546, loss_ce: 0.036539\n","iteration 11406 : loss : 0.117692, loss_ce: 0.028871\n","iteration 11407 : loss : 0.145851, loss_ce: 0.034498\n","iteration 11408 : loss : 0.113093, loss_ce: 0.038621\n","iteration 11409 : loss : 0.115589, loss_ce: 0.044879\n","iteration 11410 : loss : 0.093572, loss_ce: 0.038468\n","iteration 11411 : loss : 0.131402, loss_ce: 0.020601\n","iteration 11412 : loss : 0.120806, loss_ce: 0.021515\n","iteration 11413 : loss : 0.139130, loss_ce: 0.025615\n","iteration 11414 : loss : 0.111442, loss_ce: 0.041560\n","iteration 11415 : loss : 0.100654, loss_ce: 0.022510\n","iteration 11416 : loss : 0.109234, loss_ce: 0.023160\n","iteration 11417 : loss : 0.104173, loss_ce: 0.047176\n","iteration 11418 : loss : 0.103903, loss_ce: 0.026300\n","iteration 11419 : loss : 0.158477, loss_ce: 0.033814\n","iteration 11420 : loss : 0.094859, loss_ce: 0.019947\n","iteration 11421 : loss : 0.115455, loss_ce: 0.041244\n","iteration 11422 : loss : 0.118224, loss_ce: 0.032100\n","iteration 11423 : loss : 0.120919, loss_ce: 0.044146\n","iteration 11424 : loss : 0.106236, loss_ce: 0.026671\n","iteration 11425 : loss : 0.089386, loss_ce: 0.037066\n","iteration 11426 : loss : 0.131158, loss_ce: 0.023406\n","iteration 11427 : loss : 0.122222, loss_ce: 0.026510\n","iteration 11428 : loss : 0.083468, loss_ce: 0.028109\n","iteration 11429 : loss : 0.126800, loss_ce: 0.040783\n","iteration 11430 : loss : 0.107950, loss_ce: 0.032689\n","iteration 11431 : loss : 0.103994, loss_ce: 0.023561\n","iteration 11432 : loss : 0.097082, loss_ce: 0.039838\n","iteration 11433 : loss : 0.117573, loss_ce: 0.031628\n","iteration 11434 : loss : 0.092019, loss_ce: 0.032781\n","iteration 11435 : loss : 0.092684, loss_ce: 0.017198\n","iteration 11436 : loss : 0.152473, loss_ce: 0.016729\n","iteration 11437 : loss : 0.105948, loss_ce: 0.035180\n","iteration 11438 : loss : 0.079719, loss_ce: 0.022247\n","iteration 11439 : loss : 0.224163, loss_ce: 0.058880\n"," 82%|███████████████████████▊     | 123/150 [2:40:53<35:09, 78.13s/it]iteration 11440 : loss : 0.107263, loss_ce: 0.017619\n","iteration 11441 : loss : 0.107130, loss_ce: 0.024312\n","iteration 11442 : loss : 0.187052, loss_ce: 0.037617\n","iteration 11443 : loss : 0.089924, loss_ce: 0.027767\n","iteration 11444 : loss : 0.096471, loss_ce: 0.042734\n","iteration 11445 : loss : 0.112423, loss_ce: 0.026647\n","iteration 11446 : loss : 0.105053, loss_ce: 0.040972\n","iteration 11447 : loss : 0.087984, loss_ce: 0.030984\n","iteration 11448 : loss : 0.098720, loss_ce: 0.034962\n","iteration 11449 : loss : 0.093059, loss_ce: 0.033610\n","iteration 11450 : loss : 0.159832, loss_ce: 0.022001\n","iteration 11451 : loss : 0.101151, loss_ce: 0.031650\n","iteration 11452 : loss : 0.105379, loss_ce: 0.032048\n","iteration 11453 : loss : 0.091805, loss_ce: 0.031196\n","iteration 11454 : loss : 0.124673, loss_ce: 0.047255\n","iteration 11455 : loss : 0.107782, loss_ce: 0.032807\n","iteration 11456 : loss : 0.103246, loss_ce: 0.019383\n","iteration 11457 : loss : 0.106836, loss_ce: 0.031367\n","iteration 11458 : loss : 0.153678, loss_ce: 0.025915\n","iteration 11459 : loss : 0.148390, loss_ce: 0.023760\n","iteration 11460 : loss : 0.086906, loss_ce: 0.017115\n","iteration 11461 : loss : 0.083749, loss_ce: 0.019945\n","iteration 11462 : loss : 0.110262, loss_ce: 0.020924\n","iteration 11463 : loss : 0.115102, loss_ce: 0.028872\n","iteration 11464 : loss : 0.120164, loss_ce: 0.023878\n","iteration 11465 : loss : 0.087788, loss_ce: 0.022909\n","iteration 11466 : loss : 0.127884, loss_ce: 0.027687\n","iteration 11467 : loss : 0.107282, loss_ce: 0.023998\n","iteration 11468 : loss : 0.171062, loss_ce: 0.028672\n","iteration 11469 : loss : 0.110309, loss_ce: 0.024849\n","iteration 11470 : loss : 0.087204, loss_ce: 0.023355\n","iteration 11471 : loss : 0.099780, loss_ce: 0.019970\n","iteration 11472 : loss : 0.187149, loss_ce: 0.024271\n","iteration 11473 : loss : 0.129857, loss_ce: 0.035313\n","iteration 11474 : loss : 0.116048, loss_ce: 0.053751\n","iteration 11475 : loss : 0.117455, loss_ce: 0.024049\n","iteration 11476 : loss : 0.135221, loss_ce: 0.039723\n","iteration 11477 : loss : 0.082949, loss_ce: 0.031226\n","iteration 11478 : loss : 0.112769, loss_ce: 0.051838\n","iteration 11479 : loss : 0.113350, loss_ce: 0.038721\n","iteration 11480 : loss : 0.094281, loss_ce: 0.033622\n","iteration 11481 : loss : 0.130321, loss_ce: 0.022264\n","iteration 11482 : loss : 0.098585, loss_ce: 0.032462\n","iteration 11483 : loss : 0.117927, loss_ce: 0.037494\n","iteration 11484 : loss : 0.091156, loss_ce: 0.037472\n","iteration 11485 : loss : 0.094360, loss_ce: 0.045057\n","iteration 11486 : loss : 0.089757, loss_ce: 0.031199\n","iteration 11487 : loss : 0.117036, loss_ce: 0.032669\n","iteration 11488 : loss : 0.153420, loss_ce: 0.031379\n","iteration 11489 : loss : 0.091485, loss_ce: 0.034129\n","iteration 11490 : loss : 0.101581, loss_ce: 0.029810\n","iteration 11491 : loss : 0.114354, loss_ce: 0.028500\n","iteration 11492 : loss : 0.081812, loss_ce: 0.027850\n","iteration 11493 : loss : 0.103950, loss_ce: 0.024548\n","iteration 11494 : loss : 0.107535, loss_ce: 0.025290\n","iteration 11495 : loss : 0.076625, loss_ce: 0.026621\n","iteration 11496 : loss : 0.078373, loss_ce: 0.020915\n","iteration 11497 : loss : 0.122503, loss_ce: 0.038741\n","iteration 11498 : loss : 0.234253, loss_ce: 0.029843\n","iteration 11499 : loss : 0.131824, loss_ce: 0.041068\n","iteration 11500 : loss : 0.099196, loss_ce: 0.036383\n","iteration 11501 : loss : 0.117437, loss_ce: 0.038147\n","iteration 11502 : loss : 0.117513, loss_ce: 0.035460\n","iteration 11503 : loss : 0.106305, loss_ce: 0.042930\n","iteration 11504 : loss : 0.135543, loss_ce: 0.035420\n","iteration 11505 : loss : 0.130304, loss_ce: 0.047174\n","iteration 11506 : loss : 0.092729, loss_ce: 0.031509\n","iteration 11507 : loss : 0.123827, loss_ce: 0.038069\n","iteration 11508 : loss : 0.100195, loss_ce: 0.027992\n","iteration 11509 : loss : 0.131511, loss_ce: 0.037977\n","iteration 11510 : loss : 0.183071, loss_ce: 0.027486\n","iteration 11511 : loss : 0.093631, loss_ce: 0.026157\n","iteration 11512 : loss : 0.137914, loss_ce: 0.048055\n","iteration 11513 : loss : 0.082087, loss_ce: 0.023339\n","iteration 11514 : loss : 0.095224, loss_ce: 0.028133\n","iteration 11515 : loss : 0.174992, loss_ce: 0.027276\n","iteration 11516 : loss : 0.115076, loss_ce: 0.039702\n","iteration 11517 : loss : 0.112053, loss_ce: 0.039783\n","iteration 11518 : loss : 0.133513, loss_ce: 0.051448\n","iteration 11519 : loss : 0.104080, loss_ce: 0.029779\n","iteration 11520 : loss : 0.128294, loss_ce: 0.031831\n","iteration 11521 : loss : 0.090905, loss_ce: 0.039131\n","iteration 11522 : loss : 0.068464, loss_ce: 0.023176\n","iteration 11523 : loss : 0.097255, loss_ce: 0.022741\n","iteration 11524 : loss : 0.144227, loss_ce: 0.021922\n","iteration 11525 : loss : 0.180483, loss_ce: 0.013231\n","iteration 11526 : loss : 0.096944, loss_ce: 0.023264\n","iteration 11527 : loss : 0.102085, loss_ce: 0.041643\n","iteration 11528 : loss : 0.111287, loss_ce: 0.037442\n","iteration 11529 : loss : 0.119594, loss_ce: 0.037701\n","iteration 11530 : loss : 0.097197, loss_ce: 0.028283\n","iteration 11531 : loss : 0.122163, loss_ce: 0.027711\n","iteration 11532 : loss : 0.310939, loss_ce: 0.069753\n"," 83%|███████████████████████▉     | 124/150 [2:42:14<34:10, 78.86s/it]iteration 11533 : loss : 0.151266, loss_ce: 0.040528\n","iteration 11534 : loss : 0.112336, loss_ce: 0.043889\n","iteration 11535 : loss : 0.103594, loss_ce: 0.034044\n","iteration 11536 : loss : 0.111147, loss_ce: 0.040930\n","iteration 11537 : loss : 0.109843, loss_ce: 0.032824\n","iteration 11538 : loss : 0.108615, loss_ce: 0.035305\n","iteration 11539 : loss : 0.111002, loss_ce: 0.049680\n","iteration 11540 : loss : 0.108555, loss_ce: 0.027369\n","iteration 11541 : loss : 0.103462, loss_ce: 0.028328\n","iteration 11542 : loss : 0.081428, loss_ce: 0.022500\n","iteration 11543 : loss : 0.114137, loss_ce: 0.054226\n","iteration 11544 : loss : 0.126008, loss_ce: 0.040917\n","iteration 11545 : loss : 0.117592, loss_ce: 0.031694\n","iteration 11546 : loss : 0.171704, loss_ce: 0.022353\n","iteration 11547 : loss : 0.118819, loss_ce: 0.029923\n","iteration 11548 : loss : 0.117625, loss_ce: 0.035153\n","iteration 11549 : loss : 0.132261, loss_ce: 0.022508\n","iteration 11550 : loss : 0.117107, loss_ce: 0.024776\n","iteration 11551 : loss : 0.104151, loss_ce: 0.048210\n","iteration 11552 : loss : 0.115657, loss_ce: 0.034709\n","iteration 11553 : loss : 0.091784, loss_ce: 0.024100\n","iteration 11554 : loss : 0.095922, loss_ce: 0.022224\n","iteration 11555 : loss : 0.105904, loss_ce: 0.030726\n","iteration 11556 : loss : 0.097425, loss_ce: 0.035509\n","iteration 11557 : loss : 0.091303, loss_ce: 0.027738\n","iteration 11558 : loss : 0.137052, loss_ce: 0.036589\n","iteration 11559 : loss : 0.117850, loss_ce: 0.037047\n","iteration 11560 : loss : 0.113419, loss_ce: 0.031971\n","iteration 11561 : loss : 0.096369, loss_ce: 0.030214\n","iteration 11562 : loss : 0.112615, loss_ce: 0.033814\n","iteration 11563 : loss : 0.111435, loss_ce: 0.034254\n","iteration 11564 : loss : 0.092526, loss_ce: 0.033602\n","iteration 11565 : loss : 0.130671, loss_ce: 0.034150\n","iteration 11566 : loss : 0.097380, loss_ce: 0.035106\n","iteration 11567 : loss : 0.106486, loss_ce: 0.017920\n","iteration 11568 : loss : 0.107932, loss_ce: 0.028973\n","iteration 11569 : loss : 0.094721, loss_ce: 0.027732\n","iteration 11570 : loss : 0.159199, loss_ce: 0.024030\n","iteration 11571 : loss : 0.092119, loss_ce: 0.024070\n","iteration 11572 : loss : 0.138112, loss_ce: 0.036475\n","iteration 11573 : loss : 0.098748, loss_ce: 0.030544\n","iteration 11574 : loss : 0.132201, loss_ce: 0.019693\n","iteration 11575 : loss : 0.095889, loss_ce: 0.048957\n","iteration 11576 : loss : 0.062930, loss_ce: 0.019149\n","iteration 11577 : loss : 0.077420, loss_ce: 0.024749\n","iteration 11578 : loss : 0.138764, loss_ce: 0.017102\n","iteration 11579 : loss : 0.090685, loss_ce: 0.030490\n","iteration 11580 : loss : 0.108894, loss_ce: 0.025359\n","iteration 11581 : loss : 0.162086, loss_ce: 0.034157\n","iteration 11582 : loss : 0.093199, loss_ce: 0.025031\n","iteration 11583 : loss : 0.108982, loss_ce: 0.035182\n","iteration 11584 : loss : 0.095592, loss_ce: 0.034804\n","iteration 11585 : loss : 0.135422, loss_ce: 0.021982\n","iteration 11586 : loss : 0.123275, loss_ce: 0.023579\n","iteration 11587 : loss : 0.095681, loss_ce: 0.041736\n","iteration 11588 : loss : 0.103653, loss_ce: 0.028348\n","iteration 11589 : loss : 0.093869, loss_ce: 0.039570\n","iteration 11590 : loss : 0.088926, loss_ce: 0.029668\n","iteration 11591 : loss : 0.136701, loss_ce: 0.032634\n","iteration 11592 : loss : 0.111235, loss_ce: 0.039412\n","iteration 11593 : loss : 0.102952, loss_ce: 0.032202\n","iteration 11594 : loss : 0.104050, loss_ce: 0.020979\n","iteration 11595 : loss : 0.097832, loss_ce: 0.032860\n","iteration 11596 : loss : 0.109282, loss_ce: 0.048171\n","iteration 11597 : loss : 0.077017, loss_ce: 0.014493\n","iteration 11598 : loss : 0.094740, loss_ce: 0.030923\n","iteration 11599 : loss : 0.111972, loss_ce: 0.037542\n","iteration 11600 : loss : 0.093187, loss_ce: 0.016093\n","iteration 11601 : loss : 0.124504, loss_ce: 0.031055\n","iteration 11602 : loss : 0.111121, loss_ce: 0.025934\n","iteration 11603 : loss : 0.102914, loss_ce: 0.037901\n","iteration 11604 : loss : 0.166123, loss_ce: 0.027903\n","iteration 11605 : loss : 0.090065, loss_ce: 0.025679\n","iteration 11606 : loss : 0.103657, loss_ce: 0.033468\n","iteration 11607 : loss : 0.123477, loss_ce: 0.036243\n","iteration 11608 : loss : 0.100106, loss_ce: 0.031111\n","iteration 11609 : loss : 0.103173, loss_ce: 0.035243\n","iteration 11610 : loss : 0.112635, loss_ce: 0.030729\n","iteration 11611 : loss : 0.082777, loss_ce: 0.020880\n","iteration 11612 : loss : 0.094232, loss_ce: 0.037018\n","iteration 11613 : loss : 0.096628, loss_ce: 0.022090\n","iteration 11614 : loss : 0.148162, loss_ce: 0.035880\n","iteration 11615 : loss : 0.103453, loss_ce: 0.034549\n","iteration 11616 : loss : 0.103963, loss_ce: 0.036539\n","iteration 11617 : loss : 0.104412, loss_ce: 0.023353\n","iteration 11618 : loss : 0.090955, loss_ce: 0.032916\n","iteration 11619 : loss : 0.109241, loss_ce: 0.024186\n","iteration 11620 : loss : 0.111716, loss_ce: 0.018428\n","iteration 11621 : loss : 0.098300, loss_ce: 0.015591\n","iteration 11622 : loss : 0.095941, loss_ce: 0.025548\n","iteration 11623 : loss : 0.117208, loss_ce: 0.021103\n","iteration 11624 : loss : 0.098896, loss_ce: 0.028522\n","iteration 11625 : loss : 0.108477, loss_ce: 0.090122\n"," 83%|████████████████████████▏    | 125/150 [2:43:35<33:11, 79.66s/it]iteration 11626 : loss : 0.102729, loss_ce: 0.028527\n","iteration 11627 : loss : 0.107967, loss_ce: 0.026554\n","iteration 11628 : loss : 0.112331, loss_ce: 0.034918\n","iteration 11629 : loss : 0.092603, loss_ce: 0.021169\n","iteration 11630 : loss : 0.095539, loss_ce: 0.038453\n","iteration 11631 : loss : 0.103866, loss_ce: 0.041003\n","iteration 11632 : loss : 0.143360, loss_ce: 0.026673\n","iteration 11633 : loss : 0.099919, loss_ce: 0.032040\n","iteration 11634 : loss : 0.110060, loss_ce: 0.029236\n","iteration 11635 : loss : 0.088297, loss_ce: 0.027561\n","iteration 11636 : loss : 0.092438, loss_ce: 0.018821\n","iteration 11637 : loss : 0.085650, loss_ce: 0.035181\n","iteration 11638 : loss : 0.135355, loss_ce: 0.037188\n","iteration 11639 : loss : 0.107670, loss_ce: 0.025099\n","iteration 11640 : loss : 0.102377, loss_ce: 0.039578\n","iteration 11641 : loss : 0.136353, loss_ce: 0.024907\n","iteration 11642 : loss : 0.100574, loss_ce: 0.020643\n","iteration 11643 : loss : 0.101582, loss_ce: 0.040208\n","iteration 11644 : loss : 0.115415, loss_ce: 0.024621\n","iteration 11645 : loss : 0.102150, loss_ce: 0.031015\n","iteration 11646 : loss : 0.096209, loss_ce: 0.022541\n","iteration 11647 : loss : 0.131534, loss_ce: 0.025638\n","iteration 11648 : loss : 0.094858, loss_ce: 0.042750\n","iteration 11649 : loss : 0.145740, loss_ce: 0.047467\n","iteration 11650 : loss : 0.085622, loss_ce: 0.027301\n","iteration 11651 : loss : 0.083061, loss_ce: 0.030650\n","iteration 11652 : loss : 0.104518, loss_ce: 0.034285\n","iteration 11653 : loss : 0.142977, loss_ce: 0.014672\n","iteration 11654 : loss : 0.090965, loss_ce: 0.022670\n","iteration 11655 : loss : 0.100066, loss_ce: 0.029791\n","iteration 11656 : loss : 0.118050, loss_ce: 0.027047\n","iteration 11657 : loss : 0.093824, loss_ce: 0.045024\n","iteration 11658 : loss : 0.103098, loss_ce: 0.029383\n","iteration 11659 : loss : 0.081628, loss_ce: 0.026006\n","iteration 11660 : loss : 0.195800, loss_ce: 0.024061\n","iteration 11661 : loss : 0.095031, loss_ce: 0.036352\n","iteration 11662 : loss : 0.100095, loss_ce: 0.040877\n","iteration 11663 : loss : 0.087577, loss_ce: 0.035607\n","iteration 11664 : loss : 0.119796, loss_ce: 0.032862\n","iteration 11665 : loss : 0.101680, loss_ce: 0.045870\n","iteration 11666 : loss : 0.143053, loss_ce: 0.026551\n","iteration 11667 : loss : 0.077286, loss_ce: 0.022742\n","iteration 11668 : loss : 0.091017, loss_ce: 0.046253\n","iteration 11669 : loss : 0.120482, loss_ce: 0.026733\n","iteration 11670 : loss : 0.130741, loss_ce: 0.036370\n","iteration 11671 : loss : 0.248617, loss_ce: 0.027181\n","iteration 11672 : loss : 0.100670, loss_ce: 0.025694\n","iteration 11673 : loss : 0.111426, loss_ce: 0.034569\n","iteration 11674 : loss : 0.149313, loss_ce: 0.022044\n","iteration 11675 : loss : 0.102091, loss_ce: 0.016836\n","iteration 11676 : loss : 0.088658, loss_ce: 0.032235\n","iteration 11677 : loss : 0.098201, loss_ce: 0.032707\n","iteration 11678 : loss : 0.131762, loss_ce: 0.040510\n","iteration 11679 : loss : 0.151895, loss_ce: 0.026265\n","iteration 11680 : loss : 0.099637, loss_ce: 0.033043\n","iteration 11681 : loss : 0.111882, loss_ce: 0.032825\n","iteration 11682 : loss : 0.084573, loss_ce: 0.040106\n","iteration 11683 : loss : 0.072362, loss_ce: 0.022815\n","iteration 11684 : loss : 0.106077, loss_ce: 0.028619\n","iteration 11685 : loss : 0.157944, loss_ce: 0.019900\n","iteration 11686 : loss : 0.152916, loss_ce: 0.026514\n","iteration 11687 : loss : 0.117246, loss_ce: 0.053573\n","iteration 11688 : loss : 0.096209, loss_ce: 0.030968\n","iteration 11689 : loss : 0.106652, loss_ce: 0.018048\n","iteration 11690 : loss : 0.114159, loss_ce: 0.027218\n","iteration 11691 : loss : 0.112456, loss_ce: 0.022813\n","iteration 11692 : loss : 0.095812, loss_ce: 0.025744\n","iteration 11693 : loss : 0.111469, loss_ce: 0.032477\n","iteration 11694 : loss : 0.127025, loss_ce: 0.027084\n","iteration 11695 : loss : 0.134474, loss_ce: 0.033789\n","iteration 11696 : loss : 0.169769, loss_ce: 0.024383\n","iteration 11697 : loss : 0.088641, loss_ce: 0.023809\n","iteration 11698 : loss : 0.124287, loss_ce: 0.035336\n","iteration 11699 : loss : 0.100330, loss_ce: 0.033203\n","iteration 11700 : loss : 0.090985, loss_ce: 0.030687\n","iteration 11701 : loss : 0.127207, loss_ce: 0.030819\n","iteration 11702 : loss : 0.093595, loss_ce: 0.028679\n","iteration 11703 : loss : 0.087391, loss_ce: 0.028590\n","iteration 11704 : loss : 0.122886, loss_ce: 0.030381\n","iteration 11705 : loss : 0.140392, loss_ce: 0.024522\n","iteration 11706 : loss : 0.087193, loss_ce: 0.019995\n","iteration 11707 : loss : 0.107335, loss_ce: 0.034284\n","iteration 11708 : loss : 0.092881, loss_ce: 0.038519\n","iteration 11709 : loss : 0.093788, loss_ce: 0.046371\n","iteration 11710 : loss : 0.079891, loss_ce: 0.031100\n","iteration 11711 : loss : 0.112300, loss_ce: 0.032766\n","iteration 11712 : loss : 0.122388, loss_ce: 0.046071\n","iteration 11713 : loss : 0.084023, loss_ce: 0.025012\n","iteration 11714 : loss : 0.153216, loss_ce: 0.016749\n","iteration 11715 : loss : 0.102661, loss_ce: 0.022585\n","iteration 11716 : loss : 0.096622, loss_ce: 0.025216\n","iteration 11717 : loss : 0.104271, loss_ce: 0.033533\n","iteration 11718 : loss : 0.206883, loss_ce: 0.021488\n"," 84%|████████████████████████▎    | 126/150 [2:44:51<31:23, 78.48s/it]iteration 11719 : loss : 0.143368, loss_ce: 0.023680\n","iteration 11720 : loss : 0.160321, loss_ce: 0.025550\n","iteration 11721 : loss : 0.082903, loss_ce: 0.024096\n","iteration 11722 : loss : 0.093549, loss_ce: 0.021744\n","iteration 11723 : loss : 0.079968, loss_ce: 0.029184\n","iteration 11724 : loss : 0.159012, loss_ce: 0.026678\n","iteration 11725 : loss : 0.140089, loss_ce: 0.014720\n","iteration 11726 : loss : 0.129109, loss_ce: 0.016690\n","iteration 11727 : loss : 0.083996, loss_ce: 0.022860\n","iteration 11728 : loss : 0.143994, loss_ce: 0.023535\n","iteration 11729 : loss : 0.171230, loss_ce: 0.029566\n","iteration 11730 : loss : 0.071563, loss_ce: 0.027792\n","iteration 11731 : loss : 0.106053, loss_ce: 0.034077\n","iteration 11732 : loss : 0.183794, loss_ce: 0.025950\n","iteration 11733 : loss : 0.122703, loss_ce: 0.028662\n","iteration 11734 : loss : 0.113289, loss_ce: 0.044485\n","iteration 11735 : loss : 0.089897, loss_ce: 0.038994\n","iteration 11736 : loss : 0.087302, loss_ce: 0.018679\n","iteration 11737 : loss : 0.099901, loss_ce: 0.024996\n","iteration 11738 : loss : 0.099040, loss_ce: 0.017616\n","iteration 11739 : loss : 0.089184, loss_ce: 0.034152\n","iteration 11740 : loss : 0.082031, loss_ce: 0.030519\n","iteration 11741 : loss : 0.102692, loss_ce: 0.035719\n","iteration 11742 : loss : 0.147853, loss_ce: 0.026368\n","iteration 11743 : loss : 0.094627, loss_ce: 0.030000\n","iteration 11744 : loss : 0.088368, loss_ce: 0.028601\n","iteration 11745 : loss : 0.112274, loss_ce: 0.036075\n","iteration 11746 : loss : 0.110345, loss_ce: 0.035776\n","iteration 11747 : loss : 0.129067, loss_ce: 0.032431\n","iteration 11748 : loss : 0.078121, loss_ce: 0.027668\n","iteration 11749 : loss : 0.071081, loss_ce: 0.024598\n","iteration 11750 : loss : 0.107377, loss_ce: 0.039386\n","iteration 11751 : loss : 0.147018, loss_ce: 0.035831\n","iteration 11752 : loss : 0.144766, loss_ce: 0.016369\n","iteration 11753 : loss : 0.093402, loss_ce: 0.035146\n","iteration 11754 : loss : 0.107647, loss_ce: 0.032617\n","iteration 11755 : loss : 0.104569, loss_ce: 0.031565\n","iteration 11756 : loss : 0.087661, loss_ce: 0.029378\n","iteration 11757 : loss : 0.102041, loss_ce: 0.034402\n","iteration 11758 : loss : 0.091108, loss_ce: 0.031132\n","iteration 11759 : loss : 0.144569, loss_ce: 0.014967\n","iteration 11760 : loss : 0.107050, loss_ce: 0.033858\n","iteration 11761 : loss : 0.136672, loss_ce: 0.023519\n","iteration 11762 : loss : 0.108605, loss_ce: 0.019763\n","iteration 11763 : loss : 0.146632, loss_ce: 0.023136\n","iteration 11764 : loss : 0.101417, loss_ce: 0.041365\n","iteration 11765 : loss : 0.116523, loss_ce: 0.046759\n","iteration 11766 : loss : 0.099077, loss_ce: 0.019594\n","iteration 11767 : loss : 0.241871, loss_ce: 0.027528\n","iteration 11768 : loss : 0.104357, loss_ce: 0.028526\n","iteration 11769 : loss : 0.111927, loss_ce: 0.033846\n","iteration 11770 : loss : 0.099612, loss_ce: 0.030507\n","iteration 11771 : loss : 0.110921, loss_ce: 0.037220\n","iteration 11772 : loss : 0.095015, loss_ce: 0.029944\n","iteration 11773 : loss : 0.100624, loss_ce: 0.036831\n","iteration 11774 : loss : 0.100505, loss_ce: 0.026739\n","iteration 11775 : loss : 0.083972, loss_ce: 0.030276\n","iteration 11776 : loss : 0.101889, loss_ce: 0.027461\n","iteration 11777 : loss : 0.090620, loss_ce: 0.033046\n","iteration 11778 : loss : 0.087332, loss_ce: 0.043961\n","iteration 11779 : loss : 0.149032, loss_ce: 0.030855\n","iteration 11780 : loss : 0.109307, loss_ce: 0.055563\n","iteration 11781 : loss : 0.122432, loss_ce: 0.032075\n","iteration 11782 : loss : 0.094519, loss_ce: 0.033329\n","iteration 11783 : loss : 0.102962, loss_ce: 0.029379\n","iteration 11784 : loss : 0.092716, loss_ce: 0.023049\n","iteration 11785 : loss : 0.105407, loss_ce: 0.023020\n","iteration 11786 : loss : 0.110526, loss_ce: 0.037481\n","iteration 11787 : loss : 0.136971, loss_ce: 0.016978\n","iteration 11788 : loss : 0.097730, loss_ce: 0.032420\n","iteration 11789 : loss : 0.088293, loss_ce: 0.033111\n","iteration 11790 : loss : 0.095375, loss_ce: 0.032277\n","iteration 11791 : loss : 0.140652, loss_ce: 0.030373\n","iteration 11792 : loss : 0.086565, loss_ce: 0.025898\n","iteration 11793 : loss : 0.125599, loss_ce: 0.027154\n","iteration 11794 : loss : 0.211101, loss_ce: 0.013041\n","iteration 11795 : loss : 0.093841, loss_ce: 0.033541\n","iteration 11796 : loss : 0.082247, loss_ce: 0.024605\n","iteration 11797 : loss : 0.145345, loss_ce: 0.016854\n","iteration 11798 : loss : 0.110181, loss_ce: 0.045837\n","iteration 11799 : loss : 0.106087, loss_ce: 0.029716\n","iteration 11800 : loss : 0.125848, loss_ce: 0.039357\n","iteration 11801 : loss : 0.084500, loss_ce: 0.027688\n","iteration 11802 : loss : 0.101702, loss_ce: 0.038024\n","iteration 11803 : loss : 0.094195, loss_ce: 0.041545\n","iteration 11804 : loss : 0.089823, loss_ce: 0.030874\n","iteration 11805 : loss : 0.119254, loss_ce: 0.033219\n","iteration 11806 : loss : 0.090470, loss_ce: 0.021657\n","iteration 11807 : loss : 0.104328, loss_ce: 0.037757\n","iteration 11808 : loss : 0.161411, loss_ce: 0.039373\n","iteration 11809 : loss : 0.106531, loss_ce: 0.032903\n","iteration 11810 : loss : 0.088251, loss_ce: 0.021483\n","iteration 11811 : loss : 0.130769, loss_ce: 0.048232\n"," 85%|████████████████████████▌    | 127/150 [2:46:10<30:05, 78.49s/it]iteration 11812 : loss : 0.124457, loss_ce: 0.030552\n","iteration 11813 : loss : 0.111503, loss_ce: 0.033480\n","iteration 11814 : loss : 0.104709, loss_ce: 0.027871\n","iteration 11815 : loss : 0.175368, loss_ce: 0.028218\n","iteration 11816 : loss : 0.114503, loss_ce: 0.040496\n","iteration 11817 : loss : 0.094150, loss_ce: 0.031167\n","iteration 11818 : loss : 0.089351, loss_ce: 0.024801\n","iteration 11819 : loss : 0.091242, loss_ce: 0.027829\n","iteration 11820 : loss : 0.095046, loss_ce: 0.030417\n","iteration 11821 : loss : 0.117305, loss_ce: 0.028302\n","iteration 11822 : loss : 0.072960, loss_ce: 0.016093\n","iteration 11823 : loss : 0.086770, loss_ce: 0.039679\n","iteration 11824 : loss : 0.084350, loss_ce: 0.028735\n","iteration 11825 : loss : 0.160590, loss_ce: 0.016969\n","iteration 11826 : loss : 0.085483, loss_ce: 0.032064\n","iteration 11827 : loss : 0.080793, loss_ce: 0.034281\n","iteration 11828 : loss : 0.117984, loss_ce: 0.035631\n","iteration 11829 : loss : 0.119913, loss_ce: 0.046433\n","iteration 11830 : loss : 0.093159, loss_ce: 0.031812\n","iteration 11831 : loss : 0.086041, loss_ce: 0.037088\n","iteration 11832 : loss : 0.145129, loss_ce: 0.042145\n","iteration 11833 : loss : 0.106254, loss_ce: 0.028733\n","iteration 11834 : loss : 0.138253, loss_ce: 0.013741\n","iteration 11835 : loss : 0.092484, loss_ce: 0.039206\n","iteration 11836 : loss : 0.142042, loss_ce: 0.043281\n","iteration 11837 : loss : 0.108866, loss_ce: 0.021769\n","iteration 11838 : loss : 0.151991, loss_ce: 0.025907\n","iteration 11839 : loss : 0.164573, loss_ce: 0.039795\n","iteration 11840 : loss : 0.111272, loss_ce: 0.030710\n","iteration 11841 : loss : 0.123037, loss_ce: 0.033159\n","iteration 11842 : loss : 0.075582, loss_ce: 0.018261\n","iteration 11843 : loss : 0.085003, loss_ce: 0.030752\n","iteration 11844 : loss : 0.151512, loss_ce: 0.024410\n","iteration 11845 : loss : 0.086203, loss_ce: 0.020353\n","iteration 11846 : loss : 0.130610, loss_ce: 0.030934\n","iteration 11847 : loss : 0.093841, loss_ce: 0.025889\n","iteration 11848 : loss : 0.124750, loss_ce: 0.041426\n","iteration 11849 : loss : 0.096991, loss_ce: 0.029273\n","iteration 11850 : loss : 0.090170, loss_ce: 0.031396\n","iteration 11851 : loss : 0.077614, loss_ce: 0.019551\n","iteration 11852 : loss : 0.087734, loss_ce: 0.024452\n","iteration 11853 : loss : 0.092447, loss_ce: 0.036090\n","iteration 11854 : loss : 0.097975, loss_ce: 0.024847\n","iteration 11855 : loss : 0.167722, loss_ce: 0.018517\n","iteration 11856 : loss : 0.107909, loss_ce: 0.033006\n","iteration 11857 : loss : 0.115964, loss_ce: 0.027534\n","iteration 11858 : loss : 0.099626, loss_ce: 0.021624\n","iteration 11859 : loss : 0.087901, loss_ce: 0.021218\n","iteration 11860 : loss : 0.110477, loss_ce: 0.047613\n","iteration 11861 : loss : 0.116625, loss_ce: 0.040316\n","iteration 11862 : loss : 0.099493, loss_ce: 0.036334\n","iteration 11863 : loss : 0.085532, loss_ce: 0.027582\n","iteration 11864 : loss : 0.119949, loss_ce: 0.036110\n","iteration 11865 : loss : 0.085367, loss_ce: 0.025058\n","iteration 11866 : loss : 0.108571, loss_ce: 0.021763\n","iteration 11867 : loss : 0.096923, loss_ce: 0.026896\n","iteration 11868 : loss : 0.088367, loss_ce: 0.029409\n","iteration 11869 : loss : 0.195549, loss_ce: 0.026894\n","iteration 11870 : loss : 0.087943, loss_ce: 0.035050\n","iteration 11871 : loss : 0.146548, loss_ce: 0.034959\n","iteration 11872 : loss : 0.154145, loss_ce: 0.019383\n","iteration 11873 : loss : 0.147694, loss_ce: 0.021196\n","iteration 11874 : loss : 0.096419, loss_ce: 0.042297\n","iteration 11875 : loss : 0.103597, loss_ce: 0.031749\n","iteration 11876 : loss : 0.105915, loss_ce: 0.040132\n","iteration 11877 : loss : 0.164695, loss_ce: 0.031641\n","iteration 11878 : loss : 0.126523, loss_ce: 0.044128\n","iteration 11879 : loss : 0.075125, loss_ce: 0.023889\n","iteration 11880 : loss : 0.079344, loss_ce: 0.032989\n","iteration 11881 : loss : 0.099947, loss_ce: 0.044474\n","iteration 11882 : loss : 0.082804, loss_ce: 0.026852\n","iteration 11883 : loss : 0.081182, loss_ce: 0.019006\n","iteration 11884 : loss : 0.085896, loss_ce: 0.037938\n","iteration 11885 : loss : 0.074906, loss_ce: 0.029925\n","iteration 11886 : loss : 0.128848, loss_ce: 0.030076\n","iteration 11887 : loss : 0.095801, loss_ce: 0.032817\n","iteration 11888 : loss : 0.079281, loss_ce: 0.025806\n","iteration 11889 : loss : 0.084286, loss_ce: 0.019309\n","iteration 11890 : loss : 0.106623, loss_ce: 0.021818\n","iteration 11891 : loss : 0.095853, loss_ce: 0.039218\n","iteration 11892 : loss : 0.082871, loss_ce: 0.025774\n","iteration 11893 : loss : 0.102149, loss_ce: 0.033602\n","iteration 11894 : loss : 0.135716, loss_ce: 0.015272\n","iteration 11895 : loss : 0.092716, loss_ce: 0.025813\n","iteration 11896 : loss : 0.090896, loss_ce: 0.032857\n","iteration 11897 : loss : 0.114695, loss_ce: 0.028621\n","iteration 11898 : loss : 0.075652, loss_ce: 0.018090\n","iteration 11899 : loss : 0.085828, loss_ce: 0.023144\n","iteration 11900 : loss : 0.096549, loss_ce: 0.025539\n","iteration 11901 : loss : 0.089035, loss_ce: 0.026663\n","iteration 11902 : loss : 0.093558, loss_ce: 0.026774\n","iteration 11903 : loss : 0.291870, loss_ce: 0.011754\n","iteration 11904 : loss : 0.257800, loss_ce: 0.046929\n"," 85%|████████████████████████▋    | 128/150 [2:47:30<28:58, 79.02s/it]iteration 11905 : loss : 0.106523, loss_ce: 0.030761\n","iteration 11906 : loss : 0.106637, loss_ce: 0.023647\n","iteration 11907 : loss : 0.106544, loss_ce: 0.039385\n","iteration 11908 : loss : 0.110618, loss_ce: 0.022510\n","iteration 11909 : loss : 0.091598, loss_ce: 0.035849\n","iteration 11910 : loss : 0.106282, loss_ce: 0.026130\n","iteration 11911 : loss : 0.084405, loss_ce: 0.025767\n","iteration 11912 : loss : 0.075813, loss_ce: 0.021015\n","iteration 11913 : loss : 0.137973, loss_ce: 0.026689\n","iteration 11914 : loss : 0.078890, loss_ce: 0.024775\n","iteration 11915 : loss : 0.086574, loss_ce: 0.036862\n","iteration 11916 : loss : 0.084952, loss_ce: 0.033589\n","iteration 11917 : loss : 0.071157, loss_ce: 0.028330\n","iteration 11918 : loss : 0.165821, loss_ce: 0.023576\n","iteration 11919 : loss : 0.097693, loss_ce: 0.037324\n","iteration 11920 : loss : 0.097328, loss_ce: 0.030026\n","iteration 11921 : loss : 0.119354, loss_ce: 0.043031\n","iteration 11922 : loss : 0.090319, loss_ce: 0.035437\n","iteration 11923 : loss : 0.074432, loss_ce: 0.022615\n","iteration 11924 : loss : 0.091383, loss_ce: 0.020959\n","iteration 11925 : loss : 0.107528, loss_ce: 0.027111\n","iteration 11926 : loss : 0.142294, loss_ce: 0.031912\n","iteration 11927 : loss : 0.124521, loss_ce: 0.026377\n","iteration 11928 : loss : 0.082416, loss_ce: 0.032453\n","iteration 11929 : loss : 0.108238, loss_ce: 0.024660\n","iteration 11930 : loss : 0.096528, loss_ce: 0.033596\n","iteration 11931 : loss : 0.123923, loss_ce: 0.032196\n","iteration 11932 : loss : 0.111307, loss_ce: 0.036057\n","iteration 11933 : loss : 0.116490, loss_ce: 0.036366\n","iteration 11934 : loss : 0.125317, loss_ce: 0.019576\n","iteration 11935 : loss : 0.107081, loss_ce: 0.018011\n","iteration 11936 : loss : 0.095365, loss_ce: 0.035077\n","iteration 11937 : loss : 0.088819, loss_ce: 0.034326\n","iteration 11938 : loss : 0.150023, loss_ce: 0.035290\n","iteration 11939 : loss : 0.091811, loss_ce: 0.020639\n","iteration 11940 : loss : 0.180134, loss_ce: 0.016268\n","iteration 11941 : loss : 0.097604, loss_ce: 0.031500\n","iteration 11942 : loss : 0.082190, loss_ce: 0.027446\n","iteration 11943 : loss : 0.121753, loss_ce: 0.035354\n","iteration 11944 : loss : 0.069403, loss_ce: 0.026265\n","iteration 11945 : loss : 0.076344, loss_ce: 0.018675\n","iteration 11946 : loss : 0.113186, loss_ce: 0.026292\n","iteration 11947 : loss : 0.100884, loss_ce: 0.021140\n","iteration 11948 : loss : 0.088455, loss_ce: 0.028640\n","iteration 11949 : loss : 0.091068, loss_ce: 0.029633\n","iteration 11950 : loss : 0.094602, loss_ce: 0.033555\n","iteration 11951 : loss : 0.098638, loss_ce: 0.030106\n","iteration 11952 : loss : 0.081210, loss_ce: 0.017248\n","iteration 11953 : loss : 0.094548, loss_ce: 0.020700\n","iteration 11954 : loss : 0.115230, loss_ce: 0.036670\n","iteration 11955 : loss : 0.078997, loss_ce: 0.033654\n","iteration 11956 : loss : 0.087028, loss_ce: 0.034172\n","iteration 11957 : loss : 0.138792, loss_ce: 0.025485\n","iteration 11958 : loss : 0.181633, loss_ce: 0.028765\n","iteration 11959 : loss : 0.113349, loss_ce: 0.042571\n","iteration 11960 : loss : 0.121393, loss_ce: 0.035250\n","iteration 11961 : loss : 0.074932, loss_ce: 0.013482\n","iteration 11962 : loss : 0.093136, loss_ce: 0.042091\n","iteration 11963 : loss : 0.090203, loss_ce: 0.025249\n","iteration 11964 : loss : 0.105959, loss_ce: 0.025689\n","iteration 11965 : loss : 0.130717, loss_ce: 0.019617\n","iteration 11966 : loss : 0.090034, loss_ce: 0.039846\n","iteration 11967 : loss : 0.086621, loss_ce: 0.022353\n","iteration 11968 : loss : 0.104298, loss_ce: 0.020144\n","iteration 11969 : loss : 0.093653, loss_ce: 0.039236\n","iteration 11970 : loss : 0.101095, loss_ce: 0.029915\n","iteration 11971 : loss : 0.121321, loss_ce: 0.027534\n","iteration 11972 : loss : 0.121524, loss_ce: 0.021024\n","iteration 11973 : loss : 0.093054, loss_ce: 0.027522\n","iteration 11974 : loss : 0.149322, loss_ce: 0.032085\n","iteration 11975 : loss : 0.104794, loss_ce: 0.022402\n","iteration 11976 : loss : 0.101539, loss_ce: 0.018966\n","iteration 11977 : loss : 0.110770, loss_ce: 0.029826\n","iteration 11978 : loss : 0.100309, loss_ce: 0.038057\n","iteration 11979 : loss : 0.099101, loss_ce: 0.034580\n","iteration 11980 : loss : 0.081283, loss_ce: 0.024522\n","iteration 11981 : loss : 0.090670, loss_ce: 0.019790\n","iteration 11982 : loss : 0.093380, loss_ce: 0.037574\n","iteration 11983 : loss : 0.118216, loss_ce: 0.041327\n","iteration 11984 : loss : 0.123497, loss_ce: 0.024721\n","iteration 11985 : loss : 0.104750, loss_ce: 0.037090\n","iteration 11986 : loss : 0.092521, loss_ce: 0.034225\n","iteration 11987 : loss : 0.094365, loss_ce: 0.034742\n","iteration 11988 : loss : 0.091251, loss_ce: 0.026067\n","iteration 11989 : loss : 0.106117, loss_ce: 0.030501\n","iteration 11990 : loss : 0.084206, loss_ce: 0.019809\n","iteration 11991 : loss : 0.134127, loss_ce: 0.025285\n","iteration 11992 : loss : 0.104202, loss_ce: 0.034277\n","iteration 11993 : loss : 0.094462, loss_ce: 0.022843\n","iteration 11994 : loss : 0.120260, loss_ce: 0.017477\n","iteration 11995 : loss : 0.116487, loss_ce: 0.021133\n","iteration 11996 : loss : 0.103976, loss_ce: 0.042892\n","iteration 11997 : loss : 0.133770, loss_ce: 0.055024\n"," 86%|████████████████████████▉    | 129/150 [2:48:48<27:30, 78.62s/it]iteration 11998 : loss : 0.097599, loss_ce: 0.018925\n","iteration 11999 : loss : 0.162932, loss_ce: 0.032840\n","iteration 12000 : loss : 0.093439, loss_ce: 0.034912\n","iteration 12001 : loss : 0.088514, loss_ce: 0.042216\n","iteration 12002 : loss : 0.115855, loss_ce: 0.034471\n","iteration 12003 : loss : 0.093113, loss_ce: 0.026990\n","iteration 12004 : loss : 0.178539, loss_ce: 0.038282\n","iteration 12005 : loss : 0.102806, loss_ce: 0.030249\n","iteration 12006 : loss : 0.122345, loss_ce: 0.025293\n","iteration 12007 : loss : 0.106812, loss_ce: 0.019620\n","iteration 12008 : loss : 0.122413, loss_ce: 0.040103\n","iteration 12009 : loss : 0.114023, loss_ce: 0.022425\n","iteration 12010 : loss : 0.087198, loss_ce: 0.028650\n","iteration 12011 : loss : 0.094587, loss_ce: 0.030023\n","iteration 12012 : loss : 0.106631, loss_ce: 0.036884\n","iteration 12013 : loss : 0.143027, loss_ce: 0.025463\n","iteration 12014 : loss : 0.146008, loss_ce: 0.016979\n","iteration 12015 : loss : 0.079313, loss_ce: 0.021246\n","iteration 12016 : loss : 0.113233, loss_ce: 0.046582\n","iteration 12017 : loss : 0.082724, loss_ce: 0.022959\n","iteration 12018 : loss : 0.077681, loss_ce: 0.034044\n","iteration 12019 : loss : 0.128949, loss_ce: 0.015936\n","iteration 12020 : loss : 0.085547, loss_ce: 0.039024\n","iteration 12021 : loss : 0.104619, loss_ce: 0.019605\n","iteration 12022 : loss : 0.099816, loss_ce: 0.031124\n","iteration 12023 : loss : 0.100717, loss_ce: 0.038444\n","iteration 12024 : loss : 0.099351, loss_ce: 0.032410\n","iteration 12025 : loss : 0.119378, loss_ce: 0.031206\n","iteration 12026 : loss : 0.099908, loss_ce: 0.030535\n","iteration 12027 : loss : 0.093088, loss_ce: 0.023282\n","iteration 12028 : loss : 0.088758, loss_ce: 0.031707\n","iteration 12029 : loss : 0.156306, loss_ce: 0.022645\n","iteration 12030 : loss : 0.101465, loss_ce: 0.035743\n","iteration 12031 : loss : 0.091987, loss_ce: 0.021282\n","iteration 12032 : loss : 0.082463, loss_ce: 0.020069\n","iteration 12033 : loss : 0.084893, loss_ce: 0.026891\n","iteration 12034 : loss : 0.097088, loss_ce: 0.030161\n","iteration 12035 : loss : 0.103370, loss_ce: 0.033998\n","iteration 12036 : loss : 0.091535, loss_ce: 0.031612\n","iteration 12037 : loss : 0.086446, loss_ce: 0.039129\n","iteration 12038 : loss : 0.101993, loss_ce: 0.030627\n","iteration 12039 : loss : 0.087287, loss_ce: 0.026294\n","iteration 12040 : loss : 0.092016, loss_ce: 0.020456\n","iteration 12041 : loss : 0.096640, loss_ce: 0.024111\n","iteration 12042 : loss : 0.087809, loss_ce: 0.036930\n","iteration 12043 : loss : 0.107266, loss_ce: 0.046951\n","iteration 12044 : loss : 0.139313, loss_ce: 0.023087\n","iteration 12045 : loss : 0.107294, loss_ce: 0.026813\n","iteration 12046 : loss : 0.157168, loss_ce: 0.020471\n","iteration 12047 : loss : 0.136891, loss_ce: 0.028740\n","iteration 12048 : loss : 0.085434, loss_ce: 0.020030\n","iteration 12049 : loss : 0.114600, loss_ce: 0.042995\n","iteration 12050 : loss : 0.139260, loss_ce: 0.021506\n","iteration 12051 : loss : 0.103478, loss_ce: 0.016772\n","iteration 12052 : loss : 0.081181, loss_ce: 0.027633\n","iteration 12053 : loss : 0.179706, loss_ce: 0.026772\n","iteration 12054 : loss : 0.082605, loss_ce: 0.022819\n","iteration 12055 : loss : 0.101450, loss_ce: 0.033685\n","iteration 12056 : loss : 0.120947, loss_ce: 0.036843\n","iteration 12057 : loss : 0.109439, loss_ce: 0.029356\n","iteration 12058 : loss : 0.091483, loss_ce: 0.024841\n","iteration 12059 : loss : 0.100446, loss_ce: 0.031462\n","iteration 12060 : loss : 0.122910, loss_ce: 0.040855\n","iteration 12061 : loss : 0.098580, loss_ce: 0.029429\n","iteration 12062 : loss : 0.103205, loss_ce: 0.041455\n","iteration 12063 : loss : 0.088117, loss_ce: 0.025409\n","iteration 12064 : loss : 0.092243, loss_ce: 0.022104\n","iteration 12065 : loss : 0.110792, loss_ce: 0.034306\n","iteration 12066 : loss : 0.100848, loss_ce: 0.034712\n","iteration 12067 : loss : 0.102493, loss_ce: 0.034768\n","iteration 12068 : loss : 0.083464, loss_ce: 0.025795\n","iteration 12069 : loss : 0.087894, loss_ce: 0.019972\n","iteration 12070 : loss : 0.095243, loss_ce: 0.037376\n","iteration 12071 : loss : 0.090612, loss_ce: 0.033071\n","iteration 12072 : loss : 0.085932, loss_ce: 0.023543\n","iteration 12073 : loss : 0.152183, loss_ce: 0.025857\n","iteration 12074 : loss : 0.103076, loss_ce: 0.031087\n","iteration 12075 : loss : 0.082838, loss_ce: 0.030271\n","iteration 12076 : loss : 0.076824, loss_ce: 0.029656\n","iteration 12077 : loss : 0.092695, loss_ce: 0.016677\n","iteration 12078 : loss : 0.079417, loss_ce: 0.028506\n","iteration 12079 : loss : 0.096965, loss_ce: 0.022154\n","iteration 12080 : loss : 0.150790, loss_ce: 0.018438\n","iteration 12081 : loss : 0.123279, loss_ce: 0.032201\n","iteration 12082 : loss : 0.078477, loss_ce: 0.028614\n","iteration 12083 : loss : 0.109164, loss_ce: 0.033740\n","iteration 12084 : loss : 0.107577, loss_ce: 0.018212\n","iteration 12085 : loss : 0.110627, loss_ce: 0.024254\n","iteration 12086 : loss : 0.089241, loss_ce: 0.030552\n","iteration 12087 : loss : 0.082698, loss_ce: 0.032792\n","iteration 12088 : loss : 0.075331, loss_ce: 0.021234\n","iteration 12089 : loss : 0.087358, loss_ce: 0.030389\n","iteration 12090 : loss : 0.410738, loss_ce: 0.004615\n"," 87%|█████████████████████████▏   | 130/150 [2:50:04<25:59, 77.98s/it]iteration 12091 : loss : 0.129587, loss_ce: 0.015242\n","iteration 12092 : loss : 0.145528, loss_ce: 0.025776\n","iteration 12093 : loss : 0.089548, loss_ce: 0.031051\n","iteration 12094 : loss : 0.131938, loss_ce: 0.029940\n","iteration 12095 : loss : 0.107539, loss_ce: 0.040090\n","iteration 12096 : loss : 0.188988, loss_ce: 0.026045\n","iteration 12097 : loss : 0.089287, loss_ce: 0.025444\n","iteration 12098 : loss : 0.098906, loss_ce: 0.028713\n","iteration 12099 : loss : 0.080379, loss_ce: 0.030237\n","iteration 12100 : loss : 0.105577, loss_ce: 0.032668\n","iteration 12101 : loss : 0.097914, loss_ce: 0.024076\n","iteration 12102 : loss : 0.099695, loss_ce: 0.035694\n","iteration 12103 : loss : 0.121088, loss_ce: 0.038788\n","iteration 12104 : loss : 0.103278, loss_ce: 0.040662\n","iteration 12105 : loss : 0.080540, loss_ce: 0.035448\n","iteration 12106 : loss : 0.090062, loss_ce: 0.027380\n","iteration 12107 : loss : 0.095709, loss_ce: 0.044175\n","iteration 12108 : loss : 0.093271, loss_ce: 0.024751\n","iteration 12109 : loss : 0.091791, loss_ce: 0.029045\n","iteration 12110 : loss : 0.105894, loss_ce: 0.026092\n","iteration 12111 : loss : 0.088843, loss_ce: 0.026788\n","iteration 12112 : loss : 0.074591, loss_ce: 0.026794\n","iteration 12113 : loss : 0.113260, loss_ce: 0.041760\n","iteration 12114 : loss : 0.098157, loss_ce: 0.034624\n","iteration 12115 : loss : 0.095248, loss_ce: 0.040513\n","iteration 12116 : loss : 0.101815, loss_ce: 0.040631\n","iteration 12117 : loss : 0.096396, loss_ce: 0.030351\n","iteration 12118 : loss : 0.091730, loss_ce: 0.036642\n","iteration 12119 : loss : 0.144898, loss_ce: 0.020939\n","iteration 12120 : loss : 0.087695, loss_ce: 0.040218\n","iteration 12121 : loss : 0.130283, loss_ce: 0.025815\n","iteration 12122 : loss : 0.147498, loss_ce: 0.017205\n","iteration 12123 : loss : 0.142234, loss_ce: 0.026468\n","iteration 12124 : loss : 0.107387, loss_ce: 0.027470\n","iteration 12125 : loss : 0.074818, loss_ce: 0.025371\n","iteration 12126 : loss : 0.127598, loss_ce: 0.027195\n","iteration 12127 : loss : 0.097219, loss_ce: 0.033191\n","iteration 12128 : loss : 0.143858, loss_ce: 0.029767\n","iteration 12129 : loss : 0.110410, loss_ce: 0.037778\n","iteration 12130 : loss : 0.155910, loss_ce: 0.023749\n","iteration 12131 : loss : 0.086925, loss_ce: 0.033469\n","iteration 12132 : loss : 0.085173, loss_ce: 0.032848\n","iteration 12133 : loss : 0.091618, loss_ce: 0.025490\n","iteration 12134 : loss : 0.089178, loss_ce: 0.018781\n","iteration 12135 : loss : 0.090101, loss_ce: 0.035498\n","iteration 12136 : loss : 0.142843, loss_ce: 0.032582\n","iteration 12137 : loss : 0.096535, loss_ce: 0.026283\n","iteration 12138 : loss : 0.079102, loss_ce: 0.020472\n","iteration 12139 : loss : 0.102227, loss_ce: 0.039868\n","iteration 12140 : loss : 0.083446, loss_ce: 0.028016\n","iteration 12141 : loss : 0.140263, loss_ce: 0.019465\n","iteration 12142 : loss : 0.093895, loss_ce: 0.023012\n","iteration 12143 : loss : 0.098965, loss_ce: 0.021729\n","iteration 12144 : loss : 0.093505, loss_ce: 0.035391\n","iteration 12145 : loss : 0.084379, loss_ce: 0.024142\n","iteration 12146 : loss : 0.092859, loss_ce: 0.022871\n","iteration 12147 : loss : 0.094897, loss_ce: 0.032949\n","iteration 12148 : loss : 0.086994, loss_ce: 0.034495\n","iteration 12149 : loss : 0.109236, loss_ce: 0.034984\n","iteration 12150 : loss : 0.099254, loss_ce: 0.018492\n","iteration 12151 : loss : 0.080187, loss_ce: 0.026957\n","iteration 12152 : loss : 0.100396, loss_ce: 0.025517\n","iteration 12153 : loss : 0.080263, loss_ce: 0.027256\n","iteration 12154 : loss : 0.117492, loss_ce: 0.039185\n","iteration 12155 : loss : 0.153845, loss_ce: 0.025793\n","iteration 12156 : loss : 0.087246, loss_ce: 0.024896\n","iteration 12157 : loss : 0.099981, loss_ce: 0.029668\n","iteration 12158 : loss : 0.096083, loss_ce: 0.024937\n","iteration 12159 : loss : 0.104607, loss_ce: 0.029499\n","iteration 12160 : loss : 0.099485, loss_ce: 0.021670\n","iteration 12161 : loss : 0.101532, loss_ce: 0.027085\n","iteration 12162 : loss : 0.153644, loss_ce: 0.034455\n","iteration 12163 : loss : 0.107468, loss_ce: 0.027657\n","iteration 12164 : loss : 0.092329, loss_ce: 0.013415\n","iteration 12165 : loss : 0.112564, loss_ce: 0.028594\n","iteration 12166 : loss : 0.125046, loss_ce: 0.023445\n","iteration 12167 : loss : 0.087062, loss_ce: 0.036983\n","iteration 12168 : loss : 0.102539, loss_ce: 0.040091\n","iteration 12169 : loss : 0.132287, loss_ce: 0.021729\n","iteration 12170 : loss : 0.084064, loss_ce: 0.023899\n","iteration 12171 : loss : 0.143161, loss_ce: 0.005333\n","iteration 12172 : loss : 0.156575, loss_ce: 0.038067\n","iteration 12173 : loss : 0.089926, loss_ce: 0.023159\n","iteration 12174 : loss : 0.083257, loss_ce: 0.028124\n","iteration 12175 : loss : 0.092819, loss_ce: 0.028909\n","iteration 12176 : loss : 0.119624, loss_ce: 0.037906\n","iteration 12177 : loss : 0.118038, loss_ce: 0.033253\n","iteration 12178 : loss : 0.149750, loss_ce: 0.028305\n","iteration 12179 : loss : 0.102274, loss_ce: 0.025312\n","iteration 12180 : loss : 0.144653, loss_ce: 0.025448\n","iteration 12181 : loss : 0.081431, loss_ce: 0.021704\n","iteration 12182 : loss : 0.131063, loss_ce: 0.023832\n","iteration 12183 : loss : 0.532922, loss_ce: 0.002536\n"," 87%|█████████████████████████▎   | 131/150 [2:51:25<24:56, 78.75s/it]iteration 12184 : loss : 0.097116, loss_ce: 0.022817\n","iteration 12185 : loss : 0.078507, loss_ce: 0.033213\n","iteration 12186 : loss : 0.089859, loss_ce: 0.033669\n","iteration 12187 : loss : 0.145884, loss_ce: 0.027028\n","iteration 12188 : loss : 0.088669, loss_ce: 0.027695\n","iteration 12189 : loss : 0.087506, loss_ce: 0.033043\n","iteration 12190 : loss : 0.190479, loss_ce: 0.016332\n","iteration 12191 : loss : 0.097634, loss_ce: 0.026574\n","iteration 12192 : loss : 0.083623, loss_ce: 0.016858\n","iteration 12193 : loss : 0.138630, loss_ce: 0.030880\n","iteration 12194 : loss : 0.085005, loss_ce: 0.031170\n","iteration 12195 : loss : 0.073623, loss_ce: 0.014708\n","iteration 12196 : loss : 0.087478, loss_ce: 0.024145\n","iteration 12197 : loss : 0.091372, loss_ce: 0.031006\n","iteration 12198 : loss : 0.145262, loss_ce: 0.029768\n","iteration 12199 : loss : 0.106489, loss_ce: 0.027426\n","iteration 12200 : loss : 0.122312, loss_ce: 0.021286\n","iteration 12201 : loss : 0.084212, loss_ce: 0.032363\n","iteration 12202 : loss : 0.084611, loss_ce: 0.026404\n","iteration 12203 : loss : 0.081764, loss_ce: 0.028226\n","iteration 12204 : loss : 0.082864, loss_ce: 0.038483\n","iteration 12205 : loss : 0.072084, loss_ce: 0.025947\n","iteration 12206 : loss : 0.101721, loss_ce: 0.035763\n","iteration 12207 : loss : 0.095949, loss_ce: 0.021445\n","iteration 12208 : loss : 0.132002, loss_ce: 0.025781\n","iteration 12209 : loss : 0.097742, loss_ce: 0.036877\n","iteration 12210 : loss : 0.083407, loss_ce: 0.030030\n","iteration 12211 : loss : 0.099604, loss_ce: 0.041569\n","iteration 12212 : loss : 0.145029, loss_ce: 0.032594\n","iteration 12213 : loss : 0.100418, loss_ce: 0.020528\n","iteration 12214 : loss : 0.083934, loss_ce: 0.028823\n","iteration 12215 : loss : 0.131005, loss_ce: 0.030120\n","iteration 12216 : loss : 0.091993, loss_ce: 0.014707\n","iteration 12217 : loss : 0.077907, loss_ce: 0.027218\n","iteration 12218 : loss : 0.102737, loss_ce: 0.016010\n","iteration 12219 : loss : 0.071961, loss_ce: 0.023285\n","iteration 12220 : loss : 0.126323, loss_ce: 0.026657\n","iteration 12221 : loss : 0.157389, loss_ce: 0.028135\n","iteration 12222 : loss : 0.097248, loss_ce: 0.042823\n","iteration 12223 : loss : 0.093564, loss_ce: 0.021977\n","iteration 12224 : loss : 0.088302, loss_ce: 0.033403\n","iteration 12225 : loss : 0.074194, loss_ce: 0.029626\n","iteration 12226 : loss : 0.112566, loss_ce: 0.039392\n","iteration 12227 : loss : 0.097521, loss_ce: 0.023393\n","iteration 12228 : loss : 0.090195, loss_ce: 0.019387\n","iteration 12229 : loss : 0.160042, loss_ce: 0.022508\n","iteration 12230 : loss : 0.115669, loss_ce: 0.042177\n","iteration 12231 : loss : 0.077844, loss_ce: 0.026269\n","iteration 12232 : loss : 0.094124, loss_ce: 0.024732\n","iteration 12233 : loss : 0.087914, loss_ce: 0.033753\n","iteration 12234 : loss : 0.100458, loss_ce: 0.041842\n","iteration 12235 : loss : 0.118210, loss_ce: 0.021671\n","iteration 12236 : loss : 0.073651, loss_ce: 0.017980\n","iteration 12237 : loss : 0.153388, loss_ce: 0.019634\n","iteration 12238 : loss : 0.088706, loss_ce: 0.037419\n","iteration 12239 : loss : 0.091344, loss_ce: 0.029301\n","iteration 12240 : loss : 0.082788, loss_ce: 0.023185\n","iteration 12241 : loss : 0.099086, loss_ce: 0.025702\n","iteration 12242 : loss : 0.119690, loss_ce: 0.036123\n","iteration 12243 : loss : 0.090334, loss_ce: 0.021723\n","iteration 12244 : loss : 0.100114, loss_ce: 0.020549\n","iteration 12245 : loss : 0.095047, loss_ce: 0.044981\n","iteration 12246 : loss : 0.097485, loss_ce: 0.021388\n","iteration 12247 : loss : 0.091634, loss_ce: 0.025318\n","iteration 12248 : loss : 0.128604, loss_ce: 0.026457\n","iteration 12249 : loss : 0.120897, loss_ce: 0.015385\n","iteration 12250 : loss : 0.086465, loss_ce: 0.043954\n","iteration 12251 : loss : 0.107602, loss_ce: 0.039391\n","iteration 12252 : loss : 0.098133, loss_ce: 0.034386\n","iteration 12253 : loss : 0.095125, loss_ce: 0.036330\n","iteration 12254 : loss : 0.122445, loss_ce: 0.028026\n","iteration 12255 : loss : 0.075485, loss_ce: 0.020533\n","iteration 12256 : loss : 0.135235, loss_ce: 0.015442\n","iteration 12257 : loss : 0.100706, loss_ce: 0.027066\n","iteration 12258 : loss : 0.090983, loss_ce: 0.023668\n","iteration 12259 : loss : 0.097446, loss_ce: 0.034790\n","iteration 12260 : loss : 0.100382, loss_ce: 0.029462\n","iteration 12261 : loss : 0.091452, loss_ce: 0.028201\n","iteration 12262 : loss : 0.089674, loss_ce: 0.035722\n","iteration 12263 : loss : 0.111214, loss_ce: 0.034958\n","iteration 12264 : loss : 0.120701, loss_ce: 0.032788\n","iteration 12265 : loss : 0.072513, loss_ce: 0.022488\n","iteration 12266 : loss : 0.082529, loss_ce: 0.016737\n","iteration 12267 : loss : 0.095857, loss_ce: 0.023387\n","iteration 12268 : loss : 0.082113, loss_ce: 0.037006\n","iteration 12269 : loss : 0.085496, loss_ce: 0.025752\n","iteration 12270 : loss : 0.108555, loss_ce: 0.029494\n","iteration 12271 : loss : 0.100902, loss_ce: 0.033550\n","iteration 12272 : loss : 0.083428, loss_ce: 0.018338\n","iteration 12273 : loss : 0.099291, loss_ce: 0.031596\n","iteration 12274 : loss : 0.135738, loss_ce: 0.029039\n","iteration 12275 : loss : 0.114365, loss_ce: 0.036585\n","iteration 12276 : loss : 0.409041, loss_ce: 0.019634\n"," 88%|█████████████████████████▌   | 132/150 [2:52:41<23:22, 77.91s/it]iteration 12277 : loss : 0.078827, loss_ce: 0.027174\n","iteration 12278 : loss : 0.095396, loss_ce: 0.032213\n","iteration 12279 : loss : 0.081818, loss_ce: 0.026928\n","iteration 12280 : loss : 0.089208, loss_ce: 0.030105\n","iteration 12281 : loss : 0.093998, loss_ce: 0.019938\n","iteration 12282 : loss : 0.084884, loss_ce: 0.024070\n","iteration 12283 : loss : 0.096721, loss_ce: 0.028203\n","iteration 12284 : loss : 0.086612, loss_ce: 0.017662\n","iteration 12285 : loss : 0.088696, loss_ce: 0.018762\n","iteration 12286 : loss : 0.096260, loss_ce: 0.026820\n","iteration 12287 : loss : 0.110869, loss_ce: 0.041736\n","iteration 12288 : loss : 0.094264, loss_ce: 0.038239\n","iteration 12289 : loss : 0.151511, loss_ce: 0.030812\n","iteration 12290 : loss : 0.086357, loss_ce: 0.029216\n","iteration 12291 : loss : 0.085157, loss_ce: 0.032041\n","iteration 12292 : loss : 0.128760, loss_ce: 0.014837\n","iteration 12293 : loss : 0.175737, loss_ce: 0.023402\n","iteration 12294 : loss : 0.098283, loss_ce: 0.032474\n","iteration 12295 : loss : 0.070673, loss_ce: 0.023251\n","iteration 12296 : loss : 0.070186, loss_ce: 0.023479\n","iteration 12297 : loss : 0.103947, loss_ce: 0.025770\n","iteration 12298 : loss : 0.073257, loss_ce: 0.017125\n","iteration 12299 : loss : 0.086067, loss_ce: 0.033624\n","iteration 12300 : loss : 0.122520, loss_ce: 0.028737\n","iteration 12301 : loss : 0.102977, loss_ce: 0.033829\n","iteration 12302 : loss : 0.092257, loss_ce: 0.023034\n","iteration 12303 : loss : 0.083401, loss_ce: 0.029696\n","iteration 12304 : loss : 0.091873, loss_ce: 0.028642\n","iteration 12305 : loss : 0.084504, loss_ce: 0.027664\n","iteration 12306 : loss : 0.100223, loss_ce: 0.033673\n","iteration 12307 : loss : 0.095396, loss_ce: 0.033731\n","iteration 12308 : loss : 0.106453, loss_ce: 0.031613\n","iteration 12309 : loss : 0.103825, loss_ce: 0.045312\n","iteration 12310 : loss : 0.082315, loss_ce: 0.035655\n","iteration 12311 : loss : 0.127126, loss_ce: 0.035566\n","iteration 12312 : loss : 0.095225, loss_ce: 0.026504\n","iteration 12313 : loss : 0.086432, loss_ce: 0.022575\n","iteration 12314 : loss : 0.115519, loss_ce: 0.028618\n","iteration 12315 : loss : 0.126835, loss_ce: 0.027326\n","iteration 12316 : loss : 0.076390, loss_ce: 0.034202\n","iteration 12317 : loss : 0.118708, loss_ce: 0.026099\n","iteration 12318 : loss : 0.087825, loss_ce: 0.030102\n","iteration 12319 : loss : 0.067985, loss_ce: 0.025302\n","iteration 12320 : loss : 0.153838, loss_ce: 0.017388\n","iteration 12321 : loss : 0.070731, loss_ce: 0.022771\n","iteration 12322 : loss : 0.098177, loss_ce: 0.034545\n","iteration 12323 : loss : 0.090015, loss_ce: 0.026516\n","iteration 12324 : loss : 0.072569, loss_ce: 0.028126\n","iteration 12325 : loss : 0.085783, loss_ce: 0.035259\n","iteration 12326 : loss : 0.080811, loss_ce: 0.027736\n","iteration 12327 : loss : 0.089376, loss_ce: 0.032533\n","iteration 12328 : loss : 0.086525, loss_ce: 0.024653\n","iteration 12329 : loss : 0.099346, loss_ce: 0.030479\n","iteration 12330 : loss : 0.094204, loss_ce: 0.031392\n","iteration 12331 : loss : 0.109511, loss_ce: 0.022157\n","iteration 12332 : loss : 0.093938, loss_ce: 0.028094\n","iteration 12333 : loss : 0.089971, loss_ce: 0.030761\n","iteration 12334 : loss : 0.173437, loss_ce: 0.032446\n","iteration 12335 : loss : 0.089592, loss_ce: 0.023988\n","iteration 12336 : loss : 0.110857, loss_ce: 0.022373\n","iteration 12337 : loss : 0.100073, loss_ce: 0.036424\n","iteration 12338 : loss : 0.121915, loss_ce: 0.025972\n","iteration 12339 : loss : 0.104841, loss_ce: 0.023156\n","iteration 12340 : loss : 0.139760, loss_ce: 0.018562\n","iteration 12341 : loss : 0.096505, loss_ce: 0.021197\n","iteration 12342 : loss : 0.085809, loss_ce: 0.017509\n","iteration 12343 : loss : 0.090864, loss_ce: 0.022311\n","iteration 12344 : loss : 0.082550, loss_ce: 0.027251\n","iteration 12345 : loss : 0.103156, loss_ce: 0.028905\n","iteration 12346 : loss : 0.080296, loss_ce: 0.032219\n","iteration 12347 : loss : 0.114077, loss_ce: 0.033151\n","iteration 12348 : loss : 0.075489, loss_ce: 0.022483\n","iteration 12349 : loss : 0.096220, loss_ce: 0.029581\n","iteration 12350 : loss : 0.091448, loss_ce: 0.037896\n","iteration 12351 : loss : 0.137078, loss_ce: 0.025554\n","iteration 12352 : loss : 0.078808, loss_ce: 0.028750\n","iteration 12353 : loss : 0.091528, loss_ce: 0.036104\n","iteration 12354 : loss : 0.095835, loss_ce: 0.030877\n","iteration 12355 : loss : 0.117236, loss_ce: 0.026742\n","iteration 12356 : loss : 0.089605, loss_ce: 0.024571\n","iteration 12357 : loss : 0.157771, loss_ce: 0.044346\n","iteration 12358 : loss : 0.118194, loss_ce: 0.031896\n","iteration 12359 : loss : 0.102804, loss_ce: 0.021824\n","iteration 12360 : loss : 0.127923, loss_ce: 0.025574\n","iteration 12361 : loss : 0.092710, loss_ce: 0.024514\n","iteration 12362 : loss : 0.081170, loss_ce: 0.027247\n","iteration 12363 : loss : 0.076481, loss_ce: 0.029364\n","iteration 12364 : loss : 0.076591, loss_ce: 0.025213\n","iteration 12365 : loss : 0.154556, loss_ce: 0.032373\n","iteration 12366 : loss : 0.116427, loss_ce: 0.025860\n","iteration 12367 : loss : 0.090737, loss_ce: 0.031219\n","iteration 12368 : loss : 0.086999, loss_ce: 0.017271\n","iteration 12369 : loss : 0.429349, loss_ce: 0.009772\n"," 89%|█████████████████████████▋   | 133/150 [2:54:00<22:12, 78.38s/it]iteration 12370 : loss : 0.088979, loss_ce: 0.019197\n","iteration 12371 : loss : 0.091535, loss_ce: 0.031704\n","iteration 12372 : loss : 0.086113, loss_ce: 0.016826\n","iteration 12373 : loss : 0.118126, loss_ce: 0.032969\n","iteration 12374 : loss : 0.101788, loss_ce: 0.023643\n","iteration 12375 : loss : 0.090869, loss_ce: 0.032969\n","iteration 12376 : loss : 0.097019, loss_ce: 0.026674\n","iteration 12377 : loss : 0.093182, loss_ce: 0.035387\n","iteration 12378 : loss : 0.089917, loss_ce: 0.026967\n","iteration 12379 : loss : 0.108810, loss_ce: 0.024983\n","iteration 12380 : loss : 0.097830, loss_ce: 0.032887\n","iteration 12381 : loss : 0.084549, loss_ce: 0.019700\n","iteration 12382 : loss : 0.112169, loss_ce: 0.042407\n","iteration 12383 : loss : 0.102350, loss_ce: 0.042836\n","iteration 12384 : loss : 0.091576, loss_ce: 0.032647\n","iteration 12385 : loss : 0.090722, loss_ce: 0.028823\n","iteration 12386 : loss : 0.093744, loss_ce: 0.027157\n","iteration 12387 : loss : 0.081304, loss_ce: 0.026718\n","iteration 12388 : loss : 0.087333, loss_ce: 0.032618\n","iteration 12389 : loss : 0.095005, loss_ce: 0.028190\n","iteration 12390 : loss : 0.084173, loss_ce: 0.019410\n","iteration 12391 : loss : 0.085718, loss_ce: 0.028912\n","iteration 12392 : loss : 0.116398, loss_ce: 0.026075\n","iteration 12393 : loss : 0.085693, loss_ce: 0.023993\n","iteration 12394 : loss : 0.100857, loss_ce: 0.042365\n","iteration 12395 : loss : 0.094781, loss_ce: 0.024738\n","iteration 12396 : loss : 0.108318, loss_ce: 0.032377\n","iteration 12397 : loss : 0.099442, loss_ce: 0.020747\n","iteration 12398 : loss : 0.140689, loss_ce: 0.028904\n","iteration 12399 : loss : 0.102531, loss_ce: 0.029311\n","iteration 12400 : loss : 0.094025, loss_ce: 0.034718\n","iteration 12401 : loss : 0.073337, loss_ce: 0.024050\n","iteration 12402 : loss : 0.071164, loss_ce: 0.011821\n","iteration 12403 : loss : 0.070168, loss_ce: 0.020281\n","iteration 12404 : loss : 0.083281, loss_ce: 0.026671\n","iteration 12405 : loss : 0.148224, loss_ce: 0.022660\n","iteration 12406 : loss : 0.113078, loss_ce: 0.019845\n","iteration 12407 : loss : 0.104659, loss_ce: 0.014595\n","iteration 12408 : loss : 0.079601, loss_ce: 0.024194\n","iteration 12409 : loss : 0.102424, loss_ce: 0.032735\n","iteration 12410 : loss : 0.103462, loss_ce: 0.023669\n","iteration 12411 : loss : 0.071066, loss_ce: 0.022817\n","iteration 12412 : loss : 0.154261, loss_ce: 0.030226\n","iteration 12413 : loss : 0.123155, loss_ce: 0.021932\n","iteration 12414 : loss : 0.074360, loss_ce: 0.028717\n","iteration 12415 : loss : 0.094148, loss_ce: 0.033837\n","iteration 12416 : loss : 0.096033, loss_ce: 0.026646\n","iteration 12417 : loss : 0.083981, loss_ce: 0.023236\n","iteration 12418 : loss : 0.092036, loss_ce: 0.042513\n","iteration 12419 : loss : 0.073649, loss_ce: 0.028039\n","iteration 12420 : loss : 0.111650, loss_ce: 0.030592\n","iteration 12421 : loss : 0.096447, loss_ce: 0.023938\n","iteration 12422 : loss : 0.091600, loss_ce: 0.034475\n","iteration 12423 : loss : 0.217009, loss_ce: 0.029295\n","iteration 12424 : loss : 0.082064, loss_ce: 0.029683\n","iteration 12425 : loss : 0.141275, loss_ce: 0.025995\n","iteration 12426 : loss : 0.083607, loss_ce: 0.022977\n","iteration 12427 : loss : 0.093631, loss_ce: 0.034098\n","iteration 12428 : loss : 0.102517, loss_ce: 0.025555\n","iteration 12429 : loss : 0.089838, loss_ce: 0.029436\n","iteration 12430 : loss : 0.084510, loss_ce: 0.029000\n","iteration 12431 : loss : 0.109845, loss_ce: 0.036979\n","iteration 12432 : loss : 0.086186, loss_ce: 0.031068\n","iteration 12433 : loss : 0.087372, loss_ce: 0.039748\n","iteration 12434 : loss : 0.097620, loss_ce: 0.026456\n","iteration 12435 : loss : 0.089367, loss_ce: 0.033700\n","iteration 12436 : loss : 0.158824, loss_ce: 0.026034\n","iteration 12437 : loss : 0.086772, loss_ce: 0.022520\n","iteration 12438 : loss : 0.093568, loss_ce: 0.042956\n","iteration 12439 : loss : 0.159071, loss_ce: 0.025789\n","iteration 12440 : loss : 0.072647, loss_ce: 0.036361\n","iteration 12441 : loss : 0.095245, loss_ce: 0.023537\n","iteration 12442 : loss : 0.084378, loss_ce: 0.033702\n","iteration 12443 : loss : 0.090590, loss_ce: 0.035171\n","iteration 12444 : loss : 0.109086, loss_ce: 0.016495\n","iteration 12445 : loss : 0.092997, loss_ce: 0.029323\n","iteration 12446 : loss : 0.089963, loss_ce: 0.034352\n","iteration 12447 : loss : 0.118957, loss_ce: 0.028890\n","iteration 12448 : loss : 0.077606, loss_ce: 0.024612\n","iteration 12449 : loss : 0.095186, loss_ce: 0.037903\n","iteration 12450 : loss : 0.132015, loss_ce: 0.022539\n","iteration 12451 : loss : 0.085218, loss_ce: 0.027481\n","iteration 12452 : loss : 0.194485, loss_ce: 0.026884\n","iteration 12453 : loss : 0.094249, loss_ce: 0.022679\n","iteration 12454 : loss : 0.108828, loss_ce: 0.026322\n","iteration 12455 : loss : 0.108091, loss_ce: 0.034259\n","iteration 12456 : loss : 0.094775, loss_ce: 0.030913\n","iteration 12457 : loss : 0.085987, loss_ce: 0.021791\n","iteration 12458 : loss : 0.142964, loss_ce: 0.015102\n","iteration 12459 : loss : 0.143287, loss_ce: 0.036424\n","iteration 12460 : loss : 0.079264, loss_ce: 0.019589\n","iteration 12461 : loss : 0.130586, loss_ce: 0.024772\n","iteration 12462 : loss : 0.181544, loss_ce: 0.033106\n"," 89%|█████████████████████████▉   | 134/150 [2:55:21<21:07, 79.20s/it]iteration 12463 : loss : 0.078878, loss_ce: 0.032783\n","iteration 12464 : loss : 0.081813, loss_ce: 0.020497\n","iteration 12465 : loss : 0.117481, loss_ce: 0.016102\n","iteration 12466 : loss : 0.094029, loss_ce: 0.034179\n","iteration 12467 : loss : 0.092722, loss_ce: 0.028084\n","iteration 12468 : loss : 0.095293, loss_ce: 0.023517\n","iteration 12469 : loss : 0.105884, loss_ce: 0.037030\n","iteration 12470 : loss : 0.106458, loss_ce: 0.035857\n","iteration 12471 : loss : 0.088965, loss_ce: 0.031319\n","iteration 12472 : loss : 0.102218, loss_ce: 0.028140\n","iteration 12473 : loss : 0.101970, loss_ce: 0.022468\n","iteration 12474 : loss : 0.075807, loss_ce: 0.031139\n","iteration 12475 : loss : 0.101425, loss_ce: 0.032209\n","iteration 12476 : loss : 0.079216, loss_ce: 0.029074\n","iteration 12477 : loss : 0.113565, loss_ce: 0.029901\n","iteration 12478 : loss : 0.103948, loss_ce: 0.029547\n","iteration 12479 : loss : 0.082123, loss_ce: 0.021522\n","iteration 12480 : loss : 0.090942, loss_ce: 0.034653\n","iteration 12481 : loss : 0.099388, loss_ce: 0.045108\n","iteration 12482 : loss : 0.119733, loss_ce: 0.020309\n","iteration 12483 : loss : 0.082111, loss_ce: 0.030464\n","iteration 12484 : loss : 0.074756, loss_ce: 0.025328\n","iteration 12485 : loss : 0.095376, loss_ce: 0.026289\n","iteration 12486 : loss : 0.085947, loss_ce: 0.037205\n","iteration 12487 : loss : 0.089510, loss_ce: 0.027676\n","iteration 12488 : loss : 0.081606, loss_ce: 0.030673\n","iteration 12489 : loss : 0.087701, loss_ce: 0.038579\n","iteration 12490 : loss : 0.103877, loss_ce: 0.031540\n","iteration 12491 : loss : 0.077476, loss_ce: 0.034635\n","iteration 12492 : loss : 0.081829, loss_ce: 0.023920\n","iteration 12493 : loss : 0.090639, loss_ce: 0.032255\n","iteration 12494 : loss : 0.072123, loss_ce: 0.027888\n","iteration 12495 : loss : 0.075922, loss_ce: 0.023039\n","iteration 12496 : loss : 0.060429, loss_ce: 0.015100\n","iteration 12497 : loss : 0.100338, loss_ce: 0.029904\n","iteration 12498 : loss : 0.078792, loss_ce: 0.018029\n","iteration 12499 : loss : 0.086415, loss_ce: 0.036846\n","iteration 12500 : loss : 0.142879, loss_ce: 0.025739\n","iteration 12501 : loss : 0.152512, loss_ce: 0.025263\n","iteration 12502 : loss : 0.117965, loss_ce: 0.040893\n","iteration 12503 : loss : 0.108358, loss_ce: 0.026254\n","iteration 12504 : loss : 0.075794, loss_ce: 0.024290\n","iteration 12505 : loss : 0.100580, loss_ce: 0.025471\n","iteration 12506 : loss : 0.141267, loss_ce: 0.029954\n","iteration 12507 : loss : 0.072823, loss_ce: 0.022570\n","iteration 12508 : loss : 0.124122, loss_ce: 0.028841\n","iteration 12509 : loss : 0.068434, loss_ce: 0.023778\n","iteration 12510 : loss : 0.075561, loss_ce: 0.020364\n","iteration 12511 : loss : 0.113836, loss_ce: 0.026828\n","iteration 12512 : loss : 0.093373, loss_ce: 0.027018\n","iteration 12513 : loss : 0.098821, loss_ce: 0.019498\n","iteration 12514 : loss : 0.077560, loss_ce: 0.028826\n","iteration 12515 : loss : 0.081664, loss_ce: 0.034629\n","iteration 12516 : loss : 0.189449, loss_ce: 0.016874\n","iteration 12517 : loss : 0.106030, loss_ce: 0.021270\n","iteration 12518 : loss : 0.101405, loss_ce: 0.022360\n","iteration 12519 : loss : 0.101710, loss_ce: 0.029742\n","iteration 12520 : loss : 0.085285, loss_ce: 0.029119\n","iteration 12521 : loss : 0.132373, loss_ce: 0.031174\n","iteration 12522 : loss : 0.108885, loss_ce: 0.023861\n","iteration 12523 : loss : 0.113064, loss_ce: 0.034133\n","iteration 12524 : loss : 0.085652, loss_ce: 0.036413\n","iteration 12525 : loss : 0.093869, loss_ce: 0.026269\n","iteration 12526 : loss : 0.099816, loss_ce: 0.025722\n","iteration 12527 : loss : 0.085491, loss_ce: 0.026577\n","iteration 12528 : loss : 0.101409, loss_ce: 0.037746\n","iteration 12529 : loss : 0.092870, loss_ce: 0.024329\n","iteration 12530 : loss : 0.071607, loss_ce: 0.010799\n","iteration 12531 : loss : 0.074318, loss_ce: 0.021011\n","iteration 12532 : loss : 0.070810, loss_ce: 0.011651\n","iteration 12533 : loss : 0.128724, loss_ce: 0.026437\n","iteration 12534 : loss : 0.091921, loss_ce: 0.020828\n","iteration 12535 : loss : 0.104476, loss_ce: 0.027416\n","iteration 12536 : loss : 0.085257, loss_ce: 0.019198\n","iteration 12537 : loss : 0.134173, loss_ce: 0.020037\n","iteration 12538 : loss : 0.074977, loss_ce: 0.021192\n","iteration 12539 : loss : 0.067869, loss_ce: 0.023167\n","iteration 12540 : loss : 0.085766, loss_ce: 0.018685\n","iteration 12541 : loss : 0.070744, loss_ce: 0.033247\n","iteration 12542 : loss : 0.133043, loss_ce: 0.021725\n","iteration 12543 : loss : 0.192623, loss_ce: 0.012692\n","iteration 12544 : loss : 0.105945, loss_ce: 0.029066\n","iteration 12545 : loss : 0.129928, loss_ce: 0.017173\n","iteration 12546 : loss : 0.145741, loss_ce: 0.021345\n","iteration 12547 : loss : 0.080919, loss_ce: 0.030471\n","iteration 12548 : loss : 0.125346, loss_ce: 0.027327\n","iteration 12549 : loss : 0.090180, loss_ce: 0.029632\n","iteration 12550 : loss : 0.097581, loss_ce: 0.025133\n","iteration 12551 : loss : 0.103625, loss_ce: 0.027832\n","iteration 12552 : loss : 0.140933, loss_ce: 0.028092\n","iteration 12553 : loss : 0.080021, loss_ce: 0.033994\n","iteration 12554 : loss : 0.103335, loss_ce: 0.038068\n","iteration 12555 : loss : 0.467039, loss_ce: 0.003444\n"," 90%|██████████████████████████   | 135/150 [2:56:36<19:27, 77.86s/it]iteration 12556 : loss : 0.086314, loss_ce: 0.027355\n","iteration 12557 : loss : 0.101654, loss_ce: 0.034274\n","iteration 12558 : loss : 0.101818, loss_ce: 0.032965\n","iteration 12559 : loss : 0.106346, loss_ce: 0.022162\n","iteration 12560 : loss : 0.086270, loss_ce: 0.034794\n","iteration 12561 : loss : 0.087843, loss_ce: 0.024411\n","iteration 12562 : loss : 0.078251, loss_ce: 0.032024\n","iteration 12563 : loss : 0.076109, loss_ce: 0.025904\n","iteration 12564 : loss : 0.097181, loss_ce: 0.028617\n","iteration 12565 : loss : 0.093071, loss_ce: 0.032150\n","iteration 12566 : loss : 0.090061, loss_ce: 0.024789\n","iteration 12567 : loss : 0.112102, loss_ce: 0.036708\n","iteration 12568 : loss : 0.096048, loss_ce: 0.035277\n","iteration 12569 : loss : 0.101205, loss_ce: 0.016658\n","iteration 12570 : loss : 0.093463, loss_ce: 0.027717\n","iteration 12571 : loss : 0.097716, loss_ce: 0.029675\n","iteration 12572 : loss : 0.077607, loss_ce: 0.029820\n","iteration 12573 : loss : 0.083286, loss_ce: 0.044731\n","iteration 12574 : loss : 0.086053, loss_ce: 0.025228\n","iteration 12575 : loss : 0.078974, loss_ce: 0.037754\n","iteration 12576 : loss : 0.079569, loss_ce: 0.033573\n","iteration 12577 : loss : 0.174400, loss_ce: 0.013510\n","iteration 12578 : loss : 0.078382, loss_ce: 0.031031\n","iteration 12579 : loss : 0.097863, loss_ce: 0.025898\n","iteration 12580 : loss : 0.170429, loss_ce: 0.023394\n","iteration 12581 : loss : 0.096942, loss_ce: 0.027608\n","iteration 12582 : loss : 0.122061, loss_ce: 0.015551\n","iteration 12583 : loss : 0.092380, loss_ce: 0.027644\n","iteration 12584 : loss : 0.188611, loss_ce: 0.008538\n","iteration 12585 : loss : 0.076248, loss_ce: 0.034617\n","iteration 12586 : loss : 0.137282, loss_ce: 0.034015\n","iteration 12587 : loss : 0.083964, loss_ce: 0.024831\n","iteration 12588 : loss : 0.084212, loss_ce: 0.038064\n","iteration 12589 : loss : 0.075782, loss_ce: 0.021599\n","iteration 12590 : loss : 0.077653, loss_ce: 0.025982\n","iteration 12591 : loss : 0.089820, loss_ce: 0.038952\n","iteration 12592 : loss : 0.146129, loss_ce: 0.021077\n","iteration 12593 : loss : 0.092544, loss_ce: 0.023593\n","iteration 12594 : loss : 0.090543, loss_ce: 0.041476\n","iteration 12595 : loss : 0.089322, loss_ce: 0.024832\n","iteration 12596 : loss : 0.109467, loss_ce: 0.023683\n","iteration 12597 : loss : 0.070587, loss_ce: 0.023181\n","iteration 12598 : loss : 0.094199, loss_ce: 0.043006\n","iteration 12599 : loss : 0.096716, loss_ce: 0.028183\n","iteration 12600 : loss : 0.134216, loss_ce: 0.018143\n","iteration 12601 : loss : 0.087989, loss_ce: 0.028470\n","iteration 12602 : loss : 0.118031, loss_ce: 0.033464\n","iteration 12603 : loss : 0.086273, loss_ce: 0.018678\n","iteration 12604 : loss : 0.090445, loss_ce: 0.040521\n","iteration 12605 : loss : 0.094770, loss_ce: 0.033026\n","iteration 12606 : loss : 0.098082, loss_ce: 0.025241\n","iteration 12607 : loss : 0.074759, loss_ce: 0.022689\n","iteration 12608 : loss : 0.073631, loss_ce: 0.027540\n","iteration 12609 : loss : 0.096054, loss_ce: 0.028874\n","iteration 12610 : loss : 0.087034, loss_ce: 0.029908\n","iteration 12611 : loss : 0.081000, loss_ce: 0.029348\n","iteration 12612 : loss : 0.087016, loss_ce: 0.028754\n","iteration 12613 : loss : 0.075265, loss_ce: 0.026830\n","iteration 12614 : loss : 0.091293, loss_ce: 0.018335\n","iteration 12615 : loss : 0.114638, loss_ce: 0.020473\n","iteration 12616 : loss : 0.136544, loss_ce: 0.024676\n","iteration 12617 : loss : 0.096537, loss_ce: 0.025570\n","iteration 12618 : loss : 0.102342, loss_ce: 0.026940\n","iteration 12619 : loss : 0.095731, loss_ce: 0.021044\n","iteration 12620 : loss : 0.084657, loss_ce: 0.017100\n","iteration 12621 : loss : 0.103189, loss_ce: 0.029450\n","iteration 12622 : loss : 0.114515, loss_ce: 0.017455\n","iteration 12623 : loss : 0.081237, loss_ce: 0.030085\n","iteration 12624 : loss : 0.138435, loss_ce: 0.028020\n","iteration 12625 : loss : 0.079104, loss_ce: 0.022688\n","iteration 12626 : loss : 0.079508, loss_ce: 0.026773\n","iteration 12627 : loss : 0.115144, loss_ce: 0.022212\n","iteration 12628 : loss : 0.110877, loss_ce: 0.013359\n","iteration 12629 : loss : 0.149959, loss_ce: 0.033055\n","iteration 12630 : loss : 0.097363, loss_ce: 0.030901\n","iteration 12631 : loss : 0.118237, loss_ce: 0.024040\n","iteration 12632 : loss : 0.087571, loss_ce: 0.026496\n","iteration 12633 : loss : 0.121846, loss_ce: 0.016475\n","iteration 12634 : loss : 0.122282, loss_ce: 0.032795\n","iteration 12635 : loss : 0.070901, loss_ce: 0.021092\n","iteration 12636 : loss : 0.091249, loss_ce: 0.028109\n","iteration 12637 : loss : 0.075104, loss_ce: 0.024070\n","iteration 12638 : loss : 0.097069, loss_ce: 0.030514\n","iteration 12639 : loss : 0.090793, loss_ce: 0.029263\n","iteration 12640 : loss : 0.086946, loss_ce: 0.023099\n","iteration 12641 : loss : 0.083454, loss_ce: 0.027020\n","iteration 12642 : loss : 0.080763, loss_ce: 0.025911\n","iteration 12643 : loss : 0.090967, loss_ce: 0.028232\n","iteration 12644 : loss : 0.102218, loss_ce: 0.017560\n","iteration 12645 : loss : 0.114016, loss_ce: 0.024030\n","iteration 12646 : loss : 0.093311, loss_ce: 0.036104\n","iteration 12647 : loss : 0.073467, loss_ce: 0.021582\n","iteration 12648 : loss : 0.520611, loss_ce: 0.000423\n"," 91%|██████████████████████████▎  | 136/150 [2:57:55<18:15, 78.24s/it]iteration 12649 : loss : 0.084141, loss_ce: 0.034776\n","iteration 12650 : loss : 0.098634, loss_ce: 0.043444\n","iteration 12651 : loss : 0.086333, loss_ce: 0.022924\n","iteration 12652 : loss : 0.091002, loss_ce: 0.027007\n","iteration 12653 : loss : 0.107909, loss_ce: 0.031444\n","iteration 12654 : loss : 0.085312, loss_ce: 0.034958\n","iteration 12655 : loss : 0.075573, loss_ce: 0.017665\n","iteration 12656 : loss : 0.093044, loss_ce: 0.030226\n","iteration 12657 : loss : 0.102266, loss_ce: 0.018964\n","iteration 12658 : loss : 0.084943, loss_ce: 0.029913\n","iteration 12659 : loss : 0.077940, loss_ce: 0.025030\n","iteration 12660 : loss : 0.087325, loss_ce: 0.023659\n","iteration 12661 : loss : 0.117628, loss_ce: 0.027236\n","iteration 12662 : loss : 0.103845, loss_ce: 0.029858\n","iteration 12663 : loss : 0.113702, loss_ce: 0.033952\n","iteration 12664 : loss : 0.070959, loss_ce: 0.021422\n","iteration 12665 : loss : 0.108086, loss_ce: 0.028298\n","iteration 12666 : loss : 0.177835, loss_ce: 0.008160\n","iteration 12667 : loss : 0.084525, loss_ce: 0.028705\n","iteration 12668 : loss : 0.087490, loss_ce: 0.020636\n","iteration 12669 : loss : 0.151838, loss_ce: 0.019557\n","iteration 12670 : loss : 0.305181, loss_ce: 0.005527\n","iteration 12671 : loss : 0.113771, loss_ce: 0.021804\n","iteration 12672 : loss : 0.075975, loss_ce: 0.016757\n","iteration 12673 : loss : 0.095276, loss_ce: 0.028875\n","iteration 12674 : loss : 0.078120, loss_ce: 0.033545\n","iteration 12675 : loss : 0.093441, loss_ce: 0.015628\n","iteration 12676 : loss : 0.153681, loss_ce: 0.039297\n","iteration 12677 : loss : 0.136480, loss_ce: 0.011938\n","iteration 12678 : loss : 0.068381, loss_ce: 0.021829\n","iteration 12679 : loss : 0.079625, loss_ce: 0.031456\n","iteration 12680 : loss : 0.129796, loss_ce: 0.024329\n","iteration 12681 : loss : 0.070184, loss_ce: 0.023919\n","iteration 12682 : loss : 0.076715, loss_ce: 0.028273\n","iteration 12683 : loss : 0.075212, loss_ce: 0.028472\n","iteration 12684 : loss : 0.137747, loss_ce: 0.025930\n","iteration 12685 : loss : 0.081031, loss_ce: 0.031073\n","iteration 12686 : loss : 0.128986, loss_ce: 0.029910\n","iteration 12687 : loss : 0.100646, loss_ce: 0.034013\n","iteration 12688 : loss : 0.098033, loss_ce: 0.026730\n","iteration 12689 : loss : 0.082096, loss_ce: 0.030989\n","iteration 12690 : loss : 0.106385, loss_ce: 0.030596\n","iteration 12691 : loss : 0.108725, loss_ce: 0.037553\n","iteration 12692 : loss : 0.096092, loss_ce: 0.032499\n","iteration 12693 : loss : 0.069954, loss_ce: 0.033039\n","iteration 12694 : loss : 0.088615, loss_ce: 0.028093\n","iteration 12695 : loss : 0.104592, loss_ce: 0.030770\n","iteration 12696 : loss : 0.086340, loss_ce: 0.031015\n","iteration 12697 : loss : 0.091563, loss_ce: 0.036725\n","iteration 12698 : loss : 0.215982, loss_ce: 0.016092\n","iteration 12699 : loss : 0.122819, loss_ce: 0.015997\n","iteration 12700 : loss : 0.087775, loss_ce: 0.017651\n","iteration 12701 : loss : 0.101070, loss_ce: 0.023463\n","iteration 12702 : loss : 0.127615, loss_ce: 0.026079\n","iteration 12703 : loss : 0.081323, loss_ce: 0.029482\n","iteration 12704 : loss : 0.091005, loss_ce: 0.028794\n","iteration 12705 : loss : 0.104430, loss_ce: 0.017456\n","iteration 12706 : loss : 0.093191, loss_ce: 0.032669\n","iteration 12707 : loss : 0.091492, loss_ce: 0.023238\n","iteration 12708 : loss : 0.107869, loss_ce: 0.030939\n","iteration 12709 : loss : 0.108269, loss_ce: 0.035937\n","iteration 12710 : loss : 0.174896, loss_ce: 0.016172\n","iteration 12711 : loss : 0.094949, loss_ce: 0.028383\n","iteration 12712 : loss : 0.092956, loss_ce: 0.041950\n","iteration 12713 : loss : 0.143425, loss_ce: 0.021981\n","iteration 12714 : loss : 0.076395, loss_ce: 0.025609\n","iteration 12715 : loss : 0.073857, loss_ce: 0.033749\n","iteration 12716 : loss : 0.083015, loss_ce: 0.027417\n","iteration 12717 : loss : 0.085692, loss_ce: 0.030602\n","iteration 12718 : loss : 0.102706, loss_ce: 0.028539\n","iteration 12719 : loss : 0.142786, loss_ce: 0.021288\n","iteration 12720 : loss : 0.072795, loss_ce: 0.025180\n","iteration 12721 : loss : 0.089295, loss_ce: 0.022256\n","iteration 12722 : loss : 0.080966, loss_ce: 0.034627\n","iteration 12723 : loss : 0.123169, loss_ce: 0.024740\n","iteration 12724 : loss : 0.128678, loss_ce: 0.025172\n","iteration 12725 : loss : 0.087252, loss_ce: 0.030373\n","iteration 12726 : loss : 0.090365, loss_ce: 0.027658\n","iteration 12727 : loss : 0.082281, loss_ce: 0.026148\n","iteration 12728 : loss : 0.076749, loss_ce: 0.028060\n","iteration 12729 : loss : 0.134816, loss_ce: 0.022461\n","iteration 12730 : loss : 0.100098, loss_ce: 0.034456\n","iteration 12731 : loss : 0.110211, loss_ce: 0.022948\n","iteration 12732 : loss : 0.122738, loss_ce: 0.031297\n","iteration 12733 : loss : 0.081227, loss_ce: 0.018859\n","iteration 12734 : loss : 0.089079, loss_ce: 0.028960\n","iteration 12735 : loss : 0.103522, loss_ce: 0.023158\n","iteration 12736 : loss : 0.134410, loss_ce: 0.023913\n","iteration 12737 : loss : 0.082522, loss_ce: 0.014372\n","iteration 12738 : loss : 0.114491, loss_ce: 0.052836\n","iteration 12739 : loss : 0.096958, loss_ce: 0.029111\n","iteration 12740 : loss : 0.093281, loss_ce: 0.025050\n","iteration 12741 : loss : 0.097120, loss_ce: 0.060420\n"," 91%|██████████████████████████▍  | 137/150 [2:59:15<17:05, 78.86s/it]iteration 12742 : loss : 0.071591, loss_ce: 0.026832\n","iteration 12743 : loss : 0.098298, loss_ce: 0.027201\n","iteration 12744 : loss : 0.087002, loss_ce: 0.041566\n","iteration 12745 : loss : 0.091189, loss_ce: 0.039120\n","iteration 12746 : loss : 0.090584, loss_ce: 0.027321\n","iteration 12747 : loss : 0.101512, loss_ce: 0.037877\n","iteration 12748 : loss : 0.072940, loss_ce: 0.030117\n","iteration 12749 : loss : 0.084800, loss_ce: 0.034640\n","iteration 12750 : loss : 0.100564, loss_ce: 0.028973\n","iteration 12751 : loss : 0.073070, loss_ce: 0.032657\n","iteration 12752 : loss : 0.083143, loss_ce: 0.023625\n","iteration 12753 : loss : 0.093353, loss_ce: 0.027925\n","iteration 12754 : loss : 0.086885, loss_ce: 0.019614\n","iteration 12755 : loss : 0.112030, loss_ce: 0.038834\n","iteration 12756 : loss : 0.119785, loss_ce: 0.038155\n","iteration 12757 : loss : 0.136639, loss_ce: 0.017113\n","iteration 12758 : loss : 0.068373, loss_ce: 0.018823\n","iteration 12759 : loss : 0.072402, loss_ce: 0.029316\n","iteration 12760 : loss : 0.072046, loss_ce: 0.020561\n","iteration 12761 : loss : 0.094622, loss_ce: 0.037064\n","iteration 12762 : loss : 0.070502, loss_ce: 0.020623\n","iteration 12763 : loss : 0.066307, loss_ce: 0.021632\n","iteration 12764 : loss : 0.073053, loss_ce: 0.024864\n","iteration 12765 : loss : 0.089078, loss_ce: 0.030514\n","iteration 12766 : loss : 0.148007, loss_ce: 0.009524\n","iteration 12767 : loss : 0.096975, loss_ce: 0.020602\n","iteration 12768 : loss : 0.080430, loss_ce: 0.020405\n","iteration 12769 : loss : 0.090684, loss_ce: 0.035912\n","iteration 12770 : loss : 0.089409, loss_ce: 0.034483\n","iteration 12771 : loss : 0.141490, loss_ce: 0.013795\n","iteration 12772 : loss : 0.094670, loss_ce: 0.037744\n","iteration 12773 : loss : 0.146638, loss_ce: 0.019220\n","iteration 12774 : loss : 0.077688, loss_ce: 0.019940\n","iteration 12775 : loss : 0.084348, loss_ce: 0.029194\n","iteration 12776 : loss : 0.079649, loss_ce: 0.026255\n","iteration 12777 : loss : 0.094568, loss_ce: 0.034454\n","iteration 12778 : loss : 0.100908, loss_ce: 0.027430\n","iteration 12779 : loss : 0.075496, loss_ce: 0.030932\n","iteration 12780 : loss : 0.091186, loss_ce: 0.017664\n","iteration 12781 : loss : 0.074298, loss_ce: 0.017630\n","iteration 12782 : loss : 0.089285, loss_ce: 0.020974\n","iteration 12783 : loss : 0.102862, loss_ce: 0.035873\n","iteration 12784 : loss : 0.093912, loss_ce: 0.029170\n","iteration 12785 : loss : 0.080731, loss_ce: 0.020154\n","iteration 12786 : loss : 0.088105, loss_ce: 0.022480\n","iteration 12787 : loss : 0.148496, loss_ce: 0.024722\n","iteration 12788 : loss : 0.093120, loss_ce: 0.033087\n","iteration 12789 : loss : 0.077550, loss_ce: 0.031889\n","iteration 12790 : loss : 0.084500, loss_ce: 0.011914\n","iteration 12791 : loss : 0.084884, loss_ce: 0.030253\n","iteration 12792 : loss : 0.098393, loss_ce: 0.051307\n","iteration 12793 : loss : 0.083557, loss_ce: 0.025735\n","iteration 12794 : loss : 0.149186, loss_ce: 0.034672\n","iteration 12795 : loss : 0.082980, loss_ce: 0.019198\n","iteration 12796 : loss : 0.089934, loss_ce: 0.023846\n","iteration 12797 : loss : 0.081376, loss_ce: 0.022382\n","iteration 12798 : loss : 0.074461, loss_ce: 0.021333\n","iteration 12799 : loss : 0.094009, loss_ce: 0.027788\n","iteration 12800 : loss : 0.079323, loss_ce: 0.030740\n","iteration 12801 : loss : 0.103392, loss_ce: 0.012735\n","iteration 12802 : loss : 0.096807, loss_ce: 0.025629\n","iteration 12803 : loss : 0.170556, loss_ce: 0.023065\n","iteration 12804 : loss : 0.082101, loss_ce: 0.024149\n","iteration 12805 : loss : 0.136198, loss_ce: 0.021366\n","iteration 12806 : loss : 0.083592, loss_ce: 0.017095\n","iteration 12807 : loss : 0.076686, loss_ce: 0.026576\n","iteration 12808 : loss : 0.101365, loss_ce: 0.016975\n","iteration 12809 : loss : 0.093892, loss_ce: 0.021523\n","iteration 12810 : loss : 0.088103, loss_ce: 0.029037\n","iteration 12811 : loss : 0.079855, loss_ce: 0.022957\n","iteration 12812 : loss : 0.080653, loss_ce: 0.025288\n","iteration 12813 : loss : 0.078760, loss_ce: 0.025674\n","iteration 12814 : loss : 0.101201, loss_ce: 0.018214\n","iteration 12815 : loss : 0.088647, loss_ce: 0.024148\n","iteration 12816 : loss : 0.141688, loss_ce: 0.020953\n","iteration 12817 : loss : 0.095931, loss_ce: 0.028586\n","iteration 12818 : loss : 0.103440, loss_ce: 0.032483\n","iteration 12819 : loss : 0.135517, loss_ce: 0.017383\n","iteration 12820 : loss : 0.088660, loss_ce: 0.029832\n","iteration 12821 : loss : 0.164995, loss_ce: 0.016226\n","iteration 12822 : loss : 0.074503, loss_ce: 0.016342\n","iteration 12823 : loss : 0.091545, loss_ce: 0.022111\n","iteration 12824 : loss : 0.087582, loss_ce: 0.018522\n","iteration 12825 : loss : 0.092487, loss_ce: 0.039215\n","iteration 12826 : loss : 0.103232, loss_ce: 0.024834\n","iteration 12827 : loss : 0.147949, loss_ce: 0.024572\n","iteration 12828 : loss : 0.094074, loss_ce: 0.033738\n","iteration 12829 : loss : 0.094510, loss_ce: 0.039141\n","iteration 12830 : loss : 0.148914, loss_ce: 0.024810\n","iteration 12831 : loss : 0.080612, loss_ce: 0.038002\n","iteration 12832 : loss : 0.084365, loss_ce: 0.024763\n","iteration 12833 : loss : 0.110258, loss_ce: 0.021723\n","iteration 12834 : loss : 0.362928, loss_ce: 0.010277\n"," 92%|██████████████████████████▋  | 138/150 [3:00:32<15:38, 78.21s/it]iteration 12835 : loss : 0.097769, loss_ce: 0.032563\n","iteration 12836 : loss : 0.171320, loss_ce: 0.026370\n","iteration 12837 : loss : 0.088759, loss_ce: 0.020641\n","iteration 12838 : loss : 0.068163, loss_ce: 0.027364\n","iteration 12839 : loss : 0.128155, loss_ce: 0.010899\n","iteration 12840 : loss : 0.100316, loss_ce: 0.024955\n","iteration 12841 : loss : 0.096099, loss_ce: 0.022408\n","iteration 12842 : loss : 0.108699, loss_ce: 0.032514\n","iteration 12843 : loss : 0.065196, loss_ce: 0.020617\n","iteration 12844 : loss : 0.086168, loss_ce: 0.036810\n","iteration 12845 : loss : 0.103476, loss_ce: 0.036752\n","iteration 12846 : loss : 0.073731, loss_ce: 0.015589\n","iteration 12847 : loss : 0.081328, loss_ce: 0.023391\n","iteration 12848 : loss : 0.087361, loss_ce: 0.028122\n","iteration 12849 : loss : 0.136867, loss_ce: 0.015765\n","iteration 12850 : loss : 0.089646, loss_ce: 0.031980\n","iteration 12851 : loss : 0.081741, loss_ce: 0.027503\n","iteration 12852 : loss : 0.093577, loss_ce: 0.038891\n","iteration 12853 : loss : 0.068785, loss_ce: 0.024052\n","iteration 12854 : loss : 0.077343, loss_ce: 0.039148\n","iteration 12855 : loss : 0.076342, loss_ce: 0.015941\n","iteration 12856 : loss : 0.089441, loss_ce: 0.020734\n","iteration 12857 : loss : 0.086798, loss_ce: 0.025455\n","iteration 12858 : loss : 0.082035, loss_ce: 0.030186\n","iteration 12859 : loss : 0.070002, loss_ce: 0.027367\n","iteration 12860 : loss : 0.086328, loss_ce: 0.032837\n","iteration 12861 : loss : 0.077348, loss_ce: 0.019749\n","iteration 12862 : loss : 0.087302, loss_ce: 0.039174\n","iteration 12863 : loss : 0.153743, loss_ce: 0.028459\n","iteration 12864 : loss : 0.074723, loss_ce: 0.026640\n","iteration 12865 : loss : 0.114036, loss_ce: 0.035103\n","iteration 12866 : loss : 0.076497, loss_ce: 0.033423\n","iteration 12867 : loss : 0.097488, loss_ce: 0.017997\n","iteration 12868 : loss : 0.082545, loss_ce: 0.027632\n","iteration 12869 : loss : 0.093186, loss_ce: 0.027089\n","iteration 12870 : loss : 0.097129, loss_ce: 0.035688\n","iteration 12871 : loss : 0.104592, loss_ce: 0.022378\n","iteration 12872 : loss : 0.069114, loss_ce: 0.021270\n","iteration 12873 : loss : 0.071201, loss_ce: 0.015820\n","iteration 12874 : loss : 0.097558, loss_ce: 0.016914\n","iteration 12875 : loss : 0.103846, loss_ce: 0.025700\n","iteration 12876 : loss : 0.080780, loss_ce: 0.031186\n","iteration 12877 : loss : 0.103849, loss_ce: 0.021656\n","iteration 12878 : loss : 0.081013, loss_ce: 0.019820\n","iteration 12879 : loss : 0.079314, loss_ce: 0.016109\n","iteration 12880 : loss : 0.083079, loss_ce: 0.033783\n","iteration 12881 : loss : 0.101376, loss_ce: 0.041311\n","iteration 12882 : loss : 0.093238, loss_ce: 0.028779\n","iteration 12883 : loss : 0.098603, loss_ce: 0.022105\n","iteration 12884 : loss : 0.156616, loss_ce: 0.027101\n","iteration 12885 : loss : 0.107168, loss_ce: 0.022470\n","iteration 12886 : loss : 0.091366, loss_ce: 0.033183\n","iteration 12887 : loss : 0.069887, loss_ce: 0.022731\n","iteration 12888 : loss : 0.101979, loss_ce: 0.033462\n","iteration 12889 : loss : 0.079676, loss_ce: 0.024337\n","iteration 12890 : loss : 0.078344, loss_ce: 0.020036\n","iteration 12891 : loss : 0.081557, loss_ce: 0.023836\n","iteration 12892 : loss : 0.092244, loss_ce: 0.034301\n","iteration 12893 : loss : 0.115051, loss_ce: 0.033207\n","iteration 12894 : loss : 0.098089, loss_ce: 0.036066\n","iteration 12895 : loss : 0.082805, loss_ce: 0.029551\n","iteration 12896 : loss : 0.087018, loss_ce: 0.029923\n","iteration 12897 : loss : 0.107251, loss_ce: 0.024410\n","iteration 12898 : loss : 0.078018, loss_ce: 0.033560\n","iteration 12899 : loss : 0.091505, loss_ce: 0.029250\n","iteration 12900 : loss : 0.082418, loss_ce: 0.028739\n","iteration 12901 : loss : 0.093548, loss_ce: 0.020244\n","iteration 12902 : loss : 0.131166, loss_ce: 0.031886\n","iteration 12903 : loss : 0.083896, loss_ce: 0.017699\n","iteration 12904 : loss : 0.091484, loss_ce: 0.029951\n","iteration 12905 : loss : 0.088488, loss_ce: 0.026334\n","iteration 12906 : loss : 0.068697, loss_ce: 0.019174\n","iteration 12907 : loss : 0.083901, loss_ce: 0.040473\n","iteration 12908 : loss : 0.091236, loss_ce: 0.029478\n","iteration 12909 : loss : 0.128911, loss_ce: 0.018293\n","iteration 12910 : loss : 0.081162, loss_ce: 0.023506\n","iteration 12911 : loss : 0.135170, loss_ce: 0.016083\n","iteration 12912 : loss : 0.086652, loss_ce: 0.026559\n","iteration 12913 : loss : 0.115045, loss_ce: 0.022027\n","iteration 12914 : loss : 0.084362, loss_ce: 0.021933\n","iteration 12915 : loss : 0.077623, loss_ce: 0.029359\n","iteration 12916 : loss : 0.091921, loss_ce: 0.026770\n","iteration 12917 : loss : 0.074613, loss_ce: 0.021022\n","iteration 12918 : loss : 0.103546, loss_ce: 0.023495\n","iteration 12919 : loss : 0.185200, loss_ce: 0.028929\n","iteration 12920 : loss : 0.168028, loss_ce: 0.020120\n","iteration 12921 : loss : 0.075794, loss_ce: 0.021045\n","iteration 12922 : loss : 0.173374, loss_ce: 0.027746\n","iteration 12923 : loss : 0.072092, loss_ce: 0.029962\n","iteration 12924 : loss : 0.093679, loss_ce: 0.031162\n","iteration 12925 : loss : 0.083241, loss_ce: 0.027908\n","iteration 12926 : loss : 0.096423, loss_ce: 0.035456\n","iteration 12927 : loss : 0.196007, loss_ce: 0.049754\n"," 93%|██████████████████████████▊  | 139/150 [3:01:50<14:18, 78.01s/it]iteration 12928 : loss : 0.088433, loss_ce: 0.021103\n","iteration 12929 : loss : 0.142173, loss_ce: 0.026092\n","iteration 12930 : loss : 0.154669, loss_ce: 0.026418\n","iteration 12931 : loss : 0.088377, loss_ce: 0.029058\n","iteration 12932 : loss : 0.091196, loss_ce: 0.023273\n","iteration 12933 : loss : 0.080548, loss_ce: 0.032769\n","iteration 12934 : loss : 0.089163, loss_ce: 0.019500\n","iteration 12935 : loss : 0.078278, loss_ce: 0.040138\n","iteration 12936 : loss : 0.078640, loss_ce: 0.017439\n","iteration 12937 : loss : 0.089345, loss_ce: 0.030749\n","iteration 12938 : loss : 0.084575, loss_ce: 0.020125\n","iteration 12939 : loss : 0.093757, loss_ce: 0.028119\n","iteration 12940 : loss : 0.087582, loss_ce: 0.021247\n","iteration 12941 : loss : 0.106617, loss_ce: 0.030636\n","iteration 12942 : loss : 0.070625, loss_ce: 0.021614\n","iteration 12943 : loss : 0.136836, loss_ce: 0.034092\n","iteration 12944 : loss : 0.080563, loss_ce: 0.036250\n","iteration 12945 : loss : 0.094830, loss_ce: 0.029918\n","iteration 12946 : loss : 0.061760, loss_ce: 0.011868\n","iteration 12947 : loss : 0.100889, loss_ce: 0.018281\n","iteration 12948 : loss : 0.090558, loss_ce: 0.022499\n","iteration 12949 : loss : 0.102535, loss_ce: 0.036205\n","iteration 12950 : loss : 0.077608, loss_ce: 0.027186\n","iteration 12951 : loss : 0.123510, loss_ce: 0.039071\n","iteration 12952 : loss : 0.087197, loss_ce: 0.025007\n","iteration 12953 : loss : 0.094393, loss_ce: 0.033850\n","iteration 12954 : loss : 0.085897, loss_ce: 0.021382\n","iteration 12955 : loss : 0.078785, loss_ce: 0.032162\n","iteration 12956 : loss : 0.127514, loss_ce: 0.020544\n","iteration 12957 : loss : 0.081715, loss_ce: 0.017394\n","iteration 12958 : loss : 0.080662, loss_ce: 0.032023\n","iteration 12959 : loss : 0.090056, loss_ce: 0.014598\n","iteration 12960 : loss : 0.081591, loss_ce: 0.020492\n","iteration 12961 : loss : 0.095483, loss_ce: 0.028805\n","iteration 12962 : loss : 0.076675, loss_ce: 0.028186\n","iteration 12963 : loss : 0.076899, loss_ce: 0.017533\n","iteration 12964 : loss : 0.139982, loss_ce: 0.068250\n","iteration 12965 : loss : 0.079485, loss_ce: 0.027359\n","iteration 12966 : loss : 0.077353, loss_ce: 0.023238\n","iteration 12967 : loss : 0.104573, loss_ce: 0.020687\n","iteration 12968 : loss : 0.083332, loss_ce: 0.033056\n","iteration 12969 : loss : 0.078939, loss_ce: 0.026970\n","iteration 12970 : loss : 0.146407, loss_ce: 0.013988\n","iteration 12971 : loss : 0.142584, loss_ce: 0.023243\n","iteration 12972 : loss : 0.088878, loss_ce: 0.024610\n","iteration 12973 : loss : 0.103339, loss_ce: 0.033727\n","iteration 12974 : loss : 0.082169, loss_ce: 0.025529\n","iteration 12975 : loss : 0.077225, loss_ce: 0.018199\n","iteration 12976 : loss : 0.080902, loss_ce: 0.021446\n","iteration 12977 : loss : 0.071258, loss_ce: 0.021804\n","iteration 12978 : loss : 0.091678, loss_ce: 0.035573\n","iteration 12979 : loss : 0.084051, loss_ce: 0.033681\n","iteration 12980 : loss : 0.098603, loss_ce: 0.029037\n","iteration 12981 : loss : 0.077321, loss_ce: 0.016231\n","iteration 12982 : loss : 0.099330, loss_ce: 0.019991\n","iteration 12983 : loss : 0.113777, loss_ce: 0.019954\n","iteration 12984 : loss : 0.074122, loss_ce: 0.025444\n","iteration 12985 : loss : 0.074045, loss_ce: 0.019362\n","iteration 12986 : loss : 0.097837, loss_ce: 0.037315\n","iteration 12987 : loss : 0.086787, loss_ce: 0.032023\n","iteration 12988 : loss : 0.085053, loss_ce: 0.027790\n","iteration 12989 : loss : 0.085898, loss_ce: 0.033920\n","iteration 12990 : loss : 0.115786, loss_ce: 0.024293\n","iteration 12991 : loss : 0.155805, loss_ce: 0.015361\n","iteration 12992 : loss : 0.076831, loss_ce: 0.025868\n","iteration 12993 : loss : 0.088494, loss_ce: 0.046862\n","iteration 12994 : loss : 0.083014, loss_ce: 0.034593\n","iteration 12995 : loss : 0.093114, loss_ce: 0.032291\n","iteration 12996 : loss : 0.102626, loss_ce: 0.024729\n","iteration 12997 : loss : 0.111236, loss_ce: 0.016245\n","iteration 12998 : loss : 0.085140, loss_ce: 0.024486\n","iteration 12999 : loss : 0.077044, loss_ce: 0.023692\n","iteration 13000 : loss : 0.092204, loss_ce: 0.024391\n","iteration 13001 : loss : 0.071621, loss_ce: 0.022572\n","iteration 13002 : loss : 0.120225, loss_ce: 0.021983\n","iteration 13003 : loss : 0.083091, loss_ce: 0.034808\n","iteration 13004 : loss : 0.086067, loss_ce: 0.020778\n","iteration 13005 : loss : 0.115297, loss_ce: 0.031351\n","iteration 13006 : loss : 0.078485, loss_ce: 0.025596\n","iteration 13007 : loss : 0.078604, loss_ce: 0.032067\n","iteration 13008 : loss : 0.096230, loss_ce: 0.031117\n","iteration 13009 : loss : 0.082074, loss_ce: 0.024065\n","iteration 13010 : loss : 0.079475, loss_ce: 0.032784\n","iteration 13011 : loss : 0.167003, loss_ce: 0.013697\n","iteration 13012 : loss : 0.071923, loss_ce: 0.024727\n","iteration 13013 : loss : 0.079468, loss_ce: 0.025876\n","iteration 13014 : loss : 0.082896, loss_ce: 0.020961\n","iteration 13015 : loss : 0.082775, loss_ce: 0.023396\n","iteration 13016 : loss : 0.099553, loss_ce: 0.031186\n","iteration 13017 : loss : 0.097763, loss_ce: 0.038437\n","iteration 13018 : loss : 0.081036, loss_ce: 0.030300\n","iteration 13019 : loss : 0.080447, loss_ce: 0.029946\n","iteration 13020 : loss : 0.262064, loss_ce: 0.036809\n"," 93%|███████████████████████████  | 140/150 [3:03:07<12:59, 77.97s/it]iteration 13021 : loss : 0.103025, loss_ce: 0.029127\n","iteration 13022 : loss : 0.094973, loss_ce: 0.026648\n","iteration 13023 : loss : 0.089963, loss_ce: 0.033292\n","iteration 13024 : loss : 0.087320, loss_ce: 0.029374\n","iteration 13025 : loss : 0.117083, loss_ce: 0.021567\n","iteration 13026 : loss : 0.070647, loss_ce: 0.020549\n","iteration 13027 : loss : 0.090798, loss_ce: 0.019781\n","iteration 13028 : loss : 0.076529, loss_ce: 0.016526\n","iteration 13029 : loss : 0.085237, loss_ce: 0.018122\n","iteration 13030 : loss : 0.068988, loss_ce: 0.025638\n","iteration 13031 : loss : 0.076865, loss_ce: 0.023851\n","iteration 13032 : loss : 0.118077, loss_ce: 0.012130\n","iteration 13033 : loss : 0.123228, loss_ce: 0.022660\n","iteration 13034 : loss : 0.091035, loss_ce: 0.031978\n","iteration 13035 : loss : 0.093202, loss_ce: 0.033514\n","iteration 13036 : loss : 0.096909, loss_ce: 0.027689\n","iteration 13037 : loss : 0.102886, loss_ce: 0.034674\n","iteration 13038 : loss : 0.075895, loss_ce: 0.018036\n","iteration 13039 : loss : 0.085818, loss_ce: 0.025713\n","iteration 13040 : loss : 0.080303, loss_ce: 0.012352\n","iteration 13041 : loss : 0.135547, loss_ce: 0.019610\n","iteration 13042 : loss : 0.116990, loss_ce: 0.019954\n","iteration 13043 : loss : 0.082883, loss_ce: 0.036680\n","iteration 13044 : loss : 0.082038, loss_ce: 0.021375\n","iteration 13045 : loss : 0.084003, loss_ce: 0.020361\n","iteration 13046 : loss : 0.085573, loss_ce: 0.030469\n","iteration 13047 : loss : 0.130925, loss_ce: 0.020728\n","iteration 13048 : loss : 0.087823, loss_ce: 0.028411\n","iteration 13049 : loss : 0.076251, loss_ce: 0.014495\n","iteration 13050 : loss : 0.077664, loss_ce: 0.029424\n","iteration 13051 : loss : 0.080329, loss_ce: 0.027963\n","iteration 13052 : loss : 0.091176, loss_ce: 0.016514\n","iteration 13053 : loss : 0.067687, loss_ce: 0.009895\n","iteration 13054 : loss : 0.069399, loss_ce: 0.019205\n","iteration 13055 : loss : 0.147377, loss_ce: 0.018520\n","iteration 13056 : loss : 0.097396, loss_ce: 0.023166\n","iteration 13057 : loss : 0.087114, loss_ce: 0.030814\n","iteration 13058 : loss : 0.081268, loss_ce: 0.033050\n","iteration 13059 : loss : 0.094273, loss_ce: 0.024800\n","iteration 13060 : loss : 0.114599, loss_ce: 0.047089\n","iteration 13061 : loss : 0.077091, loss_ce: 0.018934\n","iteration 13062 : loss : 0.077767, loss_ce: 0.022009\n","iteration 13063 : loss : 0.086959, loss_ce: 0.027568\n","iteration 13064 : loss : 0.079018, loss_ce: 0.036617\n","iteration 13065 : loss : 0.094859, loss_ce: 0.025467\n","iteration 13066 : loss : 0.069185, loss_ce: 0.023971\n","iteration 13067 : loss : 0.085825, loss_ce: 0.029945\n","iteration 13068 : loss : 0.076413, loss_ce: 0.018004\n","iteration 13069 : loss : 0.091796, loss_ce: 0.023289\n","iteration 13070 : loss : 0.093626, loss_ce: 0.029615\n","iteration 13071 : loss : 0.074254, loss_ce: 0.019742\n","iteration 13072 : loss : 0.074093, loss_ce: 0.026605\n","iteration 13073 : loss : 0.087244, loss_ce: 0.025439\n","iteration 13074 : loss : 0.127827, loss_ce: 0.022773\n","iteration 13075 : loss : 0.129605, loss_ce: 0.017939\n","iteration 13076 : loss : 0.075912, loss_ce: 0.020249\n","iteration 13077 : loss : 0.078721, loss_ce: 0.027680\n","iteration 13078 : loss : 0.075160, loss_ce: 0.024753\n","iteration 13079 : loss : 0.104406, loss_ce: 0.035129\n","iteration 13080 : loss : 0.083448, loss_ce: 0.028136\n","iteration 13081 : loss : 0.083916, loss_ce: 0.019839\n","iteration 13082 : loss : 0.082980, loss_ce: 0.032987\n","iteration 13083 : loss : 0.097517, loss_ce: 0.038654\n","iteration 13084 : loss : 0.099107, loss_ce: 0.023106\n","iteration 13085 : loss : 0.076554, loss_ce: 0.026799\n","iteration 13086 : loss : 0.085499, loss_ce: 0.022615\n","iteration 13087 : loss : 0.079389, loss_ce: 0.019116\n","iteration 13088 : loss : 0.078983, loss_ce: 0.032838\n","iteration 13089 : loss : 0.073820, loss_ce: 0.027268\n","iteration 13090 : loss : 0.084264, loss_ce: 0.026223\n","iteration 13091 : loss : 0.108378, loss_ce: 0.030331\n","iteration 13092 : loss : 0.087410, loss_ce: 0.022132\n","iteration 13093 : loss : 0.120974, loss_ce: 0.029071\n","iteration 13094 : loss : 0.079582, loss_ce: 0.030409\n","iteration 13095 : loss : 0.132352, loss_ce: 0.016781\n","iteration 13096 : loss : 0.085215, loss_ce: 0.029858\n","iteration 13097 : loss : 0.085046, loss_ce: 0.034644\n","iteration 13098 : loss : 0.078055, loss_ce: 0.017639\n","iteration 13099 : loss : 0.099054, loss_ce: 0.017797\n","iteration 13100 : loss : 0.106565, loss_ce: 0.022355\n","iteration 13101 : loss : 0.073535, loss_ce: 0.023641\n","iteration 13102 : loss : 0.069674, loss_ce: 0.019728\n","iteration 13103 : loss : 0.073258, loss_ce: 0.029127\n","iteration 13104 : loss : 0.100537, loss_ce: 0.020169\n","iteration 13105 : loss : 0.098859, loss_ce: 0.027989\n","iteration 13106 : loss : 0.080954, loss_ce: 0.025655\n","iteration 13107 : loss : 0.085957, loss_ce: 0.033466\n","iteration 13108 : loss : 0.069371, loss_ce: 0.021205\n","iteration 13109 : loss : 0.098360, loss_ce: 0.031083\n","iteration 13110 : loss : 0.103708, loss_ce: 0.033359\n","iteration 13111 : loss : 0.087823, loss_ce: 0.037484\n","iteration 13112 : loss : 0.067568, loss_ce: 0.023478\n","iteration 13113 : loss : 0.248420, loss_ce: 0.070609\n"," 94%|███████████████████████████▎ | 141/150 [3:04:23<11:35, 77.27s/it]iteration 13114 : loss : 0.096084, loss_ce: 0.025091\n","iteration 13115 : loss : 0.080001, loss_ce: 0.030647\n","iteration 13116 : loss : 0.107426, loss_ce: 0.029028\n","iteration 13117 : loss : 0.099007, loss_ce: 0.022563\n","iteration 13118 : loss : 0.103056, loss_ce: 0.032429\n","iteration 13119 : loss : 0.091208, loss_ce: 0.029013\n","iteration 13120 : loss : 0.088861, loss_ce: 0.035188\n","iteration 13121 : loss : 0.093451, loss_ce: 0.025766\n","iteration 13122 : loss : 0.120428, loss_ce: 0.023828\n","iteration 13123 : loss : 0.128792, loss_ce: 0.022939\n","iteration 13124 : loss : 0.098862, loss_ce: 0.029371\n","iteration 13125 : loss : 0.090057, loss_ce: 0.020871\n","iteration 13126 : loss : 0.210687, loss_ce: 0.022482\n","iteration 13127 : loss : 0.129503, loss_ce: 0.020835\n","iteration 13128 : loss : 0.069384, loss_ce: 0.016350\n","iteration 13129 : loss : 0.072311, loss_ce: 0.024441\n","iteration 13130 : loss : 0.106518, loss_ce: 0.024263\n","iteration 13131 : loss : 0.071824, loss_ce: 0.028894\n","iteration 13132 : loss : 0.088195, loss_ce: 0.029788\n","iteration 13133 : loss : 0.089961, loss_ce: 0.036168\n","iteration 13134 : loss : 0.074275, loss_ce: 0.024790\n","iteration 13135 : loss : 0.085202, loss_ce: 0.032208\n","iteration 13136 : loss : 0.065842, loss_ce: 0.018208\n","iteration 13137 : loss : 0.076697, loss_ce: 0.013187\n","iteration 13138 : loss : 0.149135, loss_ce: 0.031196\n","iteration 13139 : loss : 0.084739, loss_ce: 0.019615\n","iteration 13140 : loss : 0.065526, loss_ce: 0.024941\n","iteration 13141 : loss : 0.081706, loss_ce: 0.027829\n","iteration 13142 : loss : 0.089646, loss_ce: 0.028829\n","iteration 13143 : loss : 0.107474, loss_ce: 0.026774\n","iteration 13144 : loss : 0.117079, loss_ce: 0.027865\n","iteration 13145 : loss : 0.086557, loss_ce: 0.032904\n","iteration 13146 : loss : 0.089004, loss_ce: 0.025487\n","iteration 13147 : loss : 0.083677, loss_ce: 0.016459\n","iteration 13148 : loss : 0.074622, loss_ce: 0.030252\n","iteration 13149 : loss : 0.098760, loss_ce: 0.025606\n","iteration 13150 : loss : 0.073610, loss_ce: 0.025366\n","iteration 13151 : loss : 0.077841, loss_ce: 0.039686\n","iteration 13152 : loss : 0.077186, loss_ce: 0.021856\n","iteration 13153 : loss : 0.079703, loss_ce: 0.025590\n","iteration 13154 : loss : 0.094996, loss_ce: 0.030253\n","iteration 13155 : loss : 0.136248, loss_ce: 0.016487\n","iteration 13156 : loss : 0.076002, loss_ce: 0.020514\n","iteration 13157 : loss : 0.096436, loss_ce: 0.021153\n","iteration 13158 : loss : 0.083914, loss_ce: 0.028100\n","iteration 13159 : loss : 0.079217, loss_ce: 0.039917\n","iteration 13160 : loss : 0.077102, loss_ce: 0.026089\n","iteration 13161 : loss : 0.084233, loss_ce: 0.025902\n","iteration 13162 : loss : 0.084409, loss_ce: 0.020481\n","iteration 13163 : loss : 0.096035, loss_ce: 0.039743\n","iteration 13164 : loss : 0.091305, loss_ce: 0.026944\n","iteration 13165 : loss : 0.086813, loss_ce: 0.029605\n","iteration 13166 : loss : 0.094625, loss_ce: 0.026276\n","iteration 13167 : loss : 0.143372, loss_ce: 0.018227\n","iteration 13168 : loss : 0.070126, loss_ce: 0.017522\n","iteration 13169 : loss : 0.075958, loss_ce: 0.023820\n","iteration 13170 : loss : 0.083728, loss_ce: 0.026328\n","iteration 13171 : loss : 0.127376, loss_ce: 0.013021\n","iteration 13172 : loss : 0.097601, loss_ce: 0.025589\n","iteration 13173 : loss : 0.151299, loss_ce: 0.019040\n","iteration 13174 : loss : 0.085286, loss_ce: 0.027150\n","iteration 13175 : loss : 0.094293, loss_ce: 0.026466\n","iteration 13176 : loss : 0.086024, loss_ce: 0.024890\n","iteration 13177 : loss : 0.078485, loss_ce: 0.021602\n","iteration 13178 : loss : 0.093154, loss_ce: 0.022034\n","iteration 13179 : loss : 0.080636, loss_ce: 0.027253\n","iteration 13180 : loss : 0.108006, loss_ce: 0.017267\n","iteration 13181 : loss : 0.188436, loss_ce: 0.021072\n","iteration 13182 : loss : 0.095451, loss_ce: 0.032972\n","iteration 13183 : loss : 0.078788, loss_ce: 0.023215\n","iteration 13184 : loss : 0.076254, loss_ce: 0.028463\n","iteration 13185 : loss : 0.110820, loss_ce: 0.021315\n","iteration 13186 : loss : 0.084917, loss_ce: 0.019967\n","iteration 13187 : loss : 0.081170, loss_ce: 0.031660\n","iteration 13188 : loss : 0.079481, loss_ce: 0.027298\n","iteration 13189 : loss : 0.091139, loss_ce: 0.025920\n","iteration 13190 : loss : 0.081668, loss_ce: 0.023293\n","iteration 13191 : loss : 0.067532, loss_ce: 0.018814\n","iteration 13192 : loss : 0.182592, loss_ce: 0.013653\n","iteration 13193 : loss : 0.083046, loss_ce: 0.042472\n","iteration 13194 : loss : 0.090341, loss_ce: 0.035945\n","iteration 13195 : loss : 0.078787, loss_ce: 0.035870\n","iteration 13196 : loss : 0.085518, loss_ce: 0.024936\n","iteration 13197 : loss : 0.128902, loss_ce: 0.015885\n","iteration 13198 : loss : 0.088155, loss_ce: 0.025402\n","iteration 13199 : loss : 0.087289, loss_ce: 0.026776\n","iteration 13200 : loss : 0.092812, loss_ce: 0.026284\n","iteration 13201 : loss : 0.081097, loss_ce: 0.018464\n","iteration 13202 : loss : 0.078294, loss_ce: 0.019476\n","iteration 13203 : loss : 0.121302, loss_ce: 0.023411\n","iteration 13204 : loss : 0.081578, loss_ce: 0.036059\n","iteration 13205 : loss : 0.130581, loss_ce: 0.026965\n","iteration 13206 : loss : 0.531319, loss_ce: 0.001027\n"," 95%|███████████████████████████▍ | 142/150 [3:05:42<10:21, 77.68s/it]iteration 13207 : loss : 0.078900, loss_ce: 0.022302\n","iteration 13208 : loss : 0.091924, loss_ce: 0.031194\n","iteration 13209 : loss : 0.085061, loss_ce: 0.025151\n","iteration 13210 : loss : 0.082329, loss_ce: 0.027842\n","iteration 13211 : loss : 0.126748, loss_ce: 0.011008\n","iteration 13212 : loss : 0.096185, loss_ce: 0.036952\n","iteration 13213 : loss : 0.167586, loss_ce: 0.020995\n","iteration 13214 : loss : 0.136607, loss_ce: 0.030995\n","iteration 13215 : loss : 0.101413, loss_ce: 0.022295\n","iteration 13216 : loss : 0.080432, loss_ce: 0.030545\n","iteration 13217 : loss : 0.138958, loss_ce: 0.025196\n","iteration 13218 : loss : 0.107697, loss_ce: 0.032055\n","iteration 13219 : loss : 0.071541, loss_ce: 0.025270\n","iteration 13220 : loss : 0.068108, loss_ce: 0.019376\n","iteration 13221 : loss : 0.082188, loss_ce: 0.030732\n","iteration 13222 : loss : 0.072665, loss_ce: 0.026504\n","iteration 13223 : loss : 0.089264, loss_ce: 0.015856\n","iteration 13224 : loss : 0.093054, loss_ce: 0.034207\n","iteration 13225 : loss : 0.093778, loss_ce: 0.015942\n","iteration 13226 : loss : 0.068174, loss_ce: 0.023279\n","iteration 13227 : loss : 0.072608, loss_ce: 0.028987\n","iteration 13228 : loss : 0.085419, loss_ce: 0.017366\n","iteration 13229 : loss : 0.067628, loss_ce: 0.021537\n","iteration 13230 : loss : 0.067972, loss_ce: 0.024639\n","iteration 13231 : loss : 0.090223, loss_ce: 0.030459\n","iteration 13232 : loss : 0.302758, loss_ce: 0.020221\n","iteration 13233 : loss : 0.089828, loss_ce: 0.030651\n","iteration 13234 : loss : 0.160594, loss_ce: 0.022837\n","iteration 13235 : loss : 0.086832, loss_ce: 0.031412\n","iteration 13236 : loss : 0.079392, loss_ce: 0.016275\n","iteration 13237 : loss : 0.126812, loss_ce: 0.018481\n","iteration 13238 : loss : 0.080126, loss_ce: 0.030059\n","iteration 13239 : loss : 0.092037, loss_ce: 0.021070\n","iteration 13240 : loss : 0.067284, loss_ce: 0.018860\n","iteration 13241 : loss : 0.077270, loss_ce: 0.030077\n","iteration 13242 : loss : 0.071640, loss_ce: 0.023374\n","iteration 13243 : loss : 0.071297, loss_ce: 0.018173\n","iteration 13244 : loss : 0.113751, loss_ce: 0.016211\n","iteration 13245 : loss : 0.087961, loss_ce: 0.027287\n","iteration 13246 : loss : 0.079706, loss_ce: 0.023485\n","iteration 13247 : loss : 0.101347, loss_ce: 0.034907\n","iteration 13248 : loss : 0.096630, loss_ce: 0.028547\n","iteration 13249 : loss : 0.081764, loss_ce: 0.019250\n","iteration 13250 : loss : 0.072758, loss_ce: 0.023816\n","iteration 13251 : loss : 0.102766, loss_ce: 0.041310\n","iteration 13252 : loss : 0.068907, loss_ce: 0.025135\n","iteration 13253 : loss : 0.081426, loss_ce: 0.026076\n","iteration 13254 : loss : 0.068200, loss_ce: 0.023310\n","iteration 13255 : loss : 0.098079, loss_ce: 0.031306\n","iteration 13256 : loss : 0.099986, loss_ce: 0.019499\n","iteration 13257 : loss : 0.090643, loss_ce: 0.022542\n","iteration 13258 : loss : 0.088810, loss_ce: 0.023434\n","iteration 13259 : loss : 0.129330, loss_ce: 0.021239\n","iteration 13260 : loss : 0.094100, loss_ce: 0.026570\n","iteration 13261 : loss : 0.061556, loss_ce: 0.018957\n","iteration 13262 : loss : 0.072859, loss_ce: 0.020506\n","iteration 13263 : loss : 0.105724, loss_ce: 0.023166\n","iteration 13264 : loss : 0.095010, loss_ce: 0.034348\n","iteration 13265 : loss : 0.079609, loss_ce: 0.030640\n","iteration 13266 : loss : 0.085970, loss_ce: 0.017190\n","iteration 13267 : loss : 0.141207, loss_ce: 0.028951\n","iteration 13268 : loss : 0.099925, loss_ce: 0.020994\n","iteration 13269 : loss : 0.063762, loss_ce: 0.023362\n","iteration 13270 : loss : 0.096315, loss_ce: 0.034876\n","iteration 13271 : loss : 0.150056, loss_ce: 0.035153\n","iteration 13272 : loss : 0.080631, loss_ce: 0.020715\n","iteration 13273 : loss : 0.088165, loss_ce: 0.030925\n","iteration 13274 : loss : 0.074980, loss_ce: 0.022623\n","iteration 13275 : loss : 0.107985, loss_ce: 0.030664\n","iteration 13276 : loss : 0.074781, loss_ce: 0.035579\n","iteration 13277 : loss : 0.118071, loss_ce: 0.009608\n","iteration 13278 : loss : 0.136520, loss_ce: 0.020157\n","iteration 13279 : loss : 0.098288, loss_ce: 0.018536\n","iteration 13280 : loss : 0.089163, loss_ce: 0.034593\n","iteration 13281 : loss : 0.079331, loss_ce: 0.028890\n","iteration 13282 : loss : 0.096038, loss_ce: 0.023296\n","iteration 13283 : loss : 0.103985, loss_ce: 0.020440\n","iteration 13284 : loss : 0.097730, loss_ce: 0.021256\n","iteration 13285 : loss : 0.063381, loss_ce: 0.023833\n","iteration 13286 : loss : 0.099387, loss_ce: 0.031990\n","iteration 13287 : loss : 0.081998, loss_ce: 0.027942\n","iteration 13288 : loss : 0.131218, loss_ce: 0.017486\n","iteration 13289 : loss : 0.083975, loss_ce: 0.031765\n","iteration 13290 : loss : 0.074586, loss_ce: 0.025357\n","iteration 13291 : loss : 0.078193, loss_ce: 0.031150\n","iteration 13292 : loss : 0.104160, loss_ce: 0.031504\n","iteration 13293 : loss : 0.077555, loss_ce: 0.033735\n","iteration 13294 : loss : 0.071872, loss_ce: 0.032341\n","iteration 13295 : loss : 0.099312, loss_ce: 0.021646\n","iteration 13296 : loss : 0.060920, loss_ce: 0.019258\n","iteration 13297 : loss : 0.080726, loss_ce: 0.019367\n","iteration 13298 : loss : 0.085512, loss_ce: 0.023412\n","iteration 13299 : loss : 0.384608, loss_ce: 0.021801\n"," 95%|███████████████████████████▋ | 143/150 [3:07:03<09:11, 78.73s/it]iteration 13300 : loss : 0.083093, loss_ce: 0.033120\n","iteration 13301 : loss : 0.089202, loss_ce: 0.023629\n","iteration 13302 : loss : 0.083155, loss_ce: 0.028611\n","iteration 13303 : loss : 0.075188, loss_ce: 0.018423\n","iteration 13304 : loss : 0.091566, loss_ce: 0.018825\n","iteration 13305 : loss : 0.062617, loss_ce: 0.033605\n","iteration 13306 : loss : 0.081593, loss_ce: 0.030621\n","iteration 13307 : loss : 0.066423, loss_ce: 0.028395\n","iteration 13308 : loss : 0.082600, loss_ce: 0.032460\n","iteration 13309 : loss : 0.139705, loss_ce: 0.019715\n","iteration 13310 : loss : 0.096898, loss_ce: 0.041011\n","iteration 13311 : loss : 0.080121, loss_ce: 0.018032\n","iteration 13312 : loss : 0.153143, loss_ce: 0.024041\n","iteration 13313 : loss : 0.091579, loss_ce: 0.022554\n","iteration 13314 : loss : 0.067001, loss_ce: 0.030546\n","iteration 13315 : loss : 0.096567, loss_ce: 0.021893\n","iteration 13316 : loss : 0.082867, loss_ce: 0.024236\n","iteration 13317 : loss : 0.069104, loss_ce: 0.019844\n","iteration 13318 : loss : 0.090192, loss_ce: 0.029671\n","iteration 13319 : loss : 0.081301, loss_ce: 0.025849\n","iteration 13320 : loss : 0.091821, loss_ce: 0.026842\n","iteration 13321 : loss : 0.078480, loss_ce: 0.034657\n","iteration 13322 : loss : 0.068242, loss_ce: 0.028358\n","iteration 13323 : loss : 0.067871, loss_ce: 0.019624\n","iteration 13324 : loss : 0.072769, loss_ce: 0.015937\n","iteration 13325 : loss : 0.064041, loss_ce: 0.025830\n","iteration 13326 : loss : 0.066397, loss_ce: 0.018122\n","iteration 13327 : loss : 0.106853, loss_ce: 0.036601\n","iteration 13328 : loss : 0.109808, loss_ce: 0.028174\n","iteration 13329 : loss : 0.087146, loss_ce: 0.021413\n","iteration 13330 : loss : 0.085904, loss_ce: 0.043136\n","iteration 13331 : loss : 0.080133, loss_ce: 0.035964\n","iteration 13332 : loss : 0.066713, loss_ce: 0.022269\n","iteration 13333 : loss : 0.077775, loss_ce: 0.029972\n","iteration 13334 : loss : 0.091710, loss_ce: 0.036173\n","iteration 13335 : loss : 0.119037, loss_ce: 0.032758\n","iteration 13336 : loss : 0.099301, loss_ce: 0.016003\n","iteration 13337 : loss : 0.089925, loss_ce: 0.030449\n","iteration 13338 : loss : 0.134120, loss_ce: 0.031665\n","iteration 13339 : loss : 0.072840, loss_ce: 0.020643\n","iteration 13340 : loss : 0.103835, loss_ce: 0.024566\n","iteration 13341 : loss : 0.066476, loss_ce: 0.024396\n","iteration 13342 : loss : 0.116946, loss_ce: 0.019692\n","iteration 13343 : loss : 0.085653, loss_ce: 0.030796\n","iteration 13344 : loss : 0.107970, loss_ce: 0.021771\n","iteration 13345 : loss : 0.095224, loss_ce: 0.020970\n","iteration 13346 : loss : 0.083355, loss_ce: 0.025042\n","iteration 13347 : loss : 0.061730, loss_ce: 0.015564\n","iteration 13348 : loss : 0.132425, loss_ce: 0.015722\n","iteration 13349 : loss : 0.094889, loss_ce: 0.034330\n","iteration 13350 : loss : 0.091092, loss_ce: 0.025931\n","iteration 13351 : loss : 0.085437, loss_ce: 0.031307\n","iteration 13352 : loss : 0.087149, loss_ce: 0.028822\n","iteration 13353 : loss : 0.074727, loss_ce: 0.022321\n","iteration 13354 : loss : 0.118463, loss_ce: 0.019277\n","iteration 13355 : loss : 0.105976, loss_ce: 0.013817\n","iteration 13356 : loss : 0.077864, loss_ce: 0.022704\n","iteration 13357 : loss : 0.079456, loss_ce: 0.015195\n","iteration 13358 : loss : 0.084230, loss_ce: 0.023718\n","iteration 13359 : loss : 0.078446, loss_ce: 0.022185\n","iteration 13360 : loss : 0.092367, loss_ce: 0.022649\n","iteration 13361 : loss : 0.083051, loss_ce: 0.025499\n","iteration 13362 : loss : 0.075475, loss_ce: 0.024458\n","iteration 13363 : loss : 0.082348, loss_ce: 0.035583\n","iteration 13364 : loss : 0.097589, loss_ce: 0.030018\n","iteration 13365 : loss : 0.093170, loss_ce: 0.035366\n","iteration 13366 : loss : 0.076234, loss_ce: 0.037344\n","iteration 13367 : loss : 0.096571, loss_ce: 0.015601\n","iteration 13368 : loss : 0.103198, loss_ce: 0.031495\n","iteration 13369 : loss : 0.124250, loss_ce: 0.012269\n","iteration 13370 : loss : 0.105783, loss_ce: 0.019476\n","iteration 13371 : loss : 0.072148, loss_ce: 0.025745\n","iteration 13372 : loss : 0.093576, loss_ce: 0.017994\n","iteration 13373 : loss : 0.079921, loss_ce: 0.027446\n","iteration 13374 : loss : 0.071334, loss_ce: 0.019969\n","iteration 13375 : loss : 0.087907, loss_ce: 0.027018\n","iteration 13376 : loss : 0.097074, loss_ce: 0.029469\n","iteration 13377 : loss : 0.149896, loss_ce: 0.023498\n","iteration 13378 : loss : 0.074820, loss_ce: 0.013504\n","iteration 13379 : loss : 0.082271, loss_ce: 0.023750\n","iteration 13380 : loss : 0.098581, loss_ce: 0.022653\n","iteration 13381 : loss : 0.076298, loss_ce: 0.018521\n","iteration 13382 : loss : 0.078328, loss_ce: 0.021851\n","iteration 13383 : loss : 0.076280, loss_ce: 0.027709\n","iteration 13384 : loss : 0.071851, loss_ce: 0.030496\n","iteration 13385 : loss : 0.104564, loss_ce: 0.039123\n","iteration 13386 : loss : 0.086476, loss_ce: 0.028673\n","iteration 13387 : loss : 0.077694, loss_ce: 0.020984\n","iteration 13388 : loss : 0.087238, loss_ce: 0.027626\n","iteration 13389 : loss : 0.130395, loss_ce: 0.018132\n","iteration 13390 : loss : 0.083998, loss_ce: 0.021606\n","iteration 13391 : loss : 0.076614, loss_ce: 0.021777\n","iteration 13392 : loss : 0.379342, loss_ce: 0.014413\n"," 96%|███████████████████████████▊ | 144/150 [3:08:19<07:48, 78.08s/it]iteration 13393 : loss : 0.087277, loss_ce: 0.030824\n","iteration 13394 : loss : 0.067443, loss_ce: 0.026417\n","iteration 13395 : loss : 0.143366, loss_ce: 0.023588\n","iteration 13396 : loss : 0.073637, loss_ce: 0.033661\n","iteration 13397 : loss : 0.086176, loss_ce: 0.030597\n","iteration 13398 : loss : 0.070041, loss_ce: 0.031558\n","iteration 13399 : loss : 0.073560, loss_ce: 0.029128\n","iteration 13400 : loss : 0.063710, loss_ce: 0.024043\n","iteration 13401 : loss : 0.094988, loss_ce: 0.027241\n","iteration 13402 : loss : 0.096489, loss_ce: 0.028481\n","iteration 13403 : loss : 0.088582, loss_ce: 0.038253\n","iteration 13404 : loss : 0.144022, loss_ce: 0.013282\n","iteration 13405 : loss : 0.089527, loss_ce: 0.017227\n","iteration 13406 : loss : 0.075797, loss_ce: 0.025231\n","iteration 13407 : loss : 0.065453, loss_ce: 0.026787\n","iteration 13408 : loss : 0.089676, loss_ce: 0.021765\n","iteration 13409 : loss : 0.059283, loss_ce: 0.020960\n","iteration 13410 : loss : 0.132934, loss_ce: 0.016610\n","iteration 13411 : loss : 0.188235, loss_ce: 0.006597\n","iteration 13412 : loss : 0.074094, loss_ce: 0.030185\n","iteration 13413 : loss : 0.091822, loss_ce: 0.026853\n","iteration 13414 : loss : 0.092362, loss_ce: 0.017245\n","iteration 13415 : loss : 0.076964, loss_ce: 0.015037\n","iteration 13416 : loss : 0.078323, loss_ce: 0.020726\n","iteration 13417 : loss : 0.136315, loss_ce: 0.024115\n","iteration 13418 : loss : 0.149912, loss_ce: 0.027553\n","iteration 13419 : loss : 0.068239, loss_ce: 0.018950\n","iteration 13420 : loss : 0.079576, loss_ce: 0.021337\n","iteration 13421 : loss : 0.088069, loss_ce: 0.023369\n","iteration 13422 : loss : 0.144856, loss_ce: 0.026425\n","iteration 13423 : loss : 0.078182, loss_ce: 0.026641\n","iteration 13424 : loss : 0.081727, loss_ce: 0.020097\n","iteration 13425 : loss : 0.093907, loss_ce: 0.019133\n","iteration 13426 : loss : 0.087043, loss_ce: 0.030211\n","iteration 13427 : loss : 0.073968, loss_ce: 0.024487\n","iteration 13428 : loss : 0.070369, loss_ce: 0.021267\n","iteration 13429 : loss : 0.094496, loss_ce: 0.027459\n","iteration 13430 : loss : 0.077681, loss_ce: 0.025924\n","iteration 13431 : loss : 0.185288, loss_ce: 0.012768\n","iteration 13432 : loss : 0.103216, loss_ce: 0.024456\n","iteration 13433 : loss : 0.095054, loss_ce: 0.017873\n","iteration 13434 : loss : 0.074104, loss_ce: 0.024996\n","iteration 13435 : loss : 0.086569, loss_ce: 0.032675\n","iteration 13436 : loss : 0.092233, loss_ce: 0.023847\n","iteration 13437 : loss : 0.078851, loss_ce: 0.028061\n","iteration 13438 : loss : 0.097701, loss_ce: 0.036174\n","iteration 13439 : loss : 0.082453, loss_ce: 0.017723\n","iteration 13440 : loss : 0.092172, loss_ce: 0.019470\n","iteration 13441 : loss : 0.075456, loss_ce: 0.021457\n","iteration 13442 : loss : 0.098233, loss_ce: 0.019139\n","iteration 13443 : loss : 0.077451, loss_ce: 0.027319\n","iteration 13444 : loss : 0.083183, loss_ce: 0.038029\n","iteration 13445 : loss : 0.138977, loss_ce: 0.023623\n","iteration 13446 : loss : 0.067127, loss_ce: 0.023703\n","iteration 13447 : loss : 0.106875, loss_ce: 0.013103\n","iteration 13448 : loss : 0.085456, loss_ce: 0.027581\n","iteration 13449 : loss : 0.087781, loss_ce: 0.036169\n","iteration 13450 : loss : 0.069753, loss_ce: 0.028854\n","iteration 13451 : loss : 0.103897, loss_ce: 0.041872\n","iteration 13452 : loss : 0.137079, loss_ce: 0.014568\n","iteration 13453 : loss : 0.085782, loss_ce: 0.031229\n","iteration 13454 : loss : 0.095420, loss_ce: 0.034276\n","iteration 13455 : loss : 0.089746, loss_ce: 0.027447\n","iteration 13456 : loss : 0.075041, loss_ce: 0.025536\n","iteration 13457 : loss : 0.093829, loss_ce: 0.032472\n","iteration 13458 : loss : 0.138418, loss_ce: 0.020665\n","iteration 13459 : loss : 0.087495, loss_ce: 0.033391\n","iteration 13460 : loss : 0.069596, loss_ce: 0.026663\n","iteration 13461 : loss : 0.082399, loss_ce: 0.030354\n","iteration 13462 : loss : 0.071826, loss_ce: 0.020775\n","iteration 13463 : loss : 0.086447, loss_ce: 0.011928\n","iteration 13464 : loss : 0.072566, loss_ce: 0.015061\n","iteration 13465 : loss : 0.138353, loss_ce: 0.027484\n","iteration 13466 : loss : 0.071657, loss_ce: 0.020785\n","iteration 13467 : loss : 0.080057, loss_ce: 0.028356\n","iteration 13468 : loss : 0.065915, loss_ce: 0.012654\n","iteration 13469 : loss : 0.070065, loss_ce: 0.029133\n","iteration 13470 : loss : 0.062965, loss_ce: 0.020397\n","iteration 13471 : loss : 0.074394, loss_ce: 0.028681\n","iteration 13472 : loss : 0.079400, loss_ce: 0.031599\n","iteration 13473 : loss : 0.103379, loss_ce: 0.040091\n","iteration 13474 : loss : 0.069282, loss_ce: 0.027696\n","iteration 13475 : loss : 0.071543, loss_ce: 0.021024\n","iteration 13476 : loss : 0.087965, loss_ce: 0.023679\n","iteration 13477 : loss : 0.096501, loss_ce: 0.019566\n","iteration 13478 : loss : 0.081440, loss_ce: 0.022379\n","iteration 13479 : loss : 0.149287, loss_ce: 0.033517\n","iteration 13480 : loss : 0.079899, loss_ce: 0.027631\n","iteration 13481 : loss : 0.062944, loss_ce: 0.021742\n","iteration 13482 : loss : 0.095888, loss_ce: 0.040227\n","iteration 13483 : loss : 0.074212, loss_ce: 0.020038\n","iteration 13484 : loss : 0.057997, loss_ce: 0.018649\n","iteration 13485 : loss : 0.318085, loss_ce: 0.028752\n"," 97%|████████████████████████████ | 145/150 [3:09:38<06:31, 78.28s/it]iteration 13486 : loss : 0.063986, loss_ce: 0.022631\n","iteration 13487 : loss : 0.082728, loss_ce: 0.035409\n","iteration 13488 : loss : 0.078995, loss_ce: 0.023659\n","iteration 13489 : loss : 0.079282, loss_ce: 0.017898\n","iteration 13490 : loss : 0.072674, loss_ce: 0.028833\n","iteration 13491 : loss : 0.081985, loss_ce: 0.028524\n","iteration 13492 : loss : 0.089135, loss_ce: 0.030833\n","iteration 13493 : loss : 0.109553, loss_ce: 0.027097\n","iteration 13494 : loss : 0.136775, loss_ce: 0.024729\n","iteration 13495 : loss : 0.103199, loss_ce: 0.038180\n","iteration 13496 : loss : 0.144964, loss_ce: 0.024126\n","iteration 13497 : loss : 0.091051, loss_ce: 0.020180\n","iteration 13498 : loss : 0.076466, loss_ce: 0.021462\n","iteration 13499 : loss : 0.075563, loss_ce: 0.028320\n","iteration 13500 : loss : 0.071877, loss_ce: 0.020417\n","iteration 13501 : loss : 0.079302, loss_ce: 0.016349\n","iteration 13502 : loss : 0.075080, loss_ce: 0.030013\n","iteration 13503 : loss : 0.090059, loss_ce: 0.018491\n","iteration 13504 : loss : 0.144689, loss_ce: 0.013798\n","iteration 13505 : loss : 0.075014, loss_ce: 0.030701\n","iteration 13506 : loss : 0.136699, loss_ce: 0.025161\n","iteration 13507 : loss : 0.087853, loss_ce: 0.013154\n","iteration 13508 : loss : 0.155626, loss_ce: 0.014043\n","iteration 13509 : loss : 0.132190, loss_ce: 0.021431\n","iteration 13510 : loss : 0.086965, loss_ce: 0.027953\n","iteration 13511 : loss : 0.077514, loss_ce: 0.021680\n","iteration 13512 : loss : 0.121054, loss_ce: 0.022772\n","iteration 13513 : loss : 0.070247, loss_ce: 0.021408\n","iteration 13514 : loss : 0.092901, loss_ce: 0.020937\n","iteration 13515 : loss : 0.068351, loss_ce: 0.024777\n","iteration 13516 : loss : 0.097377, loss_ce: 0.030532\n","iteration 13517 : loss : 0.082091, loss_ce: 0.016619\n","iteration 13518 : loss : 0.131388, loss_ce: 0.026805\n","iteration 13519 : loss : 0.069194, loss_ce: 0.028484\n","iteration 13520 : loss : 0.076993, loss_ce: 0.031376\n","iteration 13521 : loss : 0.077848, loss_ce: 0.022321\n","iteration 13522 : loss : 0.082778, loss_ce: 0.025273\n","iteration 13523 : loss : 0.095883, loss_ce: 0.019005\n","iteration 13524 : loss : 0.122530, loss_ce: 0.030033\n","iteration 13525 : loss : 0.067205, loss_ce: 0.011504\n","iteration 13526 : loss : 0.091786, loss_ce: 0.019532\n","iteration 13527 : loss : 0.096872, loss_ce: 0.013743\n","iteration 13528 : loss : 0.074131, loss_ce: 0.020345\n","iteration 13529 : loss : 0.093828, loss_ce: 0.038844\n","iteration 13530 : loss : 0.074699, loss_ce: 0.026096\n","iteration 13531 : loss : 0.100182, loss_ce: 0.028522\n","iteration 13532 : loss : 0.095625, loss_ce: 0.021303\n","iteration 13533 : loss : 0.122185, loss_ce: 0.026927\n","iteration 13534 : loss : 0.083359, loss_ce: 0.029589\n","iteration 13535 : loss : 0.075219, loss_ce: 0.033092\n","iteration 13536 : loss : 0.081718, loss_ce: 0.022924\n","iteration 13537 : loss : 0.100544, loss_ce: 0.026738\n","iteration 13538 : loss : 0.079322, loss_ce: 0.026519\n","iteration 13539 : loss : 0.069661, loss_ce: 0.023644\n","iteration 13540 : loss : 0.074819, loss_ce: 0.014741\n","iteration 13541 : loss : 0.072044, loss_ce: 0.022342\n","iteration 13542 : loss : 0.074547, loss_ce: 0.023657\n","iteration 13543 : loss : 0.259956, loss_ce: 0.013129\n","iteration 13544 : loss : 0.084412, loss_ce: 0.022598\n","iteration 13545 : loss : 0.130967, loss_ce: 0.013062\n","iteration 13546 : loss : 0.111864, loss_ce: 0.028062\n","iteration 13547 : loss : 0.094341, loss_ce: 0.024828\n","iteration 13548 : loss : 0.087840, loss_ce: 0.026174\n","iteration 13549 : loss : 0.091346, loss_ce: 0.020697\n","iteration 13550 : loss : 0.084326, loss_ce: 0.033160\n","iteration 13551 : loss : 0.085838, loss_ce: 0.023352\n","iteration 13552 : loss : 0.088477, loss_ce: 0.023932\n","iteration 13553 : loss : 0.104795, loss_ce: 0.026478\n","iteration 13554 : loss : 0.076430, loss_ce: 0.026067\n","iteration 13555 : loss : 0.078565, loss_ce: 0.033556\n","iteration 13556 : loss : 0.091026, loss_ce: 0.036370\n","iteration 13557 : loss : 0.082467, loss_ce: 0.036890\n","iteration 13558 : loss : 0.085552, loss_ce: 0.025469\n","iteration 13559 : loss : 0.089260, loss_ce: 0.030332\n","iteration 13560 : loss : 0.104911, loss_ce: 0.020941\n","iteration 13561 : loss : 0.074199, loss_ce: 0.017533\n","iteration 13562 : loss : 0.100070, loss_ce: 0.025061\n","iteration 13563 : loss : 0.080909, loss_ce: 0.028131\n","iteration 13564 : loss : 0.138850, loss_ce: 0.029223\n","iteration 13565 : loss : 0.072015, loss_ce: 0.027469\n","iteration 13566 : loss : 0.083443, loss_ce: 0.025530\n","iteration 13567 : loss : 0.081202, loss_ce: 0.038530\n","iteration 13568 : loss : 0.084295, loss_ce: 0.027728\n","iteration 13569 : loss : 0.137151, loss_ce: 0.020138\n","iteration 13570 : loss : 0.066808, loss_ce: 0.024191\n","iteration 13571 : loss : 0.084931, loss_ce: 0.030813\n","iteration 13572 : loss : 0.072449, loss_ce: 0.030317\n","iteration 13573 : loss : 0.078501, loss_ce: 0.026396\n","iteration 13574 : loss : 0.065557, loss_ce: 0.029902\n","iteration 13575 : loss : 0.071741, loss_ce: 0.026751\n","iteration 13576 : loss : 0.085677, loss_ce: 0.028733\n","iteration 13577 : loss : 0.076295, loss_ce: 0.020537\n","iteration 13578 : loss : 0.276739, loss_ce: 0.019124\n"," 97%|████████████████████████████▏| 146/150 [3:10:59<05:15, 78.90s/it]iteration 13579 : loss : 0.076567, loss_ce: 0.014282\n","iteration 13580 : loss : 0.140062, loss_ce: 0.020015\n","iteration 13581 : loss : 0.070812, loss_ce: 0.013443\n","iteration 13582 : loss : 0.085957, loss_ce: 0.031105\n","iteration 13583 : loss : 0.080678, loss_ce: 0.033940\n","iteration 13584 : loss : 0.063580, loss_ce: 0.021176\n","iteration 13585 : loss : 0.084060, loss_ce: 0.028599\n","iteration 13586 : loss : 0.080239, loss_ce: 0.020696\n","iteration 13587 : loss : 0.089000, loss_ce: 0.029661\n","iteration 13588 : loss : 0.076280, loss_ce: 0.025572\n","iteration 13589 : loss : 0.096707, loss_ce: 0.031716\n","iteration 13590 : loss : 0.129666, loss_ce: 0.026426\n","iteration 13591 : loss : 0.092905, loss_ce: 0.022506\n","iteration 13592 : loss : 0.077018, loss_ce: 0.018617\n","iteration 13593 : loss : 0.084499, loss_ce: 0.014483\n","iteration 13594 : loss : 0.128343, loss_ce: 0.018629\n","iteration 13595 : loss : 0.092814, loss_ce: 0.030025\n","iteration 13596 : loss : 0.073700, loss_ce: 0.031608\n","iteration 13597 : loss : 0.079110, loss_ce: 0.028911\n","iteration 13598 : loss : 0.074930, loss_ce: 0.022509\n","iteration 13599 : loss : 0.108646, loss_ce: 0.020494\n","iteration 13600 : loss : 0.076810, loss_ce: 0.019523\n","iteration 13601 : loss : 0.066269, loss_ce: 0.030756\n","iteration 13602 : loss : 0.078072, loss_ce: 0.030392\n","iteration 13603 : loss : 0.093856, loss_ce: 0.029143\n","iteration 13604 : loss : 0.093628, loss_ce: 0.019516\n","iteration 13605 : loss : 0.082546, loss_ce: 0.020341\n","iteration 13606 : loss : 0.070390, loss_ce: 0.019432\n","iteration 13607 : loss : 0.071060, loss_ce: 0.023186\n","iteration 13608 : loss : 0.061737, loss_ce: 0.020559\n","iteration 13609 : loss : 0.080152, loss_ce: 0.024563\n","iteration 13610 : loss : 0.092695, loss_ce: 0.028493\n","iteration 13611 : loss : 0.191166, loss_ce: 0.014376\n","iteration 13612 : loss : 0.084801, loss_ce: 0.037092\n","iteration 13613 : loss : 0.162974, loss_ce: 0.038326\n","iteration 13614 : loss : 0.087804, loss_ce: 0.020862\n","iteration 13615 : loss : 0.068650, loss_ce: 0.028638\n","iteration 13616 : loss : 0.087256, loss_ce: 0.033562\n","iteration 13617 : loss : 0.070213, loss_ce: 0.025052\n","iteration 13618 : loss : 0.056433, loss_ce: 0.021425\n","iteration 13619 : loss : 0.072482, loss_ce: 0.020400\n","iteration 13620 : loss : 0.078361, loss_ce: 0.029620\n","iteration 13621 : loss : 0.139229, loss_ce: 0.022916\n","iteration 13622 : loss : 0.091631, loss_ce: 0.032794\n","iteration 13623 : loss : 0.228358, loss_ce: 0.011130\n","iteration 13624 : loss : 0.084904, loss_ce: 0.018962\n","iteration 13625 : loss : 0.068514, loss_ce: 0.019498\n","iteration 13626 : loss : 0.077979, loss_ce: 0.024909\n","iteration 13627 : loss : 0.091787, loss_ce: 0.024725\n","iteration 13628 : loss : 0.070718, loss_ce: 0.026661\n","iteration 13629 : loss : 0.076613, loss_ce: 0.023336\n","iteration 13630 : loss : 0.115369, loss_ce: 0.015054\n","iteration 13631 : loss : 0.099034, loss_ce: 0.027421\n","iteration 13632 : loss : 0.076142, loss_ce: 0.020351\n","iteration 13633 : loss : 0.081132, loss_ce: 0.016016\n","iteration 13634 : loss : 0.077788, loss_ce: 0.023139\n","iteration 13635 : loss : 0.122283, loss_ce: 0.023243\n","iteration 13636 : loss : 0.078426, loss_ce: 0.024997\n","iteration 13637 : loss : 0.084494, loss_ce: 0.022712\n","iteration 13638 : loss : 0.093553, loss_ce: 0.025510\n","iteration 13639 : loss : 0.090670, loss_ce: 0.037190\n","iteration 13640 : loss : 0.095547, loss_ce: 0.028126\n","iteration 13641 : loss : 0.196175, loss_ce: 0.016662\n","iteration 13642 : loss : 0.072959, loss_ce: 0.018389\n","iteration 13643 : loss : 0.081127, loss_ce: 0.025211\n","iteration 13644 : loss : 0.080224, loss_ce: 0.020637\n","iteration 13645 : loss : 0.065575, loss_ce: 0.020044\n","iteration 13646 : loss : 0.095176, loss_ce: 0.034914\n","iteration 13647 : loss : 0.114947, loss_ce: 0.029111\n","iteration 13648 : loss : 0.064229, loss_ce: 0.012353\n","iteration 13649 : loss : 0.066982, loss_ce: 0.027160\n","iteration 13650 : loss : 0.105456, loss_ce: 0.025234\n","iteration 13651 : loss : 0.063956, loss_ce: 0.018125\n","iteration 13652 : loss : 0.095448, loss_ce: 0.028887\n","iteration 13653 : loss : 0.082598, loss_ce: 0.029688\n","iteration 13654 : loss : 0.067424, loss_ce: 0.015836\n","iteration 13655 : loss : 0.070525, loss_ce: 0.019845\n","iteration 13656 : loss : 0.108392, loss_ce: 0.041272\n","iteration 13657 : loss : 0.067173, loss_ce: 0.021762\n","iteration 13658 : loss : 0.093115, loss_ce: 0.023980\n","iteration 13659 : loss : 0.085399, loss_ce: 0.025360\n","iteration 13660 : loss : 0.090414, loss_ce: 0.037720\n","iteration 13661 : loss : 0.092024, loss_ce: 0.040081\n","iteration 13662 : loss : 0.073614, loss_ce: 0.027603\n","iteration 13663 : loss : 0.091817, loss_ce: 0.035956\n","iteration 13664 : loss : 0.083218, loss_ce: 0.023197\n","iteration 13665 : loss : 0.106915, loss_ce: 0.018025\n","iteration 13666 : loss : 0.073469, loss_ce: 0.013395\n","iteration 13667 : loss : 0.073674, loss_ce: 0.032709\n","iteration 13668 : loss : 0.083654, loss_ce: 0.032153\n","iteration 13669 : loss : 0.094576, loss_ce: 0.025065\n","iteration 13670 : loss : 0.087513, loss_ce: 0.026527\n","iteration 13671 : loss : 0.488592, loss_ce: 0.009171\n"," 98%|████████████████████████████▍| 147/150 [3:12:19<03:58, 79.46s/it]iteration 13672 : loss : 0.072332, loss_ce: 0.025943\n","iteration 13673 : loss : 0.107246, loss_ce: 0.023610\n","iteration 13674 : loss : 0.089967, loss_ce: 0.026255\n","iteration 13675 : loss : 0.085875, loss_ce: 0.019756\n","iteration 13676 : loss : 0.082684, loss_ce: 0.020772\n","iteration 13677 : loss : 0.081620, loss_ce: 0.025714\n","iteration 13678 : loss : 0.073080, loss_ce: 0.030213\n","iteration 13679 : loss : 0.074384, loss_ce: 0.025698\n","iteration 13680 : loss : 0.090686, loss_ce: 0.040681\n","iteration 13681 : loss : 0.086340, loss_ce: 0.024019\n","iteration 13682 : loss : 0.070944, loss_ce: 0.018928\n","iteration 13683 : loss : 0.064931, loss_ce: 0.023581\n","iteration 13684 : loss : 0.095142, loss_ce: 0.029827\n","iteration 13685 : loss : 0.074617, loss_ce: 0.018363\n","iteration 13686 : loss : 0.086668, loss_ce: 0.026015\n","iteration 13687 : loss : 0.083347, loss_ce: 0.027877\n","iteration 13688 : loss : 0.069818, loss_ce: 0.025253\n","iteration 13689 : loss : 0.076972, loss_ce: 0.013836\n","iteration 13690 : loss : 0.086625, loss_ce: 0.030488\n","iteration 13691 : loss : 0.079776, loss_ce: 0.016281\n","iteration 13692 : loss : 0.079237, loss_ce: 0.025643\n","iteration 13693 : loss : 0.079385, loss_ce: 0.025937\n","iteration 13694 : loss : 0.082699, loss_ce: 0.024962\n","iteration 13695 : loss : 0.082175, loss_ce: 0.027450\n","iteration 13696 : loss : 0.065113, loss_ce: 0.026653\n","iteration 13697 : loss : 0.085194, loss_ce: 0.025825\n","iteration 13698 : loss : 0.083385, loss_ce: 0.035081\n","iteration 13699 : loss : 0.111875, loss_ce: 0.023769\n","iteration 13700 : loss : 0.075995, loss_ce: 0.028027\n","iteration 13701 : loss : 0.088624, loss_ce: 0.022020\n","iteration 13702 : loss : 0.086346, loss_ce: 0.028019\n","iteration 13703 : loss : 0.085813, loss_ce: 0.023553\n","iteration 13704 : loss : 0.078177, loss_ce: 0.033075\n","iteration 13705 : loss : 0.077649, loss_ce: 0.016280\n","iteration 13706 : loss : 0.106636, loss_ce: 0.022383\n","iteration 13707 : loss : 0.146593, loss_ce: 0.017004\n","iteration 13708 : loss : 0.081238, loss_ce: 0.020765\n","iteration 13709 : loss : 0.086634, loss_ce: 0.021880\n","iteration 13710 : loss : 0.090916, loss_ce: 0.034945\n","iteration 13711 : loss : 0.064224, loss_ce: 0.015056\n","iteration 13712 : loss : 0.097455, loss_ce: 0.024050\n","iteration 13713 : loss : 0.079054, loss_ce: 0.018683\n","iteration 13714 : loss : 0.063663, loss_ce: 0.023657\n","iteration 13715 : loss : 0.085319, loss_ce: 0.032081\n","iteration 13716 : loss : 0.085230, loss_ce: 0.038839\n","iteration 13717 : loss : 0.099757, loss_ce: 0.037591\n","iteration 13718 : loss : 0.107110, loss_ce: 0.018812\n","iteration 13719 : loss : 0.079002, loss_ce: 0.022796\n","iteration 13720 : loss : 0.087307, loss_ce: 0.021417\n","iteration 13721 : loss : 0.155471, loss_ce: 0.018535\n","iteration 13722 : loss : 0.088084, loss_ce: 0.031722\n","iteration 13723 : loss : 0.094003, loss_ce: 0.022305\n","iteration 13724 : loss : 0.102024, loss_ce: 0.024211\n","iteration 13725 : loss : 0.076835, loss_ce: 0.025332\n","iteration 13726 : loss : 0.070934, loss_ce: 0.016076\n","iteration 13727 : loss : 0.088950, loss_ce: 0.020574\n","iteration 13728 : loss : 0.071844, loss_ce: 0.020579\n","iteration 13729 : loss : 0.069642, loss_ce: 0.028065\n","iteration 13730 : loss : 0.077949, loss_ce: 0.029130\n","iteration 13731 : loss : 0.149833, loss_ce: 0.022303\n","iteration 13732 : loss : 0.079776, loss_ce: 0.031469\n","iteration 13733 : loss : 0.073995, loss_ce: 0.023772\n","iteration 13734 : loss : 0.124930, loss_ce: 0.021621\n","iteration 13735 : loss : 0.080242, loss_ce: 0.030970\n","iteration 13736 : loss : 0.094993, loss_ce: 0.024834\n","iteration 13737 : loss : 0.103153, loss_ce: 0.020210\n","iteration 13738 : loss : 0.083307, loss_ce: 0.022510\n","iteration 13739 : loss : 0.145257, loss_ce: 0.020717\n","iteration 13740 : loss : 0.075569, loss_ce: 0.026136\n","iteration 13741 : loss : 0.068004, loss_ce: 0.024393\n","iteration 13742 : loss : 0.085743, loss_ce: 0.020949\n","iteration 13743 : loss : 0.084982, loss_ce: 0.024046\n","iteration 13744 : loss : 0.164984, loss_ce: 0.020165\n","iteration 13745 : loss : 0.184794, loss_ce: 0.020082\n","iteration 13746 : loss : 0.069955, loss_ce: 0.025782\n","iteration 13747 : loss : 0.072540, loss_ce: 0.024582\n","iteration 13748 : loss : 0.091614, loss_ce: 0.034319\n","iteration 13749 : loss : 0.071757, loss_ce: 0.030534\n","iteration 13750 : loss : 0.063096, loss_ce: 0.010236\n","iteration 13751 : loss : 0.101330, loss_ce: 0.027786\n","iteration 13752 : loss : 0.096456, loss_ce: 0.013873\n","iteration 13753 : loss : 0.091210, loss_ce: 0.022944\n","iteration 13754 : loss : 0.074331, loss_ce: 0.023095\n","iteration 13755 : loss : 0.119177, loss_ce: 0.029538\n","iteration 13756 : loss : 0.090017, loss_ce: 0.021266\n","iteration 13757 : loss : 0.069078, loss_ce: 0.032741\n","iteration 13758 : loss : 0.080350, loss_ce: 0.021152\n","iteration 13759 : loss : 0.084526, loss_ce: 0.031803\n","iteration 13760 : loss : 0.082113, loss_ce: 0.024984\n","iteration 13761 : loss : 0.080351, loss_ce: 0.027359\n","iteration 13762 : loss : 0.070068, loss_ce: 0.017793\n","iteration 13763 : loss : 0.073648, loss_ce: 0.022025\n","iteration 13764 : loss : 0.091157, loss_ce: 0.039485\n"," 99%|████████████████████████████▌| 148/150 [3:13:35<02:36, 78.39s/it]iteration 13765 : loss : 0.105332, loss_ce: 0.025457\n","iteration 13766 : loss : 0.082320, loss_ce: 0.023232\n","iteration 13767 : loss : 0.094881, loss_ce: 0.031703\n","iteration 13768 : loss : 0.103703, loss_ce: 0.028963\n","iteration 13769 : loss : 0.102173, loss_ce: 0.023616\n","iteration 13770 : loss : 0.098145, loss_ce: 0.027785\n","iteration 13771 : loss : 0.074374, loss_ce: 0.032575\n","iteration 13772 : loss : 0.066461, loss_ce: 0.022378\n","iteration 13773 : loss : 0.081525, loss_ce: 0.023518\n","iteration 13774 : loss : 0.065828, loss_ce: 0.024735\n","iteration 13775 : loss : 0.132192, loss_ce: 0.015623\n","iteration 13776 : loss : 0.090229, loss_ce: 0.016118\n","iteration 13777 : loss : 0.078747, loss_ce: 0.026941\n","iteration 13778 : loss : 0.088620, loss_ce: 0.019688\n","iteration 13779 : loss : 0.071775, loss_ce: 0.015280\n","iteration 13780 : loss : 0.092957, loss_ce: 0.027608\n","iteration 13781 : loss : 0.133527, loss_ce: 0.018640\n","iteration 13782 : loss : 0.077541, loss_ce: 0.024748\n","iteration 13783 : loss : 0.067660, loss_ce: 0.023640\n","iteration 13784 : loss : 0.074033, loss_ce: 0.039249\n","iteration 13785 : loss : 0.093131, loss_ce: 0.018980\n","iteration 13786 : loss : 0.108438, loss_ce: 0.024457\n","iteration 13787 : loss : 0.095312, loss_ce: 0.034629\n","iteration 13788 : loss : 0.102649, loss_ce: 0.035599\n","iteration 13789 : loss : 0.077855, loss_ce: 0.024593\n","iteration 13790 : loss : 0.074545, loss_ce: 0.028554\n","iteration 13791 : loss : 0.083731, loss_ce: 0.021580\n","iteration 13792 : loss : 0.073728, loss_ce: 0.027485\n","iteration 13793 : loss : 0.083312, loss_ce: 0.033622\n","iteration 13794 : loss : 0.100894, loss_ce: 0.023131\n","iteration 13795 : loss : 0.118274, loss_ce: 0.017539\n","iteration 13796 : loss : 0.086260, loss_ce: 0.025068\n","iteration 13797 : loss : 0.085148, loss_ce: 0.015935\n","iteration 13798 : loss : 0.071092, loss_ce: 0.027816\n","iteration 13799 : loss : 0.074997, loss_ce: 0.030280\n","iteration 13800 : loss : 0.105063, loss_ce: 0.037718\n","iteration 13801 : loss : 0.133689, loss_ce: 0.011645\n","iteration 13802 : loss : 0.081731, loss_ce: 0.026244\n","iteration 13803 : loss : 0.071614, loss_ce: 0.026962\n","iteration 13804 : loss : 0.090789, loss_ce: 0.022344\n","iteration 13805 : loss : 0.155584, loss_ce: 0.020643\n","iteration 13806 : loss : 0.089063, loss_ce: 0.018339\n","iteration 13807 : loss : 0.074965, loss_ce: 0.026964\n","iteration 13808 : loss : 0.073641, loss_ce: 0.025404\n","iteration 13809 : loss : 0.084310, loss_ce: 0.028484\n","iteration 13810 : loss : 0.065399, loss_ce: 0.026019\n","iteration 13811 : loss : 0.092881, loss_ce: 0.031969\n","iteration 13812 : loss : 0.073421, loss_ce: 0.020749\n","iteration 13813 : loss : 0.123797, loss_ce: 0.017725\n","iteration 13814 : loss : 0.086531, loss_ce: 0.022596\n","iteration 13815 : loss : 0.101001, loss_ce: 0.018821\n","iteration 13816 : loss : 0.092737, loss_ce: 0.014300\n","iteration 13817 : loss : 0.082477, loss_ce: 0.023853\n","iteration 13818 : loss : 0.071276, loss_ce: 0.018349\n","iteration 13819 : loss : 0.073551, loss_ce: 0.021766\n","iteration 13820 : loss : 0.073461, loss_ce: 0.026490\n","iteration 13821 : loss : 0.066131, loss_ce: 0.020691\n","iteration 13822 : loss : 0.096854, loss_ce: 0.035167\n","iteration 13823 : loss : 0.073031, loss_ce: 0.022529\n","iteration 13824 : loss : 0.079629, loss_ce: 0.026363\n","iteration 13825 : loss : 0.092440, loss_ce: 0.035608\n","iteration 13826 : loss : 0.185484, loss_ce: 0.023343\n","iteration 13827 : loss : 0.132591, loss_ce: 0.018854\n","iteration 13828 : loss : 0.074788, loss_ce: 0.027774\n","iteration 13829 : loss : 0.094154, loss_ce: 0.028520\n","iteration 13830 : loss : 0.093801, loss_ce: 0.031539\n","iteration 13831 : loss : 0.072598, loss_ce: 0.020371\n","iteration 13832 : loss : 0.081117, loss_ce: 0.033345\n","iteration 13833 : loss : 0.070605, loss_ce: 0.023727\n","iteration 13834 : loss : 0.118282, loss_ce: 0.024119\n","iteration 13835 : loss : 0.075954, loss_ce: 0.025216\n","iteration 13836 : loss : 0.141348, loss_ce: 0.018778\n","iteration 13837 : loss : 0.082854, loss_ce: 0.038522\n","iteration 13838 : loss : 0.078221, loss_ce: 0.021264\n","iteration 13839 : loss : 0.079552, loss_ce: 0.020045\n","iteration 13840 : loss : 0.070983, loss_ce: 0.028769\n","iteration 13841 : loss : 0.191444, loss_ce: 0.016436\n","iteration 13842 : loss : 0.091941, loss_ce: 0.008699\n","iteration 13843 : loss : 0.083022, loss_ce: 0.024217\n","iteration 13844 : loss : 0.082782, loss_ce: 0.035604\n","iteration 13845 : loss : 0.085039, loss_ce: 0.022767\n","iteration 13846 : loss : 0.111076, loss_ce: 0.032136\n","iteration 13847 : loss : 0.075821, loss_ce: 0.025288\n","iteration 13848 : loss : 0.104371, loss_ce: 0.024830\n","iteration 13849 : loss : 0.132205, loss_ce: 0.030828\n","iteration 13850 : loss : 0.161389, loss_ce: 0.015770\n","iteration 13851 : loss : 0.078781, loss_ce: 0.028344\n","iteration 13852 : loss : 0.134246, loss_ce: 0.024326\n","iteration 13853 : loss : 0.072147, loss_ce: 0.020005\n","iteration 13854 : loss : 0.082149, loss_ce: 0.037211\n","iteration 13855 : loss : 0.074806, loss_ce: 0.022909\n","iteration 13856 : loss : 0.078675, loss_ce: 0.017248\n","iteration 13857 : loss : 0.524588, loss_ce: 0.000876\n"," 99%|████████████████████████████▊| 149/150 [3:14:56<01:19, 79.17s/it]iteration 13858 : loss : 0.084316, loss_ce: 0.025026\n","iteration 13859 : loss : 0.085252, loss_ce: 0.028717\n","iteration 13860 : loss : 0.085455, loss_ce: 0.034029\n","iteration 13861 : loss : 0.060065, loss_ce: 0.015967\n","iteration 13862 : loss : 0.164081, loss_ce: 0.031038\n","iteration 13863 : loss : 0.130045, loss_ce: 0.027340\n","iteration 13864 : loss : 0.176418, loss_ce: 0.031005\n","iteration 13865 : loss : 0.069832, loss_ce: 0.020920\n","iteration 13866 : loss : 0.084805, loss_ce: 0.029552\n","iteration 13867 : loss : 0.077668, loss_ce: 0.021212\n","iteration 13868 : loss : 0.062709, loss_ce: 0.024807\n","iteration 13869 : loss : 0.081702, loss_ce: 0.028195\n","iteration 13870 : loss : 0.071268, loss_ce: 0.031903\n","iteration 13871 : loss : 0.076154, loss_ce: 0.022109\n","iteration 13872 : loss : 0.157332, loss_ce: 0.016509\n","iteration 13873 : loss : 0.091161, loss_ce: 0.037893\n","iteration 13874 : loss : 0.087084, loss_ce: 0.029521\n","iteration 13875 : loss : 0.083196, loss_ce: 0.022833\n","iteration 13876 : loss : 0.077081, loss_ce: 0.026523\n","iteration 13877 : loss : 0.074842, loss_ce: 0.025018\n","iteration 13878 : loss : 0.066964, loss_ce: 0.015221\n","iteration 13879 : loss : 0.078928, loss_ce: 0.030168\n","iteration 13880 : loss : 0.079082, loss_ce: 0.032397\n","iteration 13881 : loss : 0.074853, loss_ce: 0.020043\n","iteration 13882 : loss : 0.098044, loss_ce: 0.038930\n","iteration 13883 : loss : 0.094301, loss_ce: 0.020779\n","iteration 13884 : loss : 0.138153, loss_ce: 0.025125\n","iteration 13885 : loss : 0.074507, loss_ce: 0.031426\n","iteration 13886 : loss : 0.063279, loss_ce: 0.023201\n","iteration 13887 : loss : 0.077370, loss_ce: 0.011839\n","iteration 13888 : loss : 0.099711, loss_ce: 0.022906\n","iteration 13889 : loss : 0.093381, loss_ce: 0.016215\n","iteration 13890 : loss : 0.144282, loss_ce: 0.024154\n","iteration 13891 : loss : 0.126335, loss_ce: 0.019439\n","iteration 13892 : loss : 0.088969, loss_ce: 0.036463\n","iteration 13893 : loss : 0.070274, loss_ce: 0.031064\n","iteration 13894 : loss : 0.072674, loss_ce: 0.030955\n","iteration 13895 : loss : 0.111696, loss_ce: 0.028003\n","iteration 13896 : loss : 0.082588, loss_ce: 0.021539\n","iteration 13897 : loss : 0.075105, loss_ce: 0.028039\n","iteration 13898 : loss : 0.085020, loss_ce: 0.028186\n","iteration 13899 : loss : 0.173028, loss_ce: 0.018802\n","iteration 13900 : loss : 0.119544, loss_ce: 0.028541\n","iteration 13901 : loss : 0.135460, loss_ce: 0.021720\n","iteration 13902 : loss : 0.077740, loss_ce: 0.023900\n","iteration 13903 : loss : 0.093899, loss_ce: 0.035716\n","iteration 13904 : loss : 0.070152, loss_ce: 0.029284\n","iteration 13905 : loss : 0.072709, loss_ce: 0.028408\n","iteration 13906 : loss : 0.100488, loss_ce: 0.013630\n","iteration 13907 : loss : 0.090013, loss_ce: 0.030259\n","iteration 13908 : loss : 0.087055, loss_ce: 0.022995\n","iteration 13909 : loss : 0.075101, loss_ce: 0.022142\n","iteration 13910 : loss : 0.102405, loss_ce: 0.029859\n","iteration 13911 : loss : 0.102850, loss_ce: 0.020682\n","iteration 13912 : loss : 0.081105, loss_ce: 0.024138\n","iteration 13913 : loss : 0.073590, loss_ce: 0.023094\n","iteration 13914 : loss : 0.086601, loss_ce: 0.029481\n","iteration 13915 : loss : 0.075887, loss_ce: 0.017340\n","iteration 13916 : loss : 0.097920, loss_ce: 0.030875\n","iteration 13917 : loss : 0.076109, loss_ce: 0.031544\n","iteration 13918 : loss : 0.090908, loss_ce: 0.022969\n","iteration 13919 : loss : 0.145023, loss_ce: 0.023064\n","iteration 13920 : loss : 0.077980, loss_ce: 0.030845\n","iteration 13921 : loss : 0.085551, loss_ce: 0.032211\n","iteration 13922 : loss : 0.152083, loss_ce: 0.016007\n","iteration 13923 : loss : 0.103564, loss_ce: 0.024030\n","iteration 13924 : loss : 0.062680, loss_ce: 0.016010\n","iteration 13925 : loss : 0.072506, loss_ce: 0.028566\n","iteration 13926 : loss : 0.139646, loss_ce: 0.017826\n","iteration 13927 : loss : 0.085180, loss_ce: 0.016622\n","iteration 13928 : loss : 0.067721, loss_ce: 0.021586\n","iteration 13929 : loss : 0.073688, loss_ce: 0.025835\n","iteration 13930 : loss : 0.075385, loss_ce: 0.025164\n","iteration 13931 : loss : 0.089838, loss_ce: 0.034706\n","iteration 13932 : loss : 0.068084, loss_ce: 0.015557\n","iteration 13933 : loss : 0.124082, loss_ce: 0.022087\n","iteration 13934 : loss : 0.069679, loss_ce: 0.026319\n","iteration 13935 : loss : 0.097718, loss_ce: 0.014001\n","iteration 13936 : loss : 0.054063, loss_ce: 0.017554\n","iteration 13937 : loss : 0.056371, loss_ce: 0.017965\n","iteration 13938 : loss : 0.088666, loss_ce: 0.014089\n","iteration 13939 : loss : 0.090849, loss_ce: 0.026299\n","iteration 13940 : loss : 0.120171, loss_ce: 0.019431\n","iteration 13941 : loss : 0.093139, loss_ce: 0.034405\n","iteration 13942 : loss : 0.128391, loss_ce: 0.015707\n","iteration 13943 : loss : 0.105602, loss_ce: 0.032904\n","iteration 13944 : loss : 0.089078, loss_ce: 0.019216\n","iteration 13945 : loss : 0.096811, loss_ce: 0.024159\n","iteration 13946 : loss : 0.088179, loss_ce: 0.023627\n","iteration 13947 : loss : 0.098184, loss_ce: 0.020752\n","iteration 13948 : loss : 0.112914, loss_ce: 0.030379\n","iteration 13949 : loss : 0.075788, loss_ce: 0.025273\n","iteration 13950 : loss : 0.532693, loss_ce: 0.000995\n","save model to /content/drive/My Drive/itu/PhD/BLG 641E - Medical Image Computing/final project/saved/epoch_149.pth\n","save model to /content/drive/My Drive/itu/PhD/BLG 641E - Medical Image Computing/final project/saved/epoch_149.pth\n"," 99%|████████████████████████████▊| 149/150 [3:16:18<01:19, 79.05s/it]\n"]}]},{"cell_type":"markdown","source":["#Test Model"],"metadata":{"id":"lhVHktWAxNwG"}},{"cell_type":"code","source":["!python test.py --dataset Synapse --cfg configs/swin_tiny_patch4_window7_224_lite.yaml --is_saveni --volume_path \"/content/drive/MyDrive/data/project_TransUNet/data/Synapse\" --output_dir \"/content/drive/My Drive/itu/PhD/BLG 641E - Medical Image Computing/final project/saved\" --max_epoch 150 --base_lr 0.05 --img_size 224 --batch_size 24\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E1nVxqgiAycn","executionInfo":{"status":"ok","timestamp":1686460513857,"user_tz":-180,"elapsed":1713320,"user":{"displayName":"Burak Kılıç","userId":"08570120524117355643"}},"outputId":"4629ae7c-04c9-44f8-94ab-a12cc13b671c"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["=> merge config from configs/swin_tiny_patch4_window7_224_lite.yaml\n","SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.2;num_classes:9\n","/usr/local/lib/python3.7/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","---final upsample expand_first---\n","self trained swin unet <All keys matched successfully>\n","Namespace(Dataset=<class 'datasets.dataset_synapse.Synapse_dataset'>, accumulation_steps=None, amp_opt_level='O1', base_lr=0.05, batch_size=24, cache_mode='part', cfg='configs/swin_tiny_patch4_window7_224_lite.yaml', dataset='Synapse', deterministic=1, eval=False, img_size=224, is_pretrain=True, is_savenii=True, list_dir='./lists/lists_Synapse', max_epochs=150, max_iterations=30000, num_classes=9, opts=None, output_dir='/content/drive/My Drive/itu/PhD/BLG 641E - Medical Image Computing/final project/saved', resume=None, seed=1234, tag=None, test_save_dir='../predictions', throughput=False, use_checkpoint=False, volume_path='/content/drive/MyDrive/data/project_TransUNet/data/Synapse/test_vol_h5', z_spacing=1, zip=False)\n","epoch_149.pth\n","12 test iterations per epoch\n","0it [00:00, ?it/s]idx 0 case case0008 mean_dice 0.372900 mean_hd95 41.857007\n","1it [02:49, 169.87s/it]idx 1 case case0022 mean_dice 0.719948 mean_hd95 37.096668\n","2it [04:26, 126.74s/it]idx 2 case case0038 mean_dice 0.709009 mean_hd95 33.807606\n","3it [06:09, 115.74s/it]idx 3 case case0036 mean_dice 0.751931 mean_hd95 51.580053\n","4it [09:46, 155.84s/it]idx 4 case case0032 mean_dice 0.724841 mean_hd95 62.365124\n","5it [12:20, 155.35s/it]idx 5 case case0002 mean_dice 0.733306 mean_hd95 19.819447\n","6it [14:52, 154.20s/it]idx 6 case case0029 mean_dice 0.471074 mean_hd95 69.682085\n","7it [16:31, 136.03s/it]idx 7 case case0003 mean_dice 0.474981 mean_hd95 106.909489\n","8it [20:16, 164.38s/it]idx 8 case case0001 mean_dice 0.638862 mean_hd95 22.906372\n","9it [22:57, 163.26s/it]idx 9 case case0004 mean_dice 0.523244 mean_hd95 63.846228\n","10it [25:27, 159.10s/it]idx 10 case case0025 mean_dice 0.530910 mean_hd95 42.428922\n","11it [26:54, 137.02s/it]idx 11 case case0035 mean_dice 0.778868 mean_hd95 8.648829\n","12it [28:19, 141.59s/it]\n","Mean class 1 mean_dice 0.708035 mean_hd95 27.572788\n","Mean class 2 mean_dice 0.559056 mean_hd95 32.550079\n","Mean class 3 mean_dice 0.635662 mean_hd95 76.632057\n","Mean class 4 mean_dice 0.577080 mean_hd95 77.400655\n","Mean class 5 mean_dice 0.892987 mean_hd95 27.929757\n","Mean class 6 mean_dice 0.330306 mean_hd95 23.072809\n","Mean class 7 mean_dice 0.708081 mean_hd95 80.574128\n","Mean class 8 mean_dice 0.542041 mean_hd95 28.232946\n","Testing performance in best val model: mean_dice : 0.619156 mean_hd95 : 46.745653\n"]}]}]}